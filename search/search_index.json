{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My Personal Website/Blog thinggy","text":"<p>My name is Vu Lam, Dang. I'm a researcher (PhD candidate) for the Center for Bioinformatic, Saarland University under professor Sven Rahmann supervision. My works lie between biological research in Population Genomics, Computer Science and Linear Algebra, with some DevOps springle in here and there.</p> <p>In 2021, I graduated from UGA's MoSIG program (Grenoble, France), specialized in Data Science. For my Master project, I investigated the grouping of residues in protein sequence into sectors with a method called SCA (Statistical Coupling Analysis). I had also developed a model to simulate the statistical coupling of these residues, using Gibbs sampling.</p> <p>For M1 internship I worked under Dr. Michael Blum's supervision to develop new method to calculate Polygenic Risk Score.</p> <p>After graduating from the University of Science and Technology of Hanoi, I  worked on a bioinformatic project under supervision of Dr. Pierre Larmande at USTH's ICTLab, developing an engine to query from multiple genomics databases. My bachelor thesis was an effort to combine Particle Swarm Optimisation to improve Extreme Learning Machine, under Dr. Sebastian Basterrech supervision.</p> <p>Beside working in informatics, I was the founder of a Fab Lab and Technology Club at USTH. Builder of several machines and design, I am well versed in (very informal) mechanical design and CAD/CAM. Favorite flavor of CAD: Fusion 360.</p>"},{"location":"CV/","title":"DANG Vu Lam","text":"<p>Researcher (PhD candidate)  Center for Bioinformatics, Saarland University</p> <p>Email: TBD Personal Contact: lam.dv@live.com Phone number: (+33) 6 68 36 02 29 </p> <p>Github | Linkedin | Webpage</p> <p> </p>"},{"location":"CV/#research-interest","title":"RESEARCH INTEREST","text":"<p>My current research interest including, but not limited to:</p> <ol> <li>Bioinformatics: Genomics, Proteomics. Meta-omics are currently in reading list</li> <li>Mathematics: Linear Algebra, Algorithm and Complexity, Graph Theory.</li> <li>Neural Network: Deep Learning and Deep Neural Net, Extreme Learning Machine.</li> <li>Optimization algorithms: Swarm Intelligent, Swarm Optimization and Metaheuristic.</li> <li>Data Science: Advance Data Structure, Indexing, Knowledge Representation.</li> </ol> <p> </p>"},{"location":"CV/#education","title":"EDUCATION","text":""},{"location":"CV/#universtiy-grenoble-alpes","title":"Universtiy Grenoble Alpes","text":"<p>2018 - 2021 Master of Science in Informatics UFR IMAG and ENSIMAG </p>"},{"location":"CV/#universtiy-of-science-and-technology-of-hanoi","title":"Universtiy of Science and Technology of Hanoi","text":"<p>Oct. 2013 - Oct 2017 Bachelor of Science and Technology Falcuty of Information and Communication Technology Thesis topic: Extreme Learning Machine</p> <p> </p>"},{"location":"CV/#working-expriences","title":"WORKING EXPRIENCES","text":""},{"location":"CV/#center-for-bioinformatics-saarland-university","title":"Center for Bioinformatics, Saarland University","text":"<p>Dec 2021 - Current</p> <p>Position Researcher, PhD Candidate</p>"},{"location":"CV/#master-research-project-timc-laboratory","title":"Master Research Project, TIMC Laboratory","text":"<p>Mar 2021 - Sep 2021</p>"},{"location":"CV/#master-research-project-timc-laboratory_1","title":"Master Research Project, TIMC Laboratory","text":"<p>Mar 2019 \u2013 Sep 2019</p> <p>Position: Programmer, Researcher.</p>"},{"location":"CV/#rrice-project-ird","title":"rRice project, IRD","text":"<p>Oct 2017 \u2013 Aug 2018</p> <p>Under the supervision of Dr. Pierre Larmande and Dr. Ho Bich Hai, I helped develop an R package and accompanying Python library to mine genomics data across multiple databases.</p> <p>Position: Programmer, Researcher.</p> <p> </p>"},{"location":"CV/#ictlab-university-of-science-and-technology-of-hanoi","title":"ICTLab, University of Science and Technology of Hanoi","text":"<p>Apr 2017 \u2013 Oct 2017</p> <p>As part of the collaboration between CVUT (Prague, Czech Republic) and USTH (Hanoi, Vietnam), I have done an internship under the supervision of Professor Basterrech (CVUT) that resulted in my bachelor thesis. We developed metaheuristic and swarm optimization algorithms for improving Extreme Learning Neural Networks.</p> <p>Position: Intern Researcher.</p> <p> </p>"},{"location":"CV/#fablab-usth","title":"FabLab USTH","text":"<p>May 2015 \u2013 Aug 2018</p> <p>FabLab are global network of Digital Fabrication Laboratory and workshops with the purpose of enabling invention, innovation and hacking of physical world by providing access to digital fabrication tools. In 2015, I organized the FabLab at USTH as a student club with the aim to advocate and support said goals and inciting hacking mentality for students at USTH</p> <p>Position: Founding Member, Mentor.</p> <p> </p>"},{"location":"CV/#fablab-vietnam-foundation","title":"FabLab Vietnam Foundation","text":"<p>May 2015 \u2013 2016</p> <p>FabLab Vietnam Foundation is a consortium of all FabLabs operates in Vietnam. At the time consists of 5 Labs, our purpose is to promote the development of FabLabs style facilities and Maker Movement in Vietnam. In 2015, the Foundation received a grant of $100,000 from IPP to further expand the movement.</p> <p>Position: Member, Representation of FabLab USTH.</p> <p> </p>"},{"location":"CV/#dream-project-incubator","title":"Dream Project Incubator","text":"<p>September 2015 \u2013 2016</p> <p>Dream Project Incubator aims to transform ambitious, self-driven young Vietnamese into better thinkers and doers via intellectual engagement and mentorship. Every year the project award 4 scholarship for ambitious Vietnamese whose age are between 19 and 25 for a 3 months summer trip to MIT, USA. After the conclusion of my USA trip, I served in the board of alumini, and as jury member and organizer for 2016 program</p> <p>Position: Alumni, Organizer, Jury.</p> <p> </p>"},{"location":"CV/#honoractivities","title":"HONOR/ACTIVITIES","text":""},{"location":"CV/#dream-project-incubator_1","title":"Dream Project Incubator","text":"<p>Jun 2015</p> <p>Dream Project Incubator is a program setup by MIT PhDs and PhD candidates in collaboration with Boston Global Forum. I was awarded a 3 months stay in Boston for my work with FabLab USTH and FabLab Vietnam Foundation, and the chances to meet and confer with top minds from MIT and Harvard University.</p> <p> </p>"},{"location":"CV/#3rd-prize-national-gifted-students-competition-in-informatics","title":"3rd prize National gifted students\u2019 competition in Informatics","text":"<p>2007, 2008 and 2009</p> <p>Held by Vietnam Ministry of Information Technology (now Ministry of Information and Communication)</p> <p> </p>"},{"location":"CV/#skills","title":"SKILLS","text":"<ul> <li>Mathematics   Linear Algebra; Graph Theory; Numerical and Combinatorial Optimization; Algorithms Design and Computational Complexity Theory.</li> <li>Sofware Engineering   Python 3.0; C, C++, C#; Java; NodeJS; R, Matlab, Tensorflow; Ruby on Rails; Arduino and Processing.</li> <li>Hardware, Engineering, Design and Fabrication   CAD Design (CATIA, Solidwork, Fusion 360); Metal Working; CNC machining; 3D printing; Electronics.</li> <li>Language<ul> <li>English   \u2003IELTS Academic 7.5 (as of 2018)   \u2003Completed my Bachelor program in English</li> <li>Vietnamese   \u2003First language</li> <li>French   \u2003Beginner</li> </ul> </li> </ul> <p>^In Vietnamese, the last name is DANG</p>"},{"location":"cv_pdf/","title":"DANG Vu Lam","text":"<p>MoSIG, Universite Grenoble Alpes</p> <p>Email: dangv@etu.univ-grenoble-alpes.frPersonal Contact: lam.dv@live.comPhone number: (+33) 6 68 36 02 29</p> <p> </p>"},{"location":"cv_pdf/#research-interest","title":"RESEARCH INTEREST","text":"<p>My current research interest including, but not limited to:</p> <ol> <li>Bioinformatics: Genomics, Proteomics. Currently researching SCA and simulation of protein family.</li> <li>Neural Network: Deep Learning and Deep Neural Net, Extreme Learning Machine.</li> <li>Optimization algorithms: Swarm Intelligent, Swarm Optimization and Metaheuristic.</li> <li>Data Science: Advance Data Structure, Indexing, Knowledge Representation.</li> <li>Mathematics: Linear Algebra, Algorithm and Complexity, Graph Theory.</li> </ol> <p> </p>"},{"location":"cv_pdf/#education","title":"EDUCATION","text":""},{"location":"cv_pdf/#universtiy-grenoble-alpes","title":"Universtiy Grenoble Alpes","text":"<p>Sep. 2018 - Current Master of Science in Informatics UFR IMAG - ENSIMAG </p>"},{"location":"cv_pdf/#universtiy-of-science-and-technology-of-hanoi","title":"Universtiy of Science and Technology of Hanoi","text":"<p>Oct. 2013 - Oct 2017 Bachelor of Science and Technology Falcuty of Information and Communication Technology Thesis topic: Extreme Learning Machine</p>"},{"location":"cv_pdf/#nguyen-tat-thanh-high-school","title":"Nguyen Tat Thanh High School","text":"<p>Hanoi National University of Education Sep 2010 - Sep 2013 \u00a0</p>"},{"location":"cv_pdf/#working-expriences","title":"WORKING EXPRIENCES","text":""},{"location":"cv_pdf/#master-1-internship-laboratoire-timc","title":"Master 1 Internship, Laboratoire TIMC","text":"<p>Jan 2019 - Aug 2019</p> <p>My internship to fulfill the requirement for Master 1 program was directed by Dr Michael Blum of Laboratoire TIMC. Under his supervision I has investigated multiple method to calculate Polygenic Risk Score, compare them under different metrics - in support of another project to create a new method to calculate PRS. \u00a0</p>"},{"location":"cv_pdf/#rrice-project-ird","title":"rRice project, IRD","text":"<p>Oct 2017 \u2013 Aug 2018</p> <p>Under the supervision of Dr. Pierre Larmande and Dr. Ho Bich Hai, I helped develop an R package and accompanying Python library to mine genomics data across multiple databases.</p> <p>Position: Programmer, Researcher. \u00a0</p>"},{"location":"cv_pdf/#ictlab-university-of-science-and-technology-of-hanoi","title":"ICTLab, University of Science and Technology of Hanoi","text":"<p>Apr 2017 \u2013 Oct 2017</p> <p>As part of the collaboration between CVUT (Prague, Czech Republic) and USTH (Hanoi, Vietnam), I have done an internship under the supervision of Professor Basterrech (CVUT) that resulted in my bachelor thesis. We developed metaheuristic and swarm optimization algorithms for improving Extreme Learning Neural Networks.</p> <p>Position: Intern Researcher.</p>"},{"location":"cv_pdf/#fablab-usth","title":"FabLab USTH","text":"<p>May 2015 \u2013 Aug 2018</p> <p>FabLab are global network of Digital Fabrication Laboratory and workshops with the purpose of enabling invention, innovation and hacking of physical world by providing access to digital fabrication tools. In 2015, I organized the FabLab at USTH as a student club with the aim to advocate and support said goals and inciting hacking mentality for students at USTH</p> <p>Position: Founding Member, Mentor.</p>"},{"location":"cv_pdf/#fablab-vietnam-foundation","title":"FabLab Vietnam Foundation","text":"<p>May 2015 \u2013 2016</p> <p>FabLab Vietnam Foundation is a consortium of all FabLabs operates in Vietnam. At the time consists of 5 Labs, our purpose is to promote the development of FabLabs style facilities and Maker Movement in Vietnam. In 2015, the Foundation received a grant of $100,000 from IPP to further expand the movement.</p> <p>Position: Member, Representation of FabLab USTH.</p>"},{"location":"cv_pdf/#dream-project-incubator","title":"Dream Project Incubator","text":"<p>September 2015 \u2013 2016</p> <p>Dream Project Incubator aims to transform ambitious, self-driven young Vietnamese into better thinkers and doers via intellectual engagement and mentorship. Every year the project award 4 scholarship for ambitious Vietnamese whose age are between 19 and 25 for a 3 months summer trip to MIT, USA. After the conclusion of my USA trip, I served in the board of alumini, and as jury member and organizer for 2016 program</p> <p>Position: Alumni, Organizer, Jury.</p> <p> </p>"},{"location":"cv_pdf/#honoractivities","title":"HONOR/ACTIVITIES","text":""},{"location":"cv_pdf/#dream-project-incubator_1","title":"Dream Project Incubator","text":"<p>Jun 2015</p> <p>Dream Project Incubator is a program setup by MIT PhDs and PhD candidates in collaboration with Boston Global Forum. I was awarded a 3 months stay in Boston for my work with FabLab USTH and FabLab Vietnam Foundation, and the chances to meet and confer with top minds from MIT and Harvard University.</p> <p> </p>"},{"location":"cv_pdf/#3rd-prize-national-gifted-students-competition-in-informatics","title":"3rd prize National gifted students\u2019 competition in Informatics","text":"<p>2007, 2008 and 2009</p> <p>Held by Vietnam Ministry of Information Technology (now Ministry of Information and Communication)</p> <p> </p>"},{"location":"cv_pdf/#skills","title":"SKILLS","text":"<ul> <li>Mathematics   Linear Algebra; Graph Theory; Numerical and Combinatorial Optimization; Algorithms Design and Computational Complexity Theory.</li> <li>Sofware Engineering   Python 3.0; C, C++, C#; Java; NodeJS; R, Matlab, Tensorflow; Ruby on Rails; Arduino and Processing.</li> <li>Hardware, Engineering, Design and Fabrication   CAD Design (CATIA, Solidwork, Fusion 360); Metal Working; CNC machining; 3D printing; Electronics.</li> <li>Language<ul> <li>English   \u2003IELTS Academic 7.5 (as of 2018)   \u2003Completed my Bachelor program in English</li> <li>Vietnamese   \u2003First language</li> <li>French   \u2003Beginner</li> </ul> </li> </ul>"},{"location":"Log/Nov%2028/","title":"Nov 28 2017","text":""},{"location":"Log/Nov%2028/#meeting-with-master-bio-students","title":"Meeting with master bio students","text":"<ol> <li> <p>Python:</p> <ul> <li>Basic data structure<ul> <li>Binary tree</li> <li>B tree</li> <li>List, Linked list</li> <li>Object oriented</li> </ul> </li> <li>Confident with the language: Some people can work with the language, need test on the problem solving skill and data structure analysis</li> <li>Problem statement:</li> <li>Technical Problem<ul> <li>Query data from different databases, in different domaians</li> <li>Databases have different data types and data structure</li> <li>Need to combine the data</li> </ul> </li> <li> <p>Biological Problem</p> <p>Gene function analyses (Rice genenome)</p> <pre><code>    * Look for the gene match the phenotype\n</code></pre> </li> </ul> </li> </ol>"},{"location":"Log/Nov%2028/#seminar","title":"Seminar","text":"<ol> <li>Heterogeneity of databases</li> </ol>"},{"location":"Scientific%20Journey/Sequence%20visualization%20with%20matplotlib/","title":"Visualizing sequence data with Matplotlib","text":""},{"location":"Scientific%20Journey/Sequence%20visualization%20with%20matplotlib/#the-dirty-way-1","title":"The dirty way 1","text":"<p>There are probably many packages that take multiple sequences alignment and draw a pretty nice little visualization. However as a software engineer and bioinformatician, our time is probably better spend on algorithm than learning new packages, especially when it pretty one off. Furthermore, sometime it is important to have more control over our graph, including alignning elements, incorporate different metrics...</p> <p>In this tutorial we will try to visualize a short aa sequence (75 residues) with conservation calculated by norm distance to uniform distribution (it's not the best way to calculate conservation but... it's quick and dirty). Finally we display a concensus sequence, which is the most relevant amino acid at each residue. Again this is not exactly biologically correct, but that beside the point. You should be able quickly adapt this script to work on different data and different metrics.</p>"},{"location":"Scientific%20Journey/Sequence%20visualization%20with%20matplotlib/#the-idea","title":"The idea","text":"<p>The idea behind this notebook is as we plot statistic at each residue, we draw the sequence directly underneath it. The reason this method even work is matplotlib draw on the figure in \"ticks space\", as in the distance on the plot is calculated in data steps.</p> <p>So if we have a bar graph, we can quickly align vertically a text object simply by putting <code>plot.text()</code> at exactly x. Conversely of y, we need to calculate an offset from the bottom of the graph to the position we want to draw our text/residue/nucleotide acid. That simple.</p>"},{"location":"Scientific%20Journey/Sequence%20visualization%20with%20matplotlib/#prequisite","title":"Prequisite","text":"<ol> <li>The necessary libraries</li> </ol> <p>We need the following libraries: matplotlib (duh), BioPython and NumPy. BioPython is needed to effectively manage our MSA, although with parser and hard work you can read directly from fasta file. NumPy is needed to interface with matplotlib, and in general, who would not include NumPy in their tutorial these day</p> <pre><code>import numpy as np, Bio, matplotlib.pyplot as plot\nfrom matplotlib.ticker import FormatStrFormatter\nfrom Bio import SeqIO\n\n%matplotlib inline\n</code></pre> <ol> <li>In order to display the sequence in a format we familiar with we need 2 things. First, because we intent to visualize our sequences in a sequence of boxes, each box represent a residue and we want the boxes to be align, we need a monospacing font family</li> </ol> <pre><code>font = {'family': 'monospace',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 16,\n        }\n</code></pre> <p>Secondly we need a color palette. There are 20 amino acid and one color for gap, which is displayed as a dash. The color I use here is a color palette used by RasMol package (holy f it's old)</p> <pre><code>palette = [\n    '#C8C8C8', '#145AFF', '#00DCDC', '#E60A0A', '#E6E600',\n    '#00DCDC', '#E60A0A', '#EBEBEB', '#8282D2', '#0F820F', \n    '#0F820F', '#145AFF', '#E6E600', '#3232AA', '#DC9682', \n    '#FA9600', '#FA9600', '#B45AB4', '#3232AA', '#0F820F', \n    '#FFFFFF']\n</code></pre> <p>Finally we have a string of all possible amino acid + the gap</p> <pre><code>aa = 'ARNDCQEGHILKMFPSTWYV-'\n</code></pre>"},{"location":"Scientific%20Journey/Sequence%20visualization%20with%20matplotlib/#reading-stuff","title":"Reading stuff...","text":"<p>First thing first: we read our MSA with the most basic numbers: length and number of sequences</p> <pre><code>fasta_file = 'src/Inputs/example/1atzA.fas'\nmsa = list(SeqIO.parse(fasta_file,'fasta'))\nL = len(msa[0].seq)\nN = len(msa)\n</code></pre> <p>We calculate the frequency of each amino acid occurs at each residue next. We also calucate the concensus sequence at this stage</p> <pre><code>freq = np.zeros([L,21])\nconcensus = np.zeros(L)\nfor i in range(0,N):\n    for j in range(0,L):\n        j_aa = aa.find(msa[i].seq[j])\n        freq[j,j_aa] = freq[j,j_aa] + 1\nfor i in range(0, L):\n    concensus[i] = freq[i].argmax()\n</code></pre> <p>The algorithm is simple. </p> <p>Next come the conservation. The formular for conservation is:  </p> <p>For  is the conservation value of  residue.</p> <p>Why 0.05? Because it's 1/20, and guess how many amino acid there is.</p> <p>(of course there are 21 possible values but this function wrong enough lol)</p> <pre><code>conservation = np.sqrt(np.sum((np.square(freq/N - 0.05)),axis=1))\n</code></pre>"},{"location":"Scientific%20Journey/Sequence%20visualization%20with%20matplotlib/#now-the-fun-part-actually-drawing-the-msa","title":"Now, the fun part: Actually drawing the MSA.","text":"<p>Because there is no native way to draw all the thing that go into this visualization, it look a bit messy. And also because all of this is quick, dirty, this will probably not the best way to do thing. But it's a way.</p> <p>First, we create  the figure with its axes. The figure is configured to be ultra wide, because the sequences are long wink wink.</p> <pre><code>figure = plot.figure(figsize=(20,2))\naxes = plot.axes([0,0,1,1]);\nplot.close()\n</code></pre> <p>Now, to draw the conservation graph. It's a simple bargraph which display at each residue what is the conservation rate according to the formular provided previously.</p> <p>Reminder: to call bargraph: <code>axes.bar(range,data,**args)</code>. It is also important to have <code>align='edge', linewidth=0</code> for alignment purpose. <code>align='edge'</code> make the bar aligned on the right of the ticks, and <code>linewidth=0</code> make the spacing between ticks correct.</p> <pre><code>axes.bar(range(0,L),conservation, align='edge', linewidth = 0, color = 'red')\naxes.set_ylabel('Conservation')\n</code></pre> <p></p> <p>Next step we draw the sequences. We choose randomly 5 sequences from the MSA, because to display all 1000 MSA of this dataset is simply ... unrealistic. We also calculate the spacing between each sequences, as well as spacing between the bottom of the conservation graph to the first sequence displayed.</p> <p>The spacing scale is calculated by taking the limit of y and divided by 6. Why? Because it work for our particular font and spacing...</p> <pre><code>spacing_scale = axes.get_ylim()[1]/6\nspacing = spacing_scale*2\n\nseq_display = np.sort(np.random.randint(0,N,[5]))\n</code></pre> <p>So now, where to draw our sequence? For each sequence we calculate the y-offset from  position in data space. Then the x position of each residue is simply its position in the sequence... Prety neat indeed.</p> <p>The following code block go like: For each sequence, calculate the basic position (0,posit), where posit = the place of sequence times the scale minus an offset. <code>axes.text()</code> is called to draw the sequence title. Finally, each residue is draw by <code>axes.text()</code> with the x position is the position of the residue itself, and a bbox draw over the text. We feed the bbox the color according to the palette, and an <code>alpha=0.5</code> so it's see through.</p> <p>Oh, and remember to add <code>fontdict=font</code> for our monospacing font set</p> <pre><code>for j in seq_display:\n    posit = -float(np.where(seq_display == j)[0]) * spacing_scale - spacing\n    axes.text(-5,posit, \"Seq \"+(str(j+1)))\n    for i in range(0, L):\n        axes.text(float(i),posit, msa[j].seq[i],\n            bbox=dict(facecolor=palette[aa.find(msa[j].seq[i])], \n            alpha=0.5),fontdict=font)\n</code></pre> <p></p> <p>Finally we add the concensus sequence:</p> <pre><code>posit = posit - spacing\naxes.text(-5,posit, \"Concensus\")\nfor i in range(0, L):\n    axes.text(float(i),posit, 'ARNDCQEGHILKMFPSTWYV-'[int(concensus[i])] ,\n                bbox=dict(facecolor=palette[int(concensus[i])], \n                alpha=0.5),fontdict=font)\n</code></pre> <p></p> <p>Et voila! There you go, a quick and dirty way to visualize your sequence dataset. The code is (hopefully) simple enough so you can expanse on your own to other type of biological sequence data (Genomics, Trans...), and different graph can also be used as basis for visualizing your MSA. This, however, is not scalable to larger sequences. You will have to experiment on how best to show your data, but that's the fun of bioinfomatics! And now you have all the controls to do so.</p> <ol> <li> <p>The original notebook can be found here \u21a9</p> </li> </ol>"},{"location":"Scientific%20Journey/Sequence%20visualization%20with%20matplotlib_vi/","title":"Hi\u1ec3n th\u1ecb d\u1eef li\u1ec7u chu\u1ed7i v\u1edbi matplotlib","text":""},{"location":"Scientific%20Journey/Sequence%20visualization%20with%20matplotlib_vi/#the-dirty-way-1","title":"The dirty way 1","text":"<p>C\u00f3 l\u1ebd \u0111\u00e3 c\u00f3 r\u1ea5t nhi\u1ec1u package Python cho ph\u00e9p hi\u1ec3n th\u1ecb d\u1eef li\u1ec7u Multiple Sequence Alignment (MSA). Tuy v\u1eady, th\u1eddi gian c\u1ee7a k\u1ef9 s\u01b0 ph\u1ea7n m\u1ec1m v\u00e0 nh\u00e0 nghi\u00ean c\u1ee9u tin sinh h\u1ecdc c\u00f3 l\u1ebd n\u00ean d\u00e0nh \u0111\u1ec3 suy ngh\u0129 v\u1ec1 thu\u1eadt to\u00e1n v\u00e0 ph\u00e1t tri\u1ec3n ph\u1ea7n m\u1ec1m ri\u00eang h\u01a1n l\u00e0 ng\u1ed3i h\u1ecdc th\u00eam m\u1ed9t v\u00e0i package m\u1edbi. \u0110\u00f4i khi kh\u00e1c th\u00ec c\u00e1c package ch\u01b0a ch\u1eafc \u0111\u00e3 hi\u1ec3n th\u1ecb \u0111\u00fang metrics v\u00e0 \u0111\u1ecbnh d\u1ea1ng m\u00e0 ta mong mu\u1ed1n. Trong notebook n\u00e0y m\u00ecnh s\u1ebd th\u1eed hi\u1ec3n th\u1ecb m\u1ed9t dataset MSA ng\u1eafn (75 residues) v\u00e0 m\u1ed9t bi\u1ec3u \u0111\u1ed3 c\u1ed9t cho th\u1ea5y t\u00ednh b\u1ea3o to\u00e0n c\u1ee7a t\u1eebng residue s\u1eed d\u1ee5ng matplotlib. Ph\u00e9p t\u00ednh \u0111\u1ed9 b\u1ea3o to\u00e0n n\u00e0y kh\u00f4ng \u0111\u00fang l\u1eafm, nh\u01b0ng \u0111i\u1ec1u \u0111\u00f3 kh\u00f4ng qu\u00e1 quan tr\u1ecdng. Cu\u1ed1i c\u00f9ng ch\u00fang ta c\u0169ng hi\u1ec3n th\u1ecb m\u1ed9t chu\u1ed7i \u0111\u1ed3ng thu\u1eadn, chu\u1ed7i n\u00e0y l\u00e0 t\u1eebng amino acid c\u00f3 t\u1ec9 l\u1ec7 cao nh\u1ea5t \u1edf m\u1ed7i residue - m\u1ed9t l\u1ea7n n\u1eefa, \u0111i\u1ec1u n\u00e0y c\u00f3 th\u1ec3 kh\u00f4ng ch\u00ednh x\u00e1c v\u1ec1 m\u1eb7t sinh h\u1ecdc, nh\u01b0ng c\u00e1c b\u1ea1n c\u00f3 th\u1ec3 thay \u0111\u1ed5i \u0111\u1ec3 cho ph\u00f9 h\u1ee3p h\u01a1n v\u1edbi d\u1eef li\u1ec7u v\u00e0 th\u1ed1ng k\u00ea c\u1ee7a ri\u00eang m\u00ecnh.</p>"},{"location":"Scientific%20Journey/Sequence%20visualization%20with%20matplotlib_vi/#y-tuong","title":"\u00dd t\u01b0\u1edfng","text":"<p>\u00dd t\u01b0\u1edfng \u0111\u1eb1ng sau bi\u1ec3u \u0111\u1ed3 n\u00e0y l\u00e0 d\u1ef1a v\u00e0o bi\u1ec3u \u0111\u1ed3 th\u1ed1ng k\u00ea ch\u00ednh, ch\u00fang ta l\u1ea7n l\u01b0\u1ee3t \u0111\u1eb7t t\u1eebng residue v\u00e0o v\u1ecb tr\u00ed c\u1ee7a n\u00f3 d\u01b0\u1edbi c\u00e1c c\u1ed9t bi\u1ec3u \u0111\u1ed3. B\u1edfi v\u00ec matplotlib x\u00e1c \u0111\u1ecbnh v\u1ecb tr\u00ed c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng trong bi\u1ec3u \u0111\u1ed3 c\u1ee7a m\u00ecnh qua \"data space\" hay c\u00e1c m\u1ed1c v\u1ecb tr\u00ed d\u1ef1a tr\u00ean d\u1eef li\u1ec7u, ch\u00fang ta c\u00f3 th\u1ec3 \"hack\" n\u00f3 \u0111\u1ec3 gi\u00f3ng h\u00e0ng c\u00e1c residue c\u1ee7a m\u00ecnh. \u0110\u01a1n gi\u1ea3n v\u1ea7y thui.</p>"},{"location":"Scientific%20Journey/Sequence%20visualization%20with%20matplotlib_vi/#yeu-cau","title":"Y\u00eau c\u1ea7u","text":"<ol> <li>Th\u01b0 vi\u1ec7n</li> </ol> <p>Ta c\u1ea7n c\u00e1c th\u01b0 vi\u1ec7n sau: matplotlib (duh), BioPython v\u00e0 NumPy. BioPython d\u00f9ng \u0111\u1ec3 qu\u1ea3n l\u00fd, \u0111\u1ecdc v\u00e0 ghi c\u00e1c MSA, d\u00f9 ta c\u0169ng c\u00f3 th\u1ec3 t\u1ef1 vi\u1ebft parser v\u00e0 \u0111\u1ecdc file FASTA tr\u1ef1c ti\u1ebfp - nh\u01b0ng m\u00e0 l\u01b0\u1eddi :D. NumPy d\u00f9ng \u0111\u1ec3 t\u00ednh to\u00e1n v\u00e0 t\u01b0\u01a1ng t\u00e1c v\u1edbi matplotlib, ngo\u00e0i ra th\u1eddi n\u00e0y ai m\u00e0 ch\u1eb3ng d\u00f9ng numpy khi vi\u1ebft tutorial tr\u00ean m\u1ea1ng c\u01a1 ch\u1ee9...</p> <pre><code>import numpy as np, Bio, matplotlib.pyplot as plot\nfrom matplotlib.ticker import FormatStrFormatter\nfrom Bio import SeqIO\n\n%matplotlib inline\n</code></pre> <ol> <li>\u0110\u1ec3 v\u1ebd m\u1ed9t c\u00e1ch ch\u00ednh x\u00e1c, tr\u01b0\u1edbc h\u1ebft ta c\u1ea7n \u0111\u1ecbnh ngh\u0129a v\u00e0i th\u1ee9. Tr\u01b0\u1edbc ti\u00ean, ch\u00fang ta c\u1ea7n m\u1ed9t h\u1ecd font monospace \u0111\u1ec3 \u0111\u1ea3m b\u1ea3o c\u00e1c residue s\u1ebd \u0111\u01b0\u1ee3c th\u1eb3ng h\u00e0ng. Monospace font c\u00f3 kho\u1ea3ng c\u00e1ch b\u1eb1ng nhau b\u1ea5t k\u1ec3 k\u00fd t\u1ef1 - ch\u1eef A v\u00e0 ch\u1eef I \u0111\u1ec1u t\u1ed1n di\u1ec7n t\u00edch nh\u01b0 nhau. D\u1ec5 hi\u1ec3u t\u1ea1i sao ch\u00fang ta c\u1ea7n font n\u00e0y ph\u1ea3i kh\u00f4ng.</li> </ol> <pre><code>font = {'family': 'monospace',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 16,\n        }\n</code></pre> <p>Ti\u1ebfp theo ch\u00fang ta c\u1ea7n m\u1ed9t c\u00e1i b\u1ea3ng m\u00e0u. C\u00f3 20 amino acid, v\u00e0 c\u1ea7n th\u00eam 1 m\u00e0u \u0111\u1ec3 hi\u1ec3n th\u1ecb gap. B\u1ea3ng m\u00e0u m\u00ecnh s\u1eed d\u1ee5ng \u1edf \u0111\u00e2y c\u0169ng l\u00e0 b\u1ea3ng m\u00e0u \u0111\u01b0\u1ee3c RasMol s\u1eed d\u1ee5ng (c\u00e1i ph\u1ea7n m\u1ec1m n\u00e0y x\u01b0a nh\u01b0 tr\u00e1i \u0111\u1ea5t r\u1ed3i holy f)</p> <pre><code>palette = [\n    '#C8C8C8', '#145AFF', '#00DCDC', '#E60A0A', '#E6E600',\n    '#00DCDC', '#E60A0A', '#EBEBEB', '#8282D2', '#0F820F', \n    '#0F820F', '#145AFF', '#E6E600', '#3232AA', '#DC9682', \n    '#FA9600', '#FA9600', '#B45AB4', '#3232AA', '#0F820F', \n    '#FFFFFF']\n</code></pre> <p>Cu\u1ed1i c\u00f9ng ta c\u00f3 m\u1ed9t string ch\u1ee9a t\u1ea5t c\u1ea3 c\u00e1c amino acid c\u00f3 th\u1ec3 nh\u1eadn, v\u00e0 kho\u1ea3ng tr\u1ed1ng</p> <pre><code>aa = 'ARNDCQEGHILKMFPSTWYV-'\n</code></pre>"},{"location":"Scientific%20Journey/Sequence%20visualization%20with%20matplotlib_vi/#oc-oc-oc","title":"\u0110\u1ecdc \u0111\u1ecdc \u0111\u1ecdc...","text":"<p>Vi\u1ec7c \u0111\u1ea7u ti\u00ean, \u0111\u01b0\u01a1ng nhi\u00ean l\u00e0 \u0111\u1ecdc c\u1ea3 chu\u1ed7i MSA v\u00e0 t\u00ednh to\u00e1n m\u1ed9t s\u1ed1 th\u00f4ng s\u1ed1 c\u01a1 b\u1ea3n: Chi\u1ec1u d\u00e0i chu\u1ed7i L v\u00e0 s\u1ed1 chu\u1ed7i N.</p> <pre><code>fasta_file = 'src/Inputs/example/1atzA.fas'\nmsa = list(SeqIO.parse(fasta_file,'fasta'))\nL = len(msa[0].seq)\nN = len(msa)\n</code></pre> <p>T\u00ednh to\u00e1n t\u1ea7n su\u1ea5t t\u1eebng amino acid \u1edf m\u1ed7i residue, v\u00e0 nh\u00e2n ti\u1ec7n t\u00ednh lu\u00f4n chu\u1ed7i \u0111\u1ed3ng thu\u1eadn.</p> <pre><code>freq = np.zeros([L,21])\nconcensus = np.zeros(L)\nfor i in range(0,N):\n    for j in range(0,L):\n        j_aa = aa.find(msa[i].seq[j])\n        freq[j,j_aa] = freq[j,j_aa] + 1\nfor i in range(0, L):\n    concensus[i] = freq[i].argmax()\n</code></pre> <p>Ch\u1eafc \u0111o\u1ea1n n\u00e0y d\u1ec5 ha. </p> <p>\u0110\u1ec3 t\u00ednh to\u00e1n \u0111\u1ed9 b\u1ea3o to\u00e0n, ch\u00fang ta d\u00f9ng c\u00f4ng th\u1ee9c nh\u01b0 sau:</p> <p> </p> <p>v\u1edbi  l\u00e0 \u0111\u1ed9 b\u1ea3o to\u00e0n c\u1ee7a residue th\u1ee9 .</p> <p>T\u1ea1i sao l\u1ea1i l\u00e0 0.05? B\u1edfi v\u00ec n\u00f3 l\u00e0 , v\u00e0 \u1edf th\u00ec c\u00f3 20 amino acid. Kho\u1ea3ng tr\u1ed1ng kh\u00f4ng t\u00ednh... (t\u1ea5t nhi\u00ean l\u00e0 c\u00f3 21 gi\u00e1 tr\u1ecb c\u00f3 th\u1ec3 nh\u1eadn \u0111\u01b0\u1ee3c nh\u01b0ng m\u00e0 c\u00e1i note n\u00e0y \u0111\u1ee7 b\u1eady r\u1ed3i lmao)</p> <pre><code>conservation = np.sqrt(np.sum((np.square(freq/N - 0.05)),axis=1))\n</code></pre>"},{"location":"Scientific%20Journey/Sequence%20visualization%20with%20matplotlib_vi/#ok-en-oan-vui","title":"OK \u0111\u1ebfn \u0111o\u1ea1n vui","text":"<p>B\u1edfi v\u00ec kh\u00f4ng c\u00f3 c\u00e1ch n\u00e0o c\u00f3 s\u1eb5n \u0111\u1ec3 v\u1ebd c\u1ea3 m\u1edb chu\u1ed7i n\u00e0y v\u00e0o \u0111\u1ed3 th\u1ecb, ch\u1ed7 code d\u01b0\u1edbi n\u00e0y nh\u00ecn s\u1ebd kh\u00e1 r\u1ed1i r\u1eafm. \u0110\u1ed3ng th\u1eddi n\u00f3 c\u0169ng ch\u01b0a ch\u1eafc l\u00e0 c\u00e1ch ngon nh\u1ea5t \u0111\u1ec3 l\u00e0m (ch\u1eafc kh\u00f4ng ph\u1ea3i \u0111\u00e2u)... nh\u01b0ng n\u00f3 l\u00e0 m\u1ed9t c\u00e1ch</p> <p>Tr\u01b0\u1edbc ti\u00ean, t\u1ea5t nhi\u00ean ta t\u1ea1o m\u1ed9t \u0111\u1ed3 th\u1ecb v\u00e0 c\u00e1c tr\u1ee5c t\u01b0\u01a1ng \u1ee9ng. C\u00e1i \u0111\u1ed3 th\u1ecb n\u00e0y \u0111\u01b0\u1ee3c thi\u1ebft l\u1eadp kh\u00e1 r\u1ed9ng, v\u00ec c\u00e1i chu\u1ed7i nh\u00e9t v\u00e0o n\u00f3 d\u00e0ii wink wink.</p> <pre><code>figure = plot.figure(figsize=(20,2))\naxes = plot.axes([0,0,1,1]);\nplot.close()\n</code></pre> <p>OK gi\u1edd \u0111\u1ebfn l\u00fac v\u1ebd c\u00e1i \u0111\u1ed3 th\u1ecb v\u1ec1 \u0111\u1ed9 b\u1ea3o to\u00e0n. n\u00f3 \u0111\u01a1n gi\u1ea3n ch\u1ec9 l\u00e0 m\u1ed9t c\u00e1i bar graph cho th\u1ea5y \u0111\u1ed9 b\u1ea3o to\u00e0n \u1edf t\u1eebng residue.</p> <p>Reminder: \u0111\u1ec3 g\u1ecdi bargraph: <code>axes.bar(range,data,**args)</code>. Trong tr\u01b0\u1eddng h\u1ee3p n\u00e0y ch\u00fang ta c\u1ea7n c\u00e1c parameters <code>align='edge', linewidth=0</code> \u0111\u1ec3 gi\u00f3ng h\u00e0ng c\u00e1c amino acid. <code>align='edge'</code> l\u00e0m cho c\u00e1c c\u1ed9t gi\u00f3ng h\u00e0ng \u1edf c\u1ea1nh tr\u00e1i, v\u00e0 <code>linewidth=0</code> l\u00e0m cho kho\u1ea3ng c\u00e1ch c\u00e1c c\u1ed9t kh\u00edt h\u01a1n.</p> <pre><code>axes.bar(range(0,L),conservation, align='edge', linewidth = 0, color = 'red')\naxes.set_ylabel('Conservation')\n</code></pre> <p></p> <p>Gi\u1edd \u0111\u1ebfn l\u00fac v\u1ebd c\u00e1c chu\u1ed7i. Ch\u00fang ta l\u1ea5y ra 5 chu\u1ed7i ng\u1eabu nhi\u00ean t\u1eeb MSA, \u0111\u01a1n gi\u1ea3n v\u00ec v\u1edbi 1000 chu\u1ed7i th\u00ec data c\u1ee7a m\u00ecnh qu\u00e1 r\u1ed9ng \u0111\u1ec3 c\u00f3 th\u1ec3 v\u1ebd t\u1ea5t c\u1ea3. \u0110\u1ed3ng th\u1eddi ch\u00fang ta t\u00ednh to\u00e1n kho\u1ea3ng c\u00e1ch gi\u1eefa c\u00e1c chu\u1ed7i, c\u0169ng nh\u01b0 kho\u1ea3ng c\u00e1ch t\u1eeb \u0111\u00e1y \u0111\u1ed3 th\u1ecb \u0111\u1ebfn chu\u1ed7i \u0111\u1ea7u ti\u00ean.</p> <p>Kho\u1ea3ng c\u00e1ch gi\u1eefa c\u00e1c chu\u1ed7i \u0111\u01b0\u1ee3c t\u00ednh b\u1eb1ng c\u1eadn c\u1ee7a tr\u1ee5c y chia 6. T\u1ea1i sao? V\u00ec chia 6 n\u00f3 kh\u1edbp h\u00e0ng \ud83d\ude05, kh\u00f4ng c\u00f3 gi\u1ea3i th\u00edch n\u00e0o hay h\u01a1n c\u1ea3 ...</p> <pre><code>spacing_scale = axes.get_ylim()[1]/6\nspacing = spacing_scale*2\n\nseq_display = np.sort(np.random.randint(0,N,[5]))\n</code></pre> <p>Ok, gi\u1edd th\u00ec l\u00e0m sao m\u00e0 v\u1ebd?</p> <p>V\u1edbi m\u1ed7i chu\u1ed7i, tr\u01b0\u1edbc ti\u00ean ta t\u00ednh to\u00e1n kho\u1ea3ng c\u00e1ch \u0111\u1ebfn g\u1ed1c t\u1ecda \u0111\u1ed9 . V\u1ecb tr\u00ed tr\u1ee5c x c\u1ee7a t\u1eebng residue \u0111\u01b0\u1ee3c t\u00ednh to\u00e1n \u0111\u01a1n gi\u1ea3n l\u00e0 v\u1ecb tr\u00ed c\u1ee7a ch\u00ednh n\u00f3 trong chu\u1ed7i. Kh\u00e1 g\u1ecdn. Ti\u00eau \u0111\u1ec1 c\u1ee7a t\u1eebng chu\u1ed7i \u0111\u01b0\u1ee3c v\u1ebd d\u1ecbch v\u1ec1 b\u00ean tr\u00e1i b\u1eb1ng c\u00e1ch l\u1ea5y gi\u00e1 tr\u1ecb x \u00e2m, v\u00e0 trong tr\u01b0\u1eddng h\u1ee3p n\u00e0y ta l\u1ea5y l\u00e0 -5.</p> <p><code>axes.text()</code> \u0111\u01b0\u1ee3c g\u1ecdi \u0111\u1ec3 v\u1ebd t\u1eebng residue. V\u1edbi m\u1ed7i residue ch\u00fang ta c\u00f3 m\u1ed9t c\u00e1i bbox v\u1ebd \u0111\u00e8 l\u00ean. C\u00e1c bbox n\u00e0y c\u00f3 <code>alpha=0.5</code> \u0111\u1ec3 n\u00f3  trong su\u1ed1t, v\u00e0 m\u00e0u nh\u01b0 trong palette m\u00e0u \u0111\u00e3 \u0111\u1ecbnh ngh\u0129a \u1edf tr\u00ean.</p> <p>\u00c0 m\u00e0 nh\u1edb d\u00f9ng <code>fontdict=font</code> \u0111\u1ec3 x\u00e0i c\u00e1i font family n\u00e3y nh\u00e9.</p> <pre><code>for j in seq_display:\n    posit = -float(np.where(seq_display == j)[0]) * spacing_scale - spacing\n    axes.text(-5,posit, \"Seq \"+(str(j+1)))\n    for i in range(0, L):\n        axes.text(float(i),posit, msa[j].seq[i],\n            bbox=dict(facecolor=palette[aa.find(msa[j].seq[i])], \n            alpha=0.5),fontdict=font)\n</code></pre> <p></p> <p>Cu\u1ed1i c\u00f9ng, ch\u00fang ta th\u00eam chu\u1ed7i \u0111\u1ed3ng thu\u1eadn:</p> <pre><code>posit = posit - spacing\naxes.text(-5,posit, \"Concensus\")\nfor i in range(0, L):\n    axes.text(float(i),posit, 'ARNDCQEGHILKMFPSTWYV-'[int(concensus[i])] ,\n                bbox=dict(facecolor=palette[int(concensus[i])], \n                alpha=0.5),fontdict=font)\n</code></pre> <p></p> <p>Et voila! \u0110\u00e2y l\u00e0 m\u1ed9t c\u00e1ch nhanh (v\u00e0 b\u1ea9n) \u0111\u1ec3 hi\u1ec3n th\u1ecb dataset. M\u00ecnh hi v\u1ecdng n\u00f3 \u0111\u1ee7 \u0111\u01a1n gi\u1ea3n \u0111\u1ec3 c\u00e1c b\u1ea1n c\u00f3 th\u1ec3 t\u1ef1 bi\u1ec3n \u0111\u1ed5i n\u00f3 cho t\u1eebng lo\u1ea1i d\u1eef li\u1ec7u ri\u00eang (Genomics, transcriptomics...), v\u00e0 c\u0169ng nh\u01b0 d\u00f9ng c\u00e1c lo\u1ea1i \u0111\u1ed3 th\u1ecb kh\u00e1c l\u00e0m n\u1ec1n t\u1ea3ng. Tuy nhi\u00ean, ph\u01b0\u01a1ng ph\u00e1p n\u00e0y c\u00f3 l\u1ebd kh\u00f4ng ph\u1ea3i l\u00e0 c\u00e1ch t\u1ed1t nh\u1ea5t \u0111\u1ec3 hi\u1ec3n th\u1ecb c\u00e1c chu\u1ed7i d\u00e0i h\u01a1n - b\u1ea1n ph\u1ea3i t\u1ef1 t\u00ecm \u0111\u01b0\u1eddng ph\u00f9 h\u1ee3p nh\u1ea5t v\u1edbi d\u1eef li\u1ec7u c\u1ee7a m\u00ecnh. Nh\u01b0ng \u0111\u00f3 l\u00e0 \u0111i\u1ec1u hay ho khi l\u00e0m Bioinfomatics!!! V\u00e0 gi\u1edd th\u00ec b\u1ea1n c\u00f3 to\u00e0n quy\u1ec1n ki\u1ec3m so\u00e1t tr\u00ean qu\u00e1 tr\u00ecnh r\u1ed3i.</p> <ol> <li> <p>Notebook Jupyter g\u1ed1c (ti\u1ebfng Anh) th\u00ec \u1edf \u0111\u00e2y \u21a9</p> </li> </ol>"},{"location":"Scientific%20Journey/Where%20to%20start%20with%20Python/","title":"Where to start with Python","text":"<p>The other day there was another student asked me how to start with infomatics and computer science (and of course, AI). My answer is, of course, with Python.</p> <p>Now, why is Python the perfect language for starter in Comp Sci as well as AI? Simply put, Python got it right between readability, simplicity and performance. When I first started with programming, there was Pascal, C and BASIC, all was heavy hitter and formal programming languages. They are not really readability and friendly, and often it is easier to rewrite than reread - this is still the common practise for small to medium projects. However they are close to native code and can be used to perform time critical and intricate codes. Once we get to a certain proficient with programming, it is always recommended to start learning a \"real\" programming language - the modern languages are C++, Java and of course, Python</p> <p>On the other end of the spectrum, there is Scratch and to some extend, JavaScript and Lua Script. These so called scripting programming language is easier to grasp and eaiser to learn, with less formalism and more abundent </p>"},{"location":"Scientific%20Journey/building-a-pc/","title":"Building your own PC for fun and profit","text":"<p>(or if you want to do some science with it)</p>"},{"location":"Scientific%20Journey/building-a-pc/#rationality","title":"Rationality","text":"<p>Why build your own you may ask. There are plenty great laptop nowadays when it come to performance, especially if you're not either 1. a gamer or 2. a computer scientist/engineer/person. Apple's Macbook new line up is a good example.</p>"},{"location":"Scientific%20Journey/building-a-pc/#prequisite","title":"Prequisite","text":""},{"location":"Scientific%20Journey/day-one-ish/","title":"(Nh\u1eefng) ng\u00e0y \u0111\u1ea7u ti\u00ean","text":"<p>Th\u1eadt ra m\u00ecnh \u0111\u00e3 l\u00e0m trong d\u1ef1 \u00e1n n\u00e0y \u0111\u01b0\u1ee3c \u0111\u1ebfn nay ch\u1eafc c\u0169ng g\u1ea7n 3 tu\u1ea7n r\u1ed3i. Kh\u00f4ng th\u1ebf n\u00f3i l\u00e0 ng\u00e0y \u0111\u1ea7u ti\u00ean \u0111\u01b0\u1ee3c, nh\u1eefng th\u00ec may ra.</p> <p>Trong b\u01b0\u1edbc \u0111\u1ea7u c\u1ee7a d\u1ef1 \u00e1n \u0111ang l\u00e0m, c\u00f4ng vi\u1ec7c ch\u00ednh c\u1ee7a m\u00ecnh l\u00e0 ph\u00e1t tri\u1ec3n m\u1ed9t module/engine Python \u0111\u1ec3 thu th\u1eadp d\u1eef li\u1ec7u t\u1eeb m\u1ed9t v\u00e0i c\u01a1 s\u1edf d\u1eef li\u1ec7u. Vi\u1ec7c n\u00e0y \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n b\u1eb1ng m\u1ed9t s\u1ed1 th\u01b0 vi\u1ec7n c\u01a1 b\u1ea3n c\u1ee7a Python. C\u00e1c th\u01b0 vi\u1ec7n n\u00e0y bao g\u1ed3m pandas (x\u1eed l\u00fd d\u1eef li\u1ec7u d\u1ea1ng b\u1ea3ng), bs4 (x\u1eed l\u00fd d\u1eef li\u1ec7u DOM), requests (th\u1ef1c thi truy v\u1ea5n HTTPS) v\u00e0 json (x\u1eed l\u00fd d\u1eef li\u1ec7u d\u1ea1ng JSON v\u00e0 chuy\u1ec3n h\u00f3a d\u1eef li\u1ec7u d\u1ea1ng t\u1eeb \u0111i\u1ec3n - Dictionary sang d\u1ea1ng JSON tr\u1ea3 v\u1ec1)</p>"},{"location":"Scientific%20Journey/day-one-ish/#phuong-phap-ban-au","title":"Ph\u01b0\u01a1ng ph\u00e1p ban \u0111\u1ea7u","text":"<p>C\u00e1ch truy\u1ec1n th\u1ed1ng \u0111\u1ec3 th\u1ef1c hi\u1ec7n c\u00e1c truy v\u1ea5n n\u00e0y l\u00e0 vi\u1ebft c\u00e1c \u0111o\u1ea1n script \u0111\u1ec3 truy c\u1eadp v\u00e0o c\u00e1c trang web c\u1ee7a c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u n\u00f3i tr\u00ean v\u00e0 l\u1ea5y k\u1ebft qu\u1ea3 tr\u1ea3 v\u1ec1. V\u1edbi ph\u01b0\u01a1ng ph\u00e1p n\u00e0y, m\u1ed7i c\u01a1 s\u1edf d\u1eef li\u1ec7u m\u1edbi s\u1ebd c\u00f3 m\u1ed9t \u0111o\u1ea1n script ri\u00eang \u0111\u1ec3 th\u1ef1c thi truy v\u1ea5n v\u00e0 x\u1eed l\u00fd d\u1eef li\u1ec7u tr\u1ea3 v\u1ec1. Ph\u01b0\u01a1ng ph\u00e1p n\u00e0y kh\u00e1 \u0111\u01a1n gi\u1ea3n v\u00e0 hi\u1ec7u qu\u1ea3 v\u1edbi m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng nh\u1ecf c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u \u00edt li\u00ean h\u1ec7 v\u1edbi nhau. Code c\u0169ng s\u1ebd minh b\u1ea1ch v\u00e0 t\u00ednh bao \u0111\u00f3ng \u0111\u01b0\u1ee3c \u0111\u1ea3m b\u1ea3o.</p> <p>V\u1ea5n \u0111\u1ec1 v\u1edbi ph\u01b0\u01a1ng ph\u00e1p n\u00e0y b\u1eaft \u0111\u1ea7u n\u1ea3y sinh khi m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng l\u1edbn c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u00f3 quan h\u1ec7 ch\u1ed3ng ch\u00e9o c\u1ea7n \u0111\u01b0\u1ee3c x\u1eed l\u00fd. S\u1ebd kh\u00f4ng c\u00f3 c\u00e1ch n\u00e0o hi\u1ec7u qu\u1ea3 \u0111\u1ec3 t\u00ecm hi\u1ec3u m\u1ed1i quan h\u1ec7 gi\u1eefa c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u n\u00e0y, DB n\u00e0o truy v\u1ea5n t\u1eeb \u0111\u00e2u nh\u1eefng th\u00f4ng tin n\u00e0o... L\u01b0\u1ee3ng code tr\u00f9ng l\u1eb7p s\u1ebd tr\u1edf n\u00ean r\u1ea5t l\u1edbn khi \u0111\u1ea1t con s\u1ed1 10 - 12 c\u01a1 s\u1edf d\u1eef li\u1ec7u. Ph\u1ea7n l\u1edbn kh\u1ed1i l\u01b0\u1ee3ng code s\u1ebd l\u00e0 gi\u1ed1ng nhau: Th\u1ef1c hi\u1ec7n c\u00e1c truy v\u1ea5n HTTPS, ph\u00e2n t\u00e1ch d\u1eef li\u1ec7u tr\u1ea3 v\u1ec1, ghi d\u1eef li\u1ec7u v\u00e0o c\u00e1c t\u1eeb \u0111i\u1ec3n c\u1ee7a Python v\u00e0 xu\u1ea5t ch\u00fang ra d\u01b0\u1edbi d\u1ea1ng JSON. Ng\u01b0\u1eddi s\u1eed d\u1ee5ng c\u00e1c module n\u00e0y l\u1ea1i l\u00e0 c\u00e1c nh\u00e0 sinh h\u1ecdc v\u1edbi y\u00eau c\u1ea7u m\u1ed9t giao di\u1ec7n d\u1ec5 d\u00e0ng gi\u1eefa R v\u00e0 Python, trong khi h\u1ec7 th\u1ed1ng n\u00e0y kh\u00f4ng c\u00f3 c\u00e1ch n\u00e0o s\u1eed d\u1ee5ng t\u1eeb kh\u00f3a nh\u01b0 l\u00e0 1 c\u00e1ch \u0111\u1ec3 truy c\u1eadp c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u.</p> <p>C\u00e1c v\u1ea5n \u0111\u1ec1 n\u00e0y \u0111\u00f2i h\u1ecfi t\u00e1i c\u1ea5u tr\u00fac l\u1ea1i module Python sang m\u1ed9t ki\u1ebfn tr\u00fac m\u1edbi m\u1ea1nh v\u00e0 t\u1ed5ng quan h\u01a1n.</p>"},{"location":"Scientific%20Journey/day-one-ish/#ac-ta-co-so-du-lieu","title":"\u0110\u1eb7c t\u1ea3 c\u01a1 s\u1edf d\u1eef li\u1ec7u","text":"<p>\u0110\u1ed1i m\u1eb7t v\u1edbi c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u, m\u1ed9t c\u00e1ch t\u1ef1 nhi\u00ean ta ngh\u0129 \u0111\u1ebfn vi\u1ec7c gom c\u00e1c ph\u1ea7n code tr\u00f9ng l\u1eb7p gi\u1eefa c\u00e1c sript truy v\u1ea5n c\u01a1 s\u1edf d\u1eef li\u1ec7u v\u1edbi nhau th\u00e0nh m\u1ed9t m\u00e1y truy v\u1ea5n (query engine) m\u1ea1nh v\u00e0 t\u1ed5ng quan. C\u00e1c th\u00e0nh ph\u1ea7n li\u00ean quan \u0111\u1ebfn truy v\u1ea5n m\u1ea1ng, HTTPS \u0111\u01b0\u1ee3c h\u1ed7 tr\u1ee3 b\u1edfi th\u01b0 vi\u1ec7n requests ho\u00e0n to\u00e0n l\u00e0 tr\u00f9ng nhau qua c\u00e1c th\u00e0nh ph\u1ea7n, v\u00e0 \u0111\u00e3 \u0111\u01b0\u1ee3c gom l\u1ea1i v\u00e0o module helper - \u0111\u00e2y l\u00e0 c\u00e1ch \u0111\u1eb7t t\u00ean theo quy \u01b0\u1edbc d\u00e0nh cho m\u1ed9t module ch\u1ee9a c\u00e1c h\u00e0m ph\u1ee5 tr\u1ee3, kh\u00f4ng s\u1eed d\u1ee5ng nh\u01b0 m\u1ed9t th\u00e0nh ph\u1ea7n c\u1ee7a c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng. Vi\u1ec7c c\u1ea7n l\u00e0m l\u00e0 vi\u1ebft c\u00e1c exception th\u00f4ng b\u00e1o l\u1ed7i t\u1eeb ph\u00eda server \u0111\u1ec3 ng\u01b0\u1eddi d\u00f9ng c\u00f3 th\u1ec3 ti\u1ebfn h\u00e0nh s\u1eeda ch\u1eefa l\u1ed7i. C\u00e1c th\u00e0nh ph\u1ea7n ph\u00e2n t\u00e1ch d\u1eef li\u1ec7u c\u1ea7n \u0111\u01b0\u1ee3c t\u1ed5 ch\u1ee9c l\u1ea1i ho\u00e0n to\u00e0n \u0111\u1ec3 c\u00f3 th\u1ec3:</p> <ol> <li>Ph\u00e2n gi\u1ea3i d\u1eef li\u1ec7u ch\u00ednh x\u00e1c gi\u1eefa c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u00f3 c\u00f9ng c\u00f4ng ngh\u1ec7 tr\u1ea3 v\u1ec1,</li> <li>Chuy\u1ec3n \u0111\u1ed5i ch\u1ebf \u0111\u1ed9 ph\u00e2n t\u00e1ch d\u1eef li\u1ec7u gi\u1eefa c\u00e1c c\u00f4ng ngh\u1ec7 kh\u00e1c nhau.</li> </ol> <p>\u0110\u1ec3 ph\u00e2n bi\u1ec7t gi\u1eefa c\u00e1c ch\u1ebf \u0111\u1ed9 ph\u00e2n t\u00e1ch kh\u00e1c nhau \u0111\u00f2i h\u1ecfi c\u1ea7n c\u00f3 m\u1ed9t ph\u01b0\u01a1ng ph\u00e1p \u0111\u1ec3 ph\u00e2n bi\u1ec7t gi\u1eefa c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u, ph\u00e2n t\u00edch v\u00e0 ph\u00e2n lo\u1ea1i ch\u00fang v\u00e0o c\u00e1c nh\u00f3m kh\u00e1c nhau. \u0110i\u1ec1u n\u00e0y c\u0169ng c\u00f3 l\u1ee3i cho c\u00e1c truy v\u1ea5n HTTPS, v\u00ec ch\u00fang ta c\u1ea7n ph\u1ea3i c\u00f3 c\u00e1c \u0111\u01b0\u1eddng d\u1eabn \u0111\u1ebfn c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u. C\u00e1c truy v\u1ea5n HTTPS c\u0169ng \u0111\u01b0\u1ee3c ph\u00e2n lo\u1ea1i th\u00e0nh 2 method: POST v\u00e0 GET, \u0111i\u1ec1u n\u00e0y c\u0169ng c\u1ea7n \u0111\u01b0\u1ee3c ghi ch\u00fa l\u1ea1i v\u00e0 chuy\u1ec3n \u0111\u1ed5i gi\u1eefa 2 ch\u1ebf \u0111\u1ed9.</p> <p>B\u00e0i to\u00e1n \u0111\u1eb7t ra tr\u01b0\u1edbc m\u1eaft \u0111\u00f2i h\u1ecfi thi\u1ebft k\u1ebf m\u1ed9t c\u01a1 s\u1edf d\u1eef li\u1ec7u ph\u1ee5 tr\u1ee3, l\u01b0u tr\u1eef th\u00f4ng tin v\u1ec1 t\u1ea5t c\u1ea3 c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u00f3 trong truy v\u1ea5n. \u0110\u00e2y l\u00e0 m\u1ed9t d\u1ea1ng si\u00eau d\u1eef li\u1ec7u (metadata). T\u1eeb kh\u00f3a n\u00e0y c\u00f3 v\u1ebb cao si\u00eau, nh\u01b0ng th\u1ef1c t\u1ebf ch\u00fang kh\u00e1 \u0111\u01a1n gi\u1ea3n. Ch\u00fang ch\u1ee9a c\u00e1c \u0111\u1eb7c t\u1ea3 v\u1ec1 c\u1ea5u tr\u00fac, c\u00e1c th\u00f4ng tin c\u1ea7n thi\u1ebft v\u00e0 li\u00ean quan \u0111\u1ebfn c\u01a1 s\u1edf d\u1eef li\u1ec7u ch\u00fang ta \u0111ang c\u1ea7n truy v\u1ea5n.</p> <p>M\u00ecnh kh\u00f4ng mu\u1ed1n c\u00f4ng b\u1ed1 \u0111\u1eb7c t\u1ea3 n\u00e0y ra trong b\u00e0i n\u00e0y, v\u00ec \u0111\u00e3 \u0111\u1ee7 n\u1eb7ng v\u1ec1 k\u1ef9 thu\u1eadt. Tuy v\u1eady m\u1ed9t \u0111i\u1ec1u c\u1ea7n \u0111\u1eb7c bi\u1ec7t v\u1edbi vi\u1ec7c s\u1eed d\u1ee5ng c\u00e1c metadata n\u00e0y l\u00e0 n\u00f3 cho ph\u00e9p ta m\u1edf r\u1ed9ng s\u1ed1 l\u01b0\u1ee3ng c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u00f3 th\u1ec3 truy v\u1ea5n l\u00ean theo c\u1ea5p s\u1ed1 c\u1ed9ng. M\u1ed7i c\u01a1 s\u1edf d\u1eef li\u1ec7u m\u1edbi gi\u1edd ch\u1ec9 c\u1ea7n vi\u1ebft c\u00e1c m\u00f4 t\u1ea3 n\u00e0y \u0111\u1ec3 c\u00f3 th\u1ec3 truy c\u1eadp theo t\u00ean v\u00e0 c\u00e1c th\u00f4ng s\u1ed1 truy v\u1ea5n ph\u00f9 h\u1ee3p. \u0110i\u1ec1u n\u00e0y th\u1ecfa m\u00e3n c\u00e1c y\u00eau c\u1ea7u v\u1ec1 t\u00ednh t\u1ed5ng quan c\u1ee7a engine truy v\u1ea5n d\u1eef li\u1ec7u \u0111ang \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng. \u0110\u00e2y c\u0169ng l\u00e0 l\u00fd do t\u1ea1i sao trong ng\u00e0nh h\u1ecdc m\u00e1y c\u00f3 nh\u1eefng n\u1ed9i dung r\u1ea5t quan tr\u1ecdng v\u1ec1 khai ph\u00e1 si\u00eau d\u1eef li\u1ec7u.</p>"},{"location":"Scientific%20Journey/day-one-ish/#cau-truc-toi-uu","title":"C\u1ea5u tr\u00fac t\u1ed1i \u01b0u","text":"<p>C\u00f4ng vi\u1ec7c thi\u1ebft k\u1ebf \u0111\u1eb7c t\u1ea3 c\u01a1 s\u1edf d\u1eef li\u1ec7u ph\u1ea3i \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n song song v\u1edbi qu\u00e1 tr\u00ecnh thi\u1ebft k\u1ebf ki\u1ebfn tr\u00fac m\u00e1y truy v\u1ea5n. M\u1ed7i ph\u1ea7n c\u1ee7a c\u1ea5u tr\u00fac \u0111\u1eb7c t\u1ea3 \u0111\u01b0\u1ee3c vi\u1ebft ra kh\u1edbp v\u1edbi gi\u1ea3i thu\u1eadt t\u01b0\u01a1ng \u1ee9ng c\u1ee7a m\u00e1y truy v\u1ea5n.</p> <p>Hi\u1ec7n t\u1ea1i c\u1ea5u tr\u00fac m\u00e1y truy v\u1ea5n \u1ed5n \u0111\u1ecbnh \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n theo gi\u1ea3i thu\u1eadt sau:</p> <ol> <li>Nh\u1eadn chu\u1ed7i t\u00ean c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u1ea7n truy v\u1ea5n</li> <li>Truy xu\u1ea5t c\u01a1 s\u1edf d\u1eef li\u1ec7u t\u01b0\u01a1ng \u1ee9ng v\u1edbi DOM cung c\u1ea5p b\u1edfi bs4</li> <li>Truy v\u1ea5n c\u01a1 s\u1edf d\u1eef li\u1ec7u th\u00f4ng qua \u0111\u01b0\u1eddng d\u1eabn l\u01b0u tr\u1eef trong \u0111\u1eb7c t\u1ea3</li> <li>X\u00e1c \u0111\u1ecbnh lo\u1ea1i d\u1eef li\u1ec7u tr\u1ea3 v\u1ec1 - \u0111\u01b0\u1ee3c l\u01b0u l\u1ea1i tr\u01b0\u1edbc trong \u0111\u1eb7c t\u1ea3</li> <li>Ph\u00e2n gi\u1ea3i d\u1eef li\u1ec7u theo c\u1ea5u h\u00ecnh \u0111\u00e3 l\u01b0u tr\u1eef s\u1eb5n</li> <li>D\u1ecbch k\u1ebft qu\u1ea3 sang c\u1ea5u tr\u00fac JSON \u0111\u1ec3 R c\u00f3 th\u1ec3 ph\u00e2n t\u00edch</li> </ol> <p>K\u1ebft th\u00fac giai \u0111o\u1ea1n n\u00e0y c\u1ee7a d\u1ef1 \u00e1n, ta \u0111\u00e3 c\u00f3 th\u1ec3 truy c\u1eadp c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u v\u1edbi \u0111\u1ed9 \u1ed5n \u0111\u1ecbnh v\u00e0 \u0111\u1ed3ng nh\u1ea5t cao c\u1ea3 v\u1ec1 k\u1ebft qu\u1ea3 tr\u1ea3 v\u1ec1 l\u1eabn hi\u1ec7u n\u0103ng truy xu\u1ea5t. N\u1ec1n t\u1ea3ng n\u00e0y c\u0169ng d\u1ec5 d\u00e0ng \u0111\u01b0\u1ee3c \u0111\u01b0a v\u00e0o \u00e1p d\u1ee5ng trong c\u00e1c b\u00e0i to\u00e1n kh\u00e1c ngo\u00e0i ng\u00e0nh Tin sinh h\u1ecdc.</p> <p>B\u00e0i ti\u1ebfp theo s\u1ebd n\u00f3i v\u1ec1 R v\u00e0 c\u00e1c b\u00e0i \u0111\u1ea7u ti\u00ean li\u00ean quan \u0111\u1ebfn R. Gi\u1edd th\u00ec ngh\u1ec9 th\u00f4i :wink:</p>"},{"location":"Scientific%20Journey/day-two-documentation-markdown/","title":"Ghi ch\u00fa b\u1eb1ng Markdown","text":"<p>Tr\u01b0\u1edbc khi \u0111\u1ebfn v\u1edbi R c\u00f3 m\u1ed9t s\u1ed1 k\u0129 n\u0103ng ch\u00fang ta c\u1ea7n bi\u1ebft khi tham gia m\u1ed9t d\u1ef1 \u00e1n CNTT.</p> <p>funfact: T\u1ea5t c\u1ea3 b\u00e0i vi\u1ebft tr\u00ean trang n\u00e0y \u0111\u1ec1u \u0111\u01b0\u1ee3c vi\u1ebft b\u1eb1ng Markdown.</p>"},{"location":"Scientific%20Journey/day-two-documentation-markdown/#git","title":"Git","text":"<p>T\u1ea5t c\u1ea3 c\u00e1c d\u1ef1 \u00e1n CNTT c\u00f3 s\u1ef1 tham gia c\u1ee7a 1 s\u1ed1 ng\u01b0\u1eddi \u0111\u1ec1u s\u1eed d\u1ee5ng 1 ph\u1ea7n m\u1ec1m qu\u1ea3n l\u00fd phi\u00ean b\u1ea3n (version control). Ph\u1ed5 bi\u1ebfn nh\u1ea5t trong s\u1ed1 n\u00e0y l\u00e0 Git - \u0111\u01b0\u1ee3c d\u00f9ng trong c\u1ea3 gi\u1edbi khoa h\u1ecdc l\u1eabn gi\u1edbi c\u00f4ng nghi\u1ec7p.</p> <p>\u0110\u1ec3 s\u1eed d\u1ee5ng Git, ch\u00fang ta ch\u1ec9 \u0111\u01a1n gi\u1ea3n download Git. C\u00e1c c\u00e2u l\u1ec7nh git c\u00f3 th\u1ec3 tra c\u1ee9u r\u1ea5t \u0111\u01a1n gi\u1ea3n b\u1eb1ng c\u00e2u l\u1ec7nh <code>git -- help</code>. Tuy nhi\u00ean \u0111\u1ec3 s\u1eed d\u1ee5ng h\u00e0ng ng\u00e0y th\u00ec c\u00f3 m\u1ed9t s\u1ed1 c\u00e1c c\u00e2u l\u1ec7nh sau:</p> <ol> <li><code>git init</code> \u0111\u1ec3 kh\u1edfi t\u1ea1o m\u1ed9t th\u01b0 m\u1ee5c git (\u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 m\u1ed9t repo)</li> <li><code>git add .</code> \u0111\u1ec3 th\u00eam t\u1ea5t c\u1ea3 c\u00e1c thay \u0111\u1ed5i c\u1ee7a m\u00ecnh v\u00e0o repo</li> <li><code>git commit -m \"Message\"</code> l\u01b0u c\u00e1c thay \u0111\u1ed5i n\u00e0y l\u1ea1i th\u00e0nh 1 version/commit</li> <li><code>git push</code> \u0111\u1ea9y m\u1ed9t commit l\u00ean server github (ho\u1eb7c m\u1ed9t server c\u00e1 nh\u00e2n)</li> <li><code>git pull</code> l\u1ea5y phi\u00ean b\u1ea3n m\u1edbi nh\u1ea5t c\u1ee7a branch/repo</li> <li><code>git clone \"repo-url\"</code> t\u1ea3i m\u1ed9t repo github v\u1ec1</li> </ol> <p>M\u1ed9t s\u1ed1 t\u1eeb kh\u00f3a c\u1ea7n t\u00ecm hi\u1ec3u th\u00eam l\u00e0 branch v\u00e0 fork. 2 kh\u00e1i ni\u1ec7m n\u00e0y gi\u00fap ta qu\u1ea3n l\u00fd c\u00e1c th\u00e0nh ph\u1ea7n c\u1ee7a d\u1ef1 \u00e1n d\u1ec5 h\u01a1n, v\u00ed d\u1ee5 nh\u01b0 khi l\u00e0m vi\u1ec7c v\u1edbi Markdown v\u00e0 MKDocs \u1edf d\u01b0\u1edbi, ch\u00fang ta s\u1ebd c\u00f3 1 branch \u0111\u1ec3 qu\u1ea3n l\u00fd c\u00e1c b\u00e0i vi\u1ebft v\u00e0 1 branch \u0111\u1ec3 c\u00f4ng b\u1ed1/render c\u00e1c b\u00e0i vi\u1ebft n\u00e0y.</p>"},{"location":"Scientific%20Journey/day-two-documentation-markdown/#markdown","title":"Markdown","text":"<p>Tr\u01b0\u1edbc khi c\u00f3 Markdown, gi\u1edbi khoa h\u1ecdc th\u01b0\u1eddng s\u1eed d\u1ee5ng LaTex \u0111\u1ec3 so\u1ea1n th\u1ea3o c\u00e1c v\u0103n b\u1ea3n khoa h\u1ecdc. \u0110i\u1ec3m m\u1ea1nh c\u1ee7a LaTex l\u00e0 n\u00f3 r\u1ea5t nhi\u1ec1u plugin v\u00e0 c\u00f4ng c\u1ee5 h\u1ed7 tr\u1ee3, tuy nhi\u00ean c\u00fa ph\u00e1p c\u1ee7a LaTex r\u1ea5t ph\u1ee9c t\u1ea1p v\u00e0 kh\u00f3 \u0111\u1ec3 h\u1ecdc.</p> <p>\u0110\u1ec3 gi\u1ea3i quy\u1ebft v\u1ea5n n\u1ea1n n\u00e0y, Markdown ra \u0111\u1eddi nh\u01b0 m\u1ed9t ng\u00f4n ng\u1eef \u0111\u00e1nh d\u1ea5u nh\u1eb9 v\u00e0 d\u1ec5 s\u1eed d\u1ee5ng. To\u00e0n b\u1ed9 c\u00fa ph\u00e1p c\u1ee7a Markdown c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c t\u00f3m l\u01b0\u1ee3c trong m\u1ed9t hai trang A4. V\u1ea5n \u0111\u1ec1 l\u00e0 khi ch\u00fang ta c\u00f3 m\u1ed9t b\u00e0i vi\u1ebft Markdown xong r\u1ed3i th\u00ec l\u00e0m g\u00ec? \u0110\u00f3n \u0111\u1ecdc ph\u1ea7n 3 (t\u1ed1i vi\u1ebft ti\u1ebfp).</p>"},{"location":"Scientific%20Journey/day-two-documentation-markdown/#github-pages-va-mkdocs","title":"Github Pages v\u00e0 MKDocs","text":"<p>\u0110\u1ec3 l\u00e0m m\u1ed9t trang web t\u01b0\u01a1ng t\u1ef1 trang n\u00e0y c\u00f3 r\u1ea5t nhi\u1ec1u l\u00fd do. M\u1ed9t trong s\u1ed1 nh\u1eefng l\u00fd do ph\u1ed5 bi\u1ebfn l\u00e0 \u0111\u1ec3 l\u00e0m t\u00e0i li\u1ec7u cho d\u1ef1 \u00e1n (project page). M\u1ed9t l\u00fd do kh\u00e1c l\u00e0 do th\u00edch vi\u1ebft (nh\u01b0 m\u00ecnh).</p> <p>Trang web n\u00e0y \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng tr\u00ean Github Pages v\u00e0 MKDocs. L\u00fd do l\u00e0 v\u00ec so v\u1edbi Wordpress (n\u1ec1n t\u1ea3ng tr\u01b0\u1edbc \u0111\u00e2y m\u00ecnh vi\u1ebft), MKDocs s\u1eed d\u1ee5ng Markdown cho ph\u00e9p vi\u1ebft r\u1ea5t nhanh v\u00e0 d\u1ec5 d\u00e0ng t\u00f9y ch\u1ec9nh. MKDocs c\u0169ng \u1ed5n \u0111\u1ecbnh v\u00e0 t\u1ed1i gi\u1ea3n h\u01a1n nhi\u1ec1u so v\u1edbi Wordpress.</p> <p>\u0110\u1ec3 c\u00e0i \u0111\u1eb7t MKDocs h\u1ebft s\u1ee9c \u0111\u01a1n gi\u1ea3n: <code>pip install mkdocs</code>. M\u00ecnh khuy\u1ebfn kh\u00edch m\u1ecdi ng\u01b0\u1eddi t\u1ef1 t\u00ecm hi\u1ec3u th\u00eam v\u1ec1 ph\u1ea7n m\u1ec1m n\u00e0y.</p> <p>\u0110\u1ec3 b\u1eaft \u0111\u1ea7u m\u1ed9t trang web MKDocs, ta di chuy\u1ec3n \u0111\u1ebfn folder ch\u1ee9a repo (xem ph\u1ea7n d\u01b0\u1edbi) ch\u1ee9a trang web c\u1ee7a ch\u00fang ta, v\u00e0 d\u00f9ng c\u00e2u l\u1ec7nh</p> <pre><code>mkdocs init .\n</code></pre> <p>C\u00e1c folder v\u00e0 file kh\u00e1c ta kh\u00f4ng c\u1ea7n qu\u00e1 ch\u00fa tr\u1ecdng, quan tr\u1ecdng nh\u1ea5t l\u00fac n\u00e0y l\u00e0 folder <code>docs</code> v\u00e0 file <code>mkdocs.yml</code>. Folder <code>docs</code> ch\u1ee9a t\u1ea5t c\u1ea3 t\u00e0i li\u1ec7u ch\u00fang ta s\u1ebd vi\u1ebft. File <code>mkdocs.yml</code> ch\u1ee9a c\u00e1c thi\u1ebft l\u1eadp cho trang web.</p> <p>Sau khi vi\u1ebft 1 t\u00e0i li\u1ec7u Markdown v\u00e0o folder <code>docs</code>, c\u00e2u l\u1ec7nh <code>mkdocs build</code> s\u1ebd x\u00e2y d\u1ef1ng trang web c\u1ee7a ta v\u00e0o folder <code>site</code>. Vi\u1ec7c c\u1ea7n l\u00e0m ti\u1ebfp theo l\u00e0 upload folder n\u00e0y l\u00ean server \u0111\u1ec3 cung c\u1ea5p trang web. \u0110\u00e2y l\u00e0 vi\u1ec7c c\u1ee7a c\u00e2u l\u1ec7nh <code>mkdocs gh-deploy</code>. S\u1ebd c\u1ea7n m\u1ed9t s\u1ed1 t\u00f9y ch\u1ec9nh trong file <code>mkdocs.yml</code>. \u0110i\u1ec1u n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c n\u00f3i th\u00eam \u1edf ph\u1ea7n d\u01b0\u1edbi.</p> <p>Server c\u1ee7a blog n\u00e0y \u0111\u01b0\u1ee3c h\u1ed7 tr\u1ee3 b\u1edfi Github Pages. D\u1ecbch v\u1ee5 n\u00e0y c\u1ee7a Github nh\u1eb1m cung c\u1ea5p 1 n\u1ec1n t\u1ea3ng website HTML t\u0129nh (static HTML pages), d\u00e0nh cho vi\u1ec7c c\u00f4ng b\u1ed1 t\u00e0i li\u1ec7u c\u1ee7a d\u1ef1 \u00e1n, ho\u1eb7c \u0111\u1ec3 m\u00ecnh l\u1ee3i d\u1ee5ng l\u00e0m web c\u00e1 nh\u00e2n.</p> <p>Th\u00f4ng th\u01b0\u1eddng Github Pages \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng b\u1eb1ng Jekyll. Tuy nhi\u00ean v\u00ec m\u00ecnh \u0111\u00e3 s\u1eed d\u1ee5ng Python, v\u00e0 kh\u00f4ng mu\u1ed1n c\u00e0i th\u00eam NodeJS n\u00ean tr\u00ean n\u00e0y ch\u00fang ta s\u1ebd d\u00f9ng MKDocs \u0111\u1ec3 x\u00e2y d\u1ef1ng 1 trang web.</p> <p>\u0110\u1ec3 \u0111\u0103ng k\u00fd 1 trang Github Pages c\u00e1 nh\u00e2n, ch\u00fang ta c\u1ea7n t\u1ea1o m\u1ed9t repo c\u00f3 t\u00ean <code>github-username</code><code>.github.io</code>, v\u1edbi <code>github-username</code> l\u00e0 t\u00ean Github c\u1ee7a ch\u00fang ta. L\u00fac n\u00e0y, Github s\u1ebd t\u1ef1 t\u1ea1o m\u1ed9t trang v\u1edbi t\u00ean mi\u1ec1n <code>github-username</code><code>.github.io</code> \u0111\u1ec3 ch\u00fang ta c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng.</p> <p>\u0110\u1ec3 post MKDocs l\u00ean trang c\u00e1 nh\u00e2n n\u00e0y, sau khi clone repo tr\u00ean v\u1ec1 v\u00e0 c\u00e0i \u0111\u1eb7t MKDocs t\u1ea1i folder c\u1ee7a repo, ch\u00fang ta c\u1ea7n ch\u1ec9nh s\u1eeda file <code>mkdocs.yml</code>. M\u1eb7c \u0111\u1ecbnh MKDocs s\u1ebd post b\u00e0i theo c\u1ea5u h\u00ecnh c\u1ee7a Github Pages d\u00e0nh cho t\u1ed5 ch\u1ee9c thay v\u00ec c\u00e1 nh\u00e2n. \u0110i\u1ec3m kh\u00e1c bi\u1ec7t gi\u1eefa 2 lo\u1ea1i trang n\u00e0y l\u00e0 trang t\u1ed5 ch\u1ee9c cho ph\u00e9p post b\u00e0i l\u00ean branch gp-deploy, trong khi trang c\u00e1 nh\u00e2n post b\u00e0i l\u00ean branch master (xem th\u00eam v\u1ec1 git branch).</p> <p>\u0110\u1ec3 c\u1ea5u h\u00ecnh cho \u0111\u00fang v\u1edbi trang c\u00e1 nh\u00e2n, ch\u1ec9 \u0111\u01a1n gi\u1ea3n th\u00eam m\u1ed9t d\u00f2ng:</p> <pre><code>remote_branch: master\n</code></pre> <p>V\u1eady th\u00f4i :smile: anh em ch\u1ecbu kh\u00f3 \u0111\u1ecdc th\u00eam t\u00e0i li\u1ec7u nha.</p>"},{"location":"Scientific%20Journey/intro/","title":"H\u00e0nh tr\u00ecnh khoa h\u1ecdc: Python v\u00e0 R","text":"<p>Trong n\u1ed7 l\u1ef1c m\u1edbi nh\u1ea5t c\u1ee7a L\u00e2m \u0111\u1ec3 t\u00e1ch r\u1eddi m\u1eb7t h\u1ecdc t\u1eadp/h\u1ecdc thu\u1eadt v\u00e0 cu\u1ed9c s\u1ed1ng n\u1ed9i t\u00e2m, \u0111\u00e2y l\u00e0 m\u1ed9t s\u1ef1 th\u1ee5t l\u00f9i tai h\u1ea1i.</p> <p>T\u1eeb gi\u1edd \u0111\u1ebfn khi sinh nh\u1eadt 23 tu\u1ed5i c\u1ee7a b\u1ea3n th\u00e2n, m\u00ecnh s\u1ebd \u0111\u1eb7t m\u1ee5c ti\u00eau m\u1ed7i ng\u00e0y (c\u1ed1 g\u1eafng) vi\u1ebft \u00edt ra 1 b\u00e0i, v\u00e0 c\u00f4ng vi\u1ec7c l\u1edbn nh\u1ea5t m\u00ecnh \u0111ang c\u1ea7n ph\u1ea3i l\u00e0m l\u00e0 nghi\u00ean c\u1ee9u. D\u0129 nhi\u00ean, m\u1ed9t ng\u00e0y th\u00ec c\u00f3 24 ch\u1ee9 kh\u00f4ng ph\u1ea3i 48 ti\u1ebfng \u0111\u1ed3ng h\u1ed3, v\u00e0 m\u1ed9t ng\u01b0\u1eddi m\u1ed7i ng\u00e0y ch\u1ec9 c\u00f3 th\u1ec3 c\u00f3 t\u1eebng \u0111\u1ea5y th\u1eddi gian \u0111\u1ec3 ng\u1ed3i \u0111\u1ecdc v\u00e0 vi\u1ebft. V\u00ec v\u1eady, t\u1ed1t nh\u1ea5t l\u00e0 vi\u1ebft v\u1ec1 c\u00e1i m\u00ecnh \u0111ang \u0111\u1ecdc, v\u1eeba \u0111\u1ec3 ghi nh\u1edb v\u1eeba \u0111\u1ec3 l\u1ea5y th\u00eam c\u1ea3m h\u1ee9ng m\u00e0 ng\u1ed3i... vi\u1ebft. M\u1ed9t ch\u00fat v\u1ec1 c\u00f4ng vi\u1ec7c hi\u1ec7n t\u1ea1i. N\u00f3i cho oai, m\u00ecnh l\u00e0 m\u1ed9t researcher \u1edf ICTLab tr\u01b0\u1eddng USTH. N\u00f3i chu\u1ea9n ra m\u00ecnh l\u00e0 m\u1ed9t th\u1ee3 code cho th\u1ea7y Pierre \u0111\u1ec3 l\u1ea5y ti\u1ec1n v\u00e0 c\u01a1 h\u1ed9i sang b\u1ec3n :)). C\u00e1c ph\u1ea7n code li\u00ean quan \u0111\u1ebfn s\u1eed d\u1ee5ng c\u00f4ng c\u1ee5 ICT trong tin sinh h\u1ecdc, c\u1ee5 th\u1ec3 l\u00e0 g\u00ec th\u00ec... m\u00ecnh c\u0169ng ch\u1eb3ng bi\u1ebft ha ha.</p> <p>2 c\u00f4ng c\u1ee5 ch\u00ednh c\u1ee7a m\u00ecnh v\u00e0 c\u1ee7a r\u1ea5t nhi\u1ec1u d\u1ef1 \u00e1n Khoa h\u1ecdc D\u1eef li\u1ec7u (Data Science) l\u00e0 R v\u00e0 Python. C\u00e1 nh\u00e2n m\u00ecnh l\u00e0 ng\u01b0\u1eddi \u0111\u00e3 kh\u00e1 c\u00f3 kinh nghi\u1ec7m s\u1eed d\u1ee5ng Python trong th\u1eddi gian kh\u00e1 l\u00e2u, nh\u01b0ng R l\u00e0 m\u1ed9t th\u1ee9 kh\u00e1c h\u1eb3n. Kh\u00f4ng nh\u01b0 Python, m\u1ed9t ng\u00f4n ng\u1eef \u0111a d\u1ee5ng (general purpose), R l\u00e0 m\u1ed9t c\u00f4ng c\u1ee5 kh\u00e1 \u0111\u1eb7c tr\u01b0ng cho ng\u00e0nh khoa h\u1ecdc d\u1eef li\u1ec7u. T\u1ea5t c\u1ea3 m\u1ecdi th\u1ee9 li\u00ean quan \u0111\u1ec3n R \u0111\u1ec1u nh\u1eb1m 1 m\u1ee5c ti\u00eau: khai ph\u00e1 d\u1eef li\u1ec7u. V\u00ec v\u1eady R l\u00e0 m\u1ed9t c\u00f4ng c\u1ee5 tuy\u1ec7t v\u1eddi cho m\u1ed9t tr\u01b0\u1eddng h\u1ee3p c\u1ee5 th\u1ec3 c\u1ee7a m\u1ed9t ng\u00e0nh khoa h\u1ecdc c\u1ee5 th\u1ec3. C\u00e1c gi\u1edbi h\u1ea1n v\u1ec1 kh\u1ea3 n\u0103ng \u1ee9ng d\u1ee5ng c\u1ee7a R trong c\u00e1c b\u00e0i to\u00e1n kh\u00e1c b\u1ed7ng tr\u1edf n\u00ean kh\u00f4ng quan tr\u1ecdng.</p> <p>(N\u00f3i nh\u01b0 v\u1eady kh\u00f4ng ph\u1ea3i l\u00e0 R kh\u00f4ng th\u1ec3 l\u00e0m \u0111\u01b0\u1ee3c nh\u1eefng vi\u1ec7c kh\u00e1c. \u00d4ng n\u1ed9i Hadley vi\u1ebft c\u1ea3 m\u1ea5y quy\u1ec3n s\u00e1ch v\u1ec1 R \u0111\u01b0\u1ee3c d\u00e0n trang b\u1eb1ng... R, s\u1eed d\u1ee5ng RMarkdown v\u00e0 Bookdown, 2 package ch\u00ednh \u1ed5ng ph\u00e1t tri\u1ec3n cho R. Ngo\u00e0i ra c\u0169ng c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng Sparkling \u0111\u1ec3 vi\u1ebft web service cho c\u00e1c routine R, kh\u00e1 l\u00e0 h\u1eefu d\u1ee5ng \u0111\u1ec3 \u0111\u01b0a c\u00e1c nghi\u00ean c\u1ee9u R v\u00e0o \u00e1p d\u1ee5ng th\u1ef1c t\u1ebf.)</p> <p>V\u00ec y\u00eau c\u1ea7u c\u1ee7a c\u00f4ng vi\u1ec7c trong d\u1ef1 \u00e1n, t\u1eeb ng\u00e0y h\u00f4m nay m\u00ecnh s\u1ebd b\u1eaft \u0111\u1ea7u c\u00e0y cu\u1ed1c R \u0111\u1ebfn khi n\u00e0o R v\u00e0 Python c\u1ee7a m\u00ecnh \u0111\u1ea1t \u0111\u01b0\u1ee3c t\u00ecnh \u0111\u1ed9 t\u01b0\u01a1ng \u0111\u1ed1i \u0111\u1ed3ng nh\u1ea5t v\u1edbi nhau. Gi\u1ed1ng nh\u01b0 tay tr\u00e1i v\u00e0 tay ph\u1ea3i v\u1eady, nh\u01b0ng m\u00ecnh l\u00e0 ng\u01b0\u1eddi thu\u1eadn c\u1ea3 2 tay.</p> <p>C\u00e1c t\u00e0i li\u1ec7u, reference v\u00e0 v\u00ed d\u1ee5 s\u1ebd s\u1eed d\u1ee5ng b\u1ed9 c\u00f4ng c\u1ee5 R chu\u1ea9n, ch\u1ea1y tr\u00ean RStudio tr\u00ean n\u1ec1n Windows 10. T\u00e0i li\u1ec7u tham kh\u1ea3o ch\u00ednh s\u1ebd l\u00e0 R for Data Science c\u1ee7a Hadley (\u00f4ng tr\u00f9m ng\u00e0nh R) \u0111\u1ed1i v\u1edbi R v\u00e0 How to think like a Computer Scientist \u0111\u1ed1i v\u1edbi Python.</p> <p>Let's the journey begin.</p>"},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning%20part%202_en/","title":"MODEL SELECTION","text":"<p>or Machine learning Machine Learning</p>"},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning%20part%202_en/#criterions-loss-function-for-functions","title":"Criterions: Loss function for functions","text":"<p>Now, how can we select the correct hyperparameter? While normal parameters are tuned by loss function, there is no such concept for hyperparameter - a hyperloss function perhap? As it turn out, there are several criteria that can be used to judge the performance of our model.</p> <p>As mention, pure loss is not a valid criteria to judge the performance of a model. More statistical approachs is required, ranging from extension of validation to penalized criteria aim to further regulate the model. In this post we will focus on BIC/AIC and AUC.</p>"},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning%20part%202_en/#auc-which-curve-youre-talking-about","title":"AUC: which curve you're talking about?","text":"<p>Area under curve is one of the major model benchmarking criteria. If we plot the True Positive Rate (TPR) versus the False Postive Rate (FPR), we obtain a plot called Receiver Operating Charateristic, or ROC curve.</p>"},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning%20part%202_en/#information-criterion","title":"Information Criterion","text":""},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning%20part%202_en/#some-viable-algorithms-for-selecting-lambda","title":"Some viable algorithms for selecting","text":""},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning%20part%202_en/#heuristic-methods","title":"Heuristic methods","text":""},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning%20part%202_en/#deterministic-methods","title":"Deterministic methods","text":""},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning%20part%202_en/#conclusion","title":"Conclusion","text":"<p>In practice, a larger model is not guarantee a better prediction power (and in most case, worst). Overfitting, loss of generalization and loss of interpretability are some among major problems</p>"},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning_en/","title":"MODEL SELECTION","text":"<p>or Machine learning Machine Learning</p> <p>Author note: I'm writing this document while waiting for my surgery to keep my mind occupied. For now I'm in too much pain to ramble on; I will follow up with the methods as soon as I can. Please excuse poor ass English usage.</p>"},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning_en/#part-1-problem-of-model-selection","title":"Part 1: Problem of Model Selection","text":""},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning_en/#so-what-is-a-model","title":"So, what is a Model?","text":"<p>A model is \"simply\" a function . Intuitively, the model describes a (statistical) connection between phenominons, or in a more \"Data science-ish\" term, describes the connection between the inputs and output.</p> <p>As most people with interest in DS well aware, the core tenet of Machine Learning is to find the  such that</p> <p> </p> <p>where  is a loss function. In fact, this problem can be solve relatively easy for a certain function . Howerver reality is usually more complex and the reality of our model is similar. Generally we would not work with just a function, but a family of functions .</p> <p>Enter Hyperparameter.</p>"},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning_en/#excusez-moi","title":"Excusez-moi?","text":"<p>Hyperparameters are parameters that instead of describe the connection between the observations and the output, they describe the difference between models of the same family. For example we are all well aware of hyperparameters of a neural network: its depth (number of layers) and its width (number of units in each layer). 2 models of a same family will give different prediction performance as well as require different computing performance.</p> <p>Theoretically we want as much information as possible in our model: input everything, compute every possible connection and let the model learn itself out. However our model is a mere approximation of the real connection - there are noise, there are missing parameters, hidden parameters and useless parameters. Trying to incorporate everything into our model will inevitably lead to the words that make all computer scientist squimish: OVERFIT. Fucking overfit.</p> <p>Therefore we need to regulate the parameters that will be included in our computation. Considering linear regression, we come to LASSO.</p>"},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning_en/#lasso","title":"LASSO","text":"<p>Lasso is a model selection and regularization algorithm for linear regression. Why do we care? Well, for linear regression, the number of hyperparameters is actually astronomical: for each parameters there is a binary hyperparameters describe if it is in or out of the model (or of course we set the weight to 0, but then how can we tell which parameter to left out?). LASSO solve this problem elegantly by introducing the concept of l1-regularization. </p> <p>Intuitively, with the assumption that not all parameters are useful, we penalize the models with too many parameters. Concretely, Lasso regularization is  </p> <p>Evidently, the more s the higher loss function will be. Intuitively the balance between loss from function and loss from regularization is controlled by . The higher , the fewer parameters actually enter the model. By controlling  we aviod the problem of have to identify which individual parameters with enter the model.</p>"},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning_en/#to-be-continued","title":"To be continued","text":""},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning_vi/","title":"Machine learning machine learning vi","text":"<p>or Machine learning Machine Learning</p> <p>Author note: I'm writing this document while waiting for my surgery to keep my mind occupied. For now I'm in too much pain to ramble on; I will follow up with the methods as soon as I can. Please excuse poor ass English usage.</p>"},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning_vi/#part-1-problem-of-model-selection","title":"Part 1: Problem of Model Selection","text":""},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning_vi/#so-what-is-a-model","title":"So, what is a Model?","text":"<p>A model is \"simply\" a function . Intuitively, the model describes a (statistical) connection between phenominons, or in a more \"Data science-ish\" term, describes the connection between the inputs and output.</p> <p>As most people with interest in DS well aware, the core tenet of Machine Learning is to find the  such that</p> <p> </p> <p>where  is a loss function. In fact, this problem can be solve relatively easy for a certain function . Howerver reality is usually more complex and the reality of our model is similar. Generally we would not work with just a function, but a family of functions .</p> <p>Enter Hyperparameter.</p>"},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning_vi/#excusez-moi","title":"Excusez-moi?","text":"<p>Hyperparameters are parameters that instead of describe the connection between the observations and the output, they describe the difference between models of the same family. For example we are all well aware of hyperparameters of a neural network: its depth (number of layers) and its width (number of units in each layer). 2 models of a same family will give different prediction performance as well as require different computing performance.</p> <p>Theoretically we want as much information as possible in our model: input everything, compute every possible connection and let the model learn itself out. However our model is a mere approximation of the real connection - there are noise, there are missing parameters, hidden parameters and useless parameters. Trying to incorporate everything into our model will inevitably lead to the words that make all computer scientist squimish: OVERFIT. Fucking overfit.</p> <p>Therefore we need to regulate the parameters that will be included in our computation. Considering linear regression, we come to LASSO.</p>"},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning_vi/#lasso","title":"LASSO","text":"<p>Lasso is a model selection and regularization algorithm for linear regression. Why do we care? Well, for linear regression, the number of hyperparameters is actually astronomical: for each parameters there is a binary hyperparameters describe if it is in or out of the model (or of course we set the weight to 0, but then how can we tell which parameter to left out?). LASSO solve this problem elegantly by introducing the concept of l1-regularization. </p> <p>Intuitively, with the assumption that not all parameters are useful, we penalize the models with too many parameters. Concretely, Lasso regularization is  </p> <p>Evidently, the more s the higher loss function will be. Intuitively the balance between loss from function and loss from regularization is controlled by . The higher , the fewer parameters actually enter the model. By controlling  we aviod the problem of have to identify which individual parameters with enter the model.</p>"},{"location":"Scientific%20Journey/machine%20learning%20machine%20learning_vi/#to-be-continued","title":"To be continued","text":""},{"location":"Scientific%20Journey/nmf-intro-presentation/","title":"NMF: An overview of Non-negative Matrix Factorization","text":""},{"location":"Scientific%20Journey/nmf-intro-presentation/#problem-statement","title":"Problem statement","text":"<p>Given a non-negative matrix .  can be express as a product of 2 matrices  and  </p> <p>where  contain no negative elements. NMF find the matrix  and  s.t</p> <p>  - Application:   - Clustering   - Dimentional reduction</p>"},{"location":"Scientific%20Journey/nmf-intro-presentation/#iterative-algorithm-for-sovling-nmf","title":"Iterative Algorithm for sovling NMF","text":"<ul> <li>NMF is non-convex over both W and H</li> <li>This imply we may not found global minima</li> <li>However, if we fix H</li> <li>  is convex</li> <li>The same for  </li> </ul> <p>Algorithm: At each step, fix one matrix and find the other.</p>"},{"location":"Scientific%20Journey/nmf-intro-presentation/#special-case-symmetrical-nmf","title":"Special case: Symmetrical NMF","text":"<ul> <li>A special :      </li> <li>Especially useful for spectral clustering</li> </ul>"},{"location":"Scientific%20Journey/nmf-intro-presentation/#sparse-nmf","title":"Sparse NMF","text":"<ul> <li>By definition, with  and , the resulting W and H is dense</li> <li>Problem: How do we know how many  </li> <li>Sparse NMF: increase  and regulate with  norm</li> <li> </li> </ul>"},{"location":"Scientific%20Journey/nmf-intro-presentation/#improvements-ideas","title":"Improvements (Ideas)","text":""},{"location":"Scientific%20Journey/nmf-intro-presentation/#nonlinear-nmf","title":"Nonlinear NMF","text":"<ul> <li>Non linear function:</li> <li>Reformulation of NMF:      where  </li> <li>Instead of a linear , we subtitute with a slightly non linear function  </li> <li>Several choices for :<ul> <li>Shifted ReLU:  </li> <li>Shifted Parametric ReLU:  </li> </ul> </li> </ul>"},{"location":"Scientific%20Journey/nmf-intro-presentation/#geometric-preserving-nmf","title":"Geometric preserving NMF","text":"<ul> <li>Intuition: Comparing the reconstructed  geometry to original , even if the values changed.</li> <li>In practice: compare the angular different between  and s principal components</li> <li>Reminder: Principal Component  </li> <li> </li> </ul> <ul> <li>Ideas:</li> <li>Comparing vanila and improved NMF performance in preserving the geometry of the dataset</li> <li>New algorithm:         We penalize the algorithm if it rotate the dataset's principal components</li> </ul>"},{"location":"Scientific%20Journey/nmf-intro-presentation/#discusion","title":"Discusion","text":""},{"location":"Scientific%20Journey/proposal/","title":"Proposal for scientific software: PySeqView (tentative)","text":"<p>We are looking for a Bachelor student to work on a possible software for sequence editing and visualization software. This topic should constitute a Bachelor thesis. I will be the direct supervisor, with Professor Rahmann advise.</p> <p>Supervisor: Vu-Lam Dang.</p> <p>Abstract: Sequence management, alignment editing and figure generation is an important task performed daily by biologists. One of the most widely used tools in use for such tasks is BioEdit 1. However, BioEdit is no longer being maintained, and thus we propose the creation of a new open-source software package that aims to replace BioEdit.</p>"},{"location":"Scientific%20Journey/proposal/#background","title":"Background","text":"<p>BioEdit 1 is a popular biological sequence alignment editor written originally for Windows 95/98/NT/2000/XP. It has a multiple document interface with convenient features that makes the alignment and manipulation of sequences relatively easy on a desktop computer. Several sequence manipulation and analysis options and links to external analysis programs facilitate a working environment that allows the user to view and manipulate sequences with simple point-and-click operations. It also has the capability to generate figures useful for scientific publications.</p> <p><p>Figure: Example of figure generated by BioEdit. Source2</p> <p>Although BioEdit is a very popular program, and still widely used as of 2021, its development had been stopped since 2007. It has an outdated graphical user interface and is likely to contain unresolved bugs and security compromises. The figures generated by BioEdit are also outdated compared to contemporary standards. Furthermore, the software is only provided for Windows operating system, thus making it more difficult to run on Mac/Linux environment.</p> <p> <p>Figure: BioEdit Graphical User Interface</p></p> <p>Other softwares provide workarounds to generate figures suitable for modern articles. One of them is SnapGene Viewer. However, its free version has some limitations, particularly in editing the sequences, forcing the users to use workarounds such as manually editing the sequence in a text editor. More tech-savvy users employ more advance graphical software like Photoshops or AutoCAD to create figures, but this is hardly ideal.</p> <p>Finally, as of 2021, the original BioEdit website host at NCSU is no longer available, as its maintainer, Dr James Brown has retired. A mirror website is available at BioEdit site.</p>"},{"location":"Scientific%20Journey/proposal/#proposal","title":"Proposal","text":"<p>The goal of this project is to create a foundation for free open source software (FOSS) that facilitate multiple sequence alignment, manipulation and visualization. The software should also perform a variety of other essential functions relevant to wet lab operation.</p> <p>Primary functions: - Alignment Management and Editing   - Align DNA sequences with a reference sequence   - Pairwise and multi-sequence DNA and Protein alignment   - Choice of alignment algorithms - Clustal Omega, MAFFT, MUSCLE, T-Coffee   - Contig Assembly - Visualizing   - See multiple views of a DNA sequence   - Large sequence support - browse chromosome size sequences   - Edit DNA and protein sequences   - Color code sequences   - Plasmid drawing interface for automated creation of plasmid vector graphic from a DNA sequence</p> <p>Further functionalities shall be added to make to program comparable to BioEdit.</p> <p>The developer is recommended to use Python with Tkinter to create the graphical interface, or propose a solution of their own.</p>"},{"location":"Scientific%20Journey/proposal/#references","title":"References","text":"<ol> <li> <p>T. A. Hall, \u201cBioEdit: A User-Friendly Biological Sequence Alignment Editor and Analysis,\u201d 1999. Website \u21a9\u21a9</p> </li> <li> <p>Peng C, Yin X, Li M, He T, Li G. Construction of a eukaryotic expression plasmid for human retina-derived neurotrophin-3. Neural Regen Res 2013;8:1031-40. Available from: Website \u21a9</p> </li> </ol>"},{"location":"Scientific%20Journey/xgboost_c%2Bt/","title":"Survey on xgboost's objective functions","text":"<p>In this document I go in depth into using XGBoost to enhence the performance of C+T by combining multiple C+T predictors. 1</p>"},{"location":"Scientific%20Journey/xgboost_c%2Bt/#prequisites","title":"Prequisites","text":"<p>The following packages are required to knit this document:</p> <ul> <li>bigstatsr</li> <li>bigsnpr</li> <li>data.table</li> <li>xgboost</li> </ul>"},{"location":"Scientific%20Journey/xgboost_c%2Bt/#dataset","title":"Dataset","text":"<p>Please change working direction in <code>directory</code></p> <p>I use the \"Simus\" simulated dataset provided by Florian. This dataset contain ~650,000 SNPs in 2 chromosomes. 20% of them are cases and the rest 80% are controls.</p>"},{"location":"Scientific%20Journey/xgboost_c%2Bt/#ct-step","title":"C+T step","text":"<p>Generating a matrix of C+T with varied clumping radius and  thresholds.</p> <p>  and  are aquired in sumstats file (see <code>sumstats.txt</code>)</p> <p>XGBoost only accept dataframe, so all FBM output must be converted to data frame</p>"},{"location":"Scientific%20Journey/xgboost_c%2Bt/#data-treatment-oversampling","title":"Data treatment: Oversampling","text":"<p>To oversampling I extract all positive (affection = 1), and append them a number of times. For this simulated dataset, I enhanced the number of cases 10 times, thus the ratio become ~70:30 (from 20:80).</p>"},{"location":"Scientific%20Journey/xgboost_c%2Bt/#xgboost","title":"XGBoost","text":"<p>XGBoost provide slightly better result compare to SCT. However this result heavily depend on booster and objective selection. Experiments shown that gblinear booster with count:poisson objective give the best results.</p> <p>For the first experimentation, I run 9 different max_depth and 9 nrounds with count:poisson objective function. Each combination is repeated 10 times and the mean value calculated to rule out the randomess of gblinear booster</p> <pre><code>poisson_AUCs &lt;- matrix(,nrow = 10, ncol = 10)\nfor (i in seq(2 : 10)){\n  for (j in seq(2: 10)) {\n    temp &lt;- c()\n    for(t in seq(1:10)){\n      bstSparse &lt;- xgboost(data = ds[], \n                           label = y.train, \n                           booster=\"gblinear\",\n                           max_depth = i, \n                           eta = 1, \n                           nthread = 2, \n                           nrounds = j, \n                           lambda = 0.1,\n                           objective = \"count:poisson\"\n                           )\n      pred &lt;- predict(bstSparse, PRS_test[])\n      temp &lt;- append(temp,AUC(pred = pred, test$fam$affection))\n    }\n    poisson_AUCs[i,j] &lt;- mean(temp)\n  }\n}\n</code></pre> <pre><code>##            [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n##  [1,] 0.7817714 0.7823509 0.7823041 0.7822926 0.7825028 0.7824836\n##  [2,] 0.7820513 0.7822373 0.7823633 0.7824287 0.7822630 0.7824896\n##  [3,] 0.7814487 0.7821501 0.7823655 0.7826146 0.7825234 0.7825795\n##  [4,] 0.7820921 0.7824120 0.7824182 0.7826145 0.7825073 0.7824446\n##  [5,] 0.7821936 0.7824070 0.7821748 0.7823153 0.7826239 0.7824142\n##  [6,] 0.7819266 0.7822376 0.7822175 0.7824563 0.7823541 0.7824829\n##  [7,] 0.7818918 0.7820830 0.7823140 0.7823306 0.7824132 0.7824501\n##  [8,] 0.7820375 0.7822164 0.7824588 0.7824749 0.7825382 0.7825095\n##  [9,] 0.7822184 0.7819440 0.7825596 0.7823647 0.7824570 0.7823492\n## [10,]        NA        NA        NA        NA        NA        NA\n##            [,7]      [,8]      [,9] [,10]\n##  [1,] 0.7823923 0.7821336 0.7818004    NA\n##  [2,] 0.7823814 0.7821682 0.7817430    NA\n##  [3,] 0.7823602 0.7821881 0.7817512    NA\n##  [4,] 0.7823655 0.7820304 0.7818152    NA\n##  [5,] 0.7822578 0.7821342 0.7818768    NA\n##  [6,] 0.7824436 0.7822035 0.7817998    NA\n##  [7,] 0.7824093 0.7822224 0.7817339    NA\n##  [8,] 0.7824048 0.7821033 0.7817913    NA\n##  [9,] 0.7823197 0.7822398 0.7817214    NA\n## [10,]        NA        NA        NA    NA\n</code></pre> <p></p> <p>I omitted the code for the next 2 experiments as they are largely similar, with exception of the option objective for xgboost function.</p> <p>For the next experimentation, I run 9 different max_depth and 9 nrounds with reg:logistic objective function. Each combination is repeated 10 times and the mean value calculated to rule out the randomess of gblinear booster</p> <p></p> <p>For the final experimentation, I run 9 different max_depth and 9 nrounds with binary:logicraw objective function. This function is interesting since it's specific for binary classification.</p> <p>Here is a composite boxplot comparing different objective functions. Index 1 is count:poisson, 2 is reg:logistic and 3 is binary:logistic</p> <p></p> <p></p>"},{"location":"Scientific%20Journey/xgboost_c%2Bt/#the-effect-of-boosters","title":"The effect of Boosters","text":"<p>I replicate the experiment with the gbtree booster. Every other aspect stay the same. The effect of max_depth and nrounds is similar to random forest: more depth means more stability, while more rounds might lead to overfitting.</p> <pre><code>tree_poisson_AUCs &lt;- matrix(,nrow = 10, ncol = 10)\nfor (i in seq(2 : 10)){\n  for (j in seq(2: 10)) {\n    temp &lt;- c()\n    bstSparse &lt;- xgboost(data = ds[], \n                         label = y.train, \n                         booster=\"gbtree\",\n                         max_depth = i, \n                         eta = 1, \n                         nthread = 2, \n                         nrounds = j, \n                         lambda = 0.1,\n                         objective = \"count:poisson\"\n                         )\n    pred &lt;- predict(bstSparse, PRS_test[])\n    tree_poisson_AUCs[i,j] &lt;- append(temp,AUC(pred = pred, test$fam$affection))\n  }\n}\n</code></pre> <pre><code>##            [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n##  [1,] 0.5997502 0.6783138 0.6797928 0.7031598 0.7184346 0.7197522\n##  [2,] 0.6857363 0.6944414 0.7274155 0.7318843 0.7333248 0.7421854\n##  [3,] 0.7133489 0.7344719 0.7470404 0.7456065 0.7437271 0.7440640\n##  [4,] 0.7346792 0.7346642 0.7398461 0.7412047 0.7496907 0.7480144\n##  [5,] 0.7158010 0.7299822 0.7408819 0.7438784 0.7454125 0.7431477\n##  [6,] 0.7019057 0.7270527 0.7316911 0.7332872 0.7334402 0.7384122\n##  [7,] 0.7100347 0.7288694 0.7394506 0.7396847 0.7387767 0.7415382\n##  [8,] 0.7152308 0.7363614 0.7428634 0.7431836 0.7450280 0.7444436\n##  [9,] 0.7097638 0.7221877 0.7252510 0.7301520 0.7315682 0.7289940\n## [10,]        NA        NA        NA        NA        NA        NA\n##            [,7]      [,8]      [,9] [,10]\n##  [1,] 0.7196084 0.7276948 0.7265067    NA\n##  [2,] 0.7405149 0.7404706 0.7459368    NA\n##  [3,] 0.7417071 0.7426235 0.7399623    NA\n##  [4,] 0.7473723 0.7508377 0.7525843    NA\n##  [5,] 0.7372777 0.7398243 0.7404263    NA\n##  [6,] 0.7414204 0.7451333 0.7419437    NA\n##  [7,] 0.7438240 0.7396513 0.7403887    NA\n##  [8,] 0.7429152 0.7452253 0.7466324    NA\n##  [9,] 0.7257426 0.7244041 0.7217195    NA\n## [10,]        NA        NA        NA    NA\n</code></pre> <p></p> <p>The following graph compare gbtree vs gblinear.</p> <p></p> <p>We can see clearly, the difference between different booster (non-linear/tree-based vs linear) booster. gblinear offer far more stable and better result compare to gbtree in this case.</p> <p>This result, however, might not reflex the whole situation. As demonstrated with gblinear, objective function play a major role in both performance and stability.</p> <ol> <li> <p>Original Rmd documment provided here \u21a9</p> </li> </ol>"},{"location":"Scientific%20Journey/Presentation-Cacao/main/","title":"Main","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#simulation-of-msa-datasets","title":"SIMULATION OF MSA DATASETS","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#with-ccmgen","title":"WITH CCMGEN","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#and-modeling-coevolution-features","title":"and modeling coevolution features","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#vu-lam-dang","title":"Vu Lam DANG","text":"<p>June 28 2021</p>"},{"location":"Scientific%20Journey/Presentation-Cacao/main/#table-of-content","title":"Table of Content","text":"<ol> <li>Background</li> <li>(Some) Math background</li> <li>Result</li> </ol>"},{"location":"Scientific%20Journey/Presentation-Cacao/main/#background","title":"Background","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#multiple-sequences-alignment","title":"Multiple sequences alignment","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#background_1","title":"Background","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#monte-carlo-simulation","title":"Monte Carlo Simulation","text":"<p>From Wikipedia: Monte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle.</p>"},{"location":"Scientific%20Journey/Presentation-Cacao/main/#in-short-mc-simulation-repeated-random-processes","title":"In short: MC simulation = Repeated random processes","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#markov-chain","title":"Markov Chain","text":"<p>From Wikipedia:A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.</p>"},{"location":"Scientific%20Journey/Presentation-Cacao/main/#some-mathematics","title":"(Some) Mathematics","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#its-short-i-promise","title":"it's short, I promise","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#markov-chain-monte-carlo","title":"Markov chain Monte Carlo","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#markov-chain-monte-carlo_1","title":"Markov chain Monte Carlo","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#probability-model","title":"Probability model","text":"<p>Components of a MSA model: - : Controlling the background (base) frequency of each amino acid at a certain residue  - : Controlling the relationship between a certain aa at a residue to another aa at another residue</p>"},{"location":"Scientific%20Journey/Presentation-Cacao/main/#size-matter","title":"Size matter !!!","text":"<ul> <li>  take the size of  </li> <li>  take the size of  </li> </ul>"},{"location":"Scientific%20Journey/Presentation-Cacao/main/#markov-chain-monte-carlo_2","title":"Markov chain Monte Carlo","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#gibb-sampling","title":"Gibb Sampling","text":"<p>At each step:  </p>"},{"location":"Scientific%20Journey/Presentation-Cacao/main/#markov-chain-monte-carlo_3","title":"Markov chain Monte Carlo","text":"<p>In summary: 1. Generate N random sequences 2. Using Gibbs sampling process, each sequences is mutated until a fix amount of rounds 3. Hopefully we arrive at the desired frequency</p>"},{"location":"Scientific%20Journey/Presentation-Cacao/main/#tree-phylogeny","title":"Tree Phylogeny","text":"<ul> <li>CCMGen allow for creating a Multiple sequence alignment according to a binary tree phylogeny instead of completely randomized (MCMC)</li> <li>Gibbs process is used to generate an ancestral sequence (root node)</li> <li>There is a trade off between retaining the tree structure and converging to the base frequency (see Result section)</li> </ul>"},{"location":"Scientific%20Journey/Presentation-Cacao/main/#result","title":"Result","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#result_1","title":"Result","text":""},{"location":"Scientific%20Journey/Presentation-Cacao/main/#reference","title":"Reference","text":""},{"location":"Scientific%20Journey/Report/report/","title":"Report on combining (Stack) C+T predictor","text":"<p>In this document I investigate multiple method to calculate PRS. Some of them are based on combining multiple C+T predictors to enhence the performance of C+T. They are based on the work of Florian Priv\u00e9. 1</p>"},{"location":"Scientific%20Journey/Report/report/#methods","title":"Methods","text":"<p>In this RMD documents there are 5 methods demonstrated:</p> <ul> <li>SCT</li> <li>XGBoost (with SCT  layer)</li> <li>Lassosum</li> <li>Random Forest (with SCT  layer)</li> <li>Neural Net (with SCT  layer)</li> </ul> <p>More promising (SCT, XGBoost, Lassosum, Random Forest) are place first, and Neural Net are included for reference.</p> <p>For some reason I can not get Keras to work with SCT first layer (always return AUC of 0.5). This can either be a problem of the target function (I have yet to find a good function that operate well), or a bug within Keras (unlikely). Or perhap the Euclid distance after C+T layer are too small for neuralnet - that's why regression based method work better?</p>"},{"location":"Scientific%20Journey/Report/report/#required-packages","title":"Required packages:","text":"<ul> <li>bigstatsr</li> <li>bigsnpr</li> <li>lassosum</li> <li>data.table</li> <li>xgboost</li> <li>ranger</li> <li>keras</li> </ul> <pre><code>library(bigstatsr)\nlibrary(bigsnpr)\nlibrary(lassosum)\nlibrary(data.table)\nlibrary(xgboost)\nlibrary(ranger)\nlibrary(keras)\n</code></pre>"},{"location":"Scientific%20Journey/Report/report/#dataset","title":"Dataset","text":"<p>Please change working direction in <code>directory</code></p> <p>I use the \"Simus\" simulated dataset provided by Florian. This dataset contain ~650,000 SNPs in 2 chromosomes. 20% of them are cases and the rest 80% are controls.</p> <pre><code>setwd(directory) \n# Train data\nsumstats &lt;- bigreadr::fread2(file_sumstat)\n# snp_readBed(paste(file_train, \"bed\", sep='.'))\ntrain &lt;- snp_attach(paste(file_train, \"rds\", sep='.'))\nG.train &lt;- train$genotypes\nCHR &lt;- train$map$chromosome\nPOS &lt;- train$map$physical.pos\nNCORES &lt;- nb_cores()\nlpval &lt;- -log10(sumstats$pval)\ny.train &lt;- train$fam$affection\n\n# Test data\n# snp_readBed(paste(file_test, \"bed\", sep='.'))\ntest &lt;- snp_attach(paste(file_test, \"rds\", sep='.'))\nG.test &lt;- test$genotypes\n# \n</code></pre>"},{"location":"Scientific%20Journey/Report/report/#ct-step","title":"C+T step","text":"<p>Generating a matrix of C+T with varied clumping radius and  thresholds.</p> <p>  and  are aquired in sumstats file (see <code>sumstats.txt</code>)</p> <pre><code>all_keep &lt;- snp_grid_clumping(G.train, CHR, POS, lpval, ncores = NCORES)\nPRS &lt;-snp_grid_PRS(G.train,all_keep = all_keep, betas = sumstats$beta,lpval)\nPRS_test &lt;- snp_grid_PRS(G.test, all_keep = all_keep, betas = sumstats$beta,lpval)\n</code></pre> <p>Other than SCT, other methods require DataFrame or R Matrix than FBM to manage data. I converted the FBM output of C+T to DataFrame and change the corresponding columns.</p>"},{"location":"Scientific%20Journey/Report/report/#data-treatment-oversampling","title":"Data treatment: Oversampling","text":"<p>To oversampling I extract all positive (affection = 1), and append them a number of times. For this simulated dataset, I enhanced the number of cases 10 times, thus the ratio become ~70:30 (from 20:80).</p> <pre><code>y_case &lt;- which(train$fam$affection %in% c(1))\nfor (i in seq(1, 10)){\n  y.train &lt;- append(y.train, train$fam$affection[y_case])\n  ds &lt;- rbind(ds, PRS[y_case,])\n}\nds.FBM &lt;- as_FBM(ds)\n</code></pre>"},{"location":"Scientific%20Journey/Report/report/#method-1-sct","title":"Method 1: SCT","text":"<p>No parameters required. Combining (Sparse Logistic Regression) C+T predictors for best result.</p> <pre><code># Reference\n# M &lt;- snp_grid_stacking(multi_PRS = ds.FBM, y.train = y.train, ncores = NCORES)\nM &lt;- snp_grid_stacking(multi_PRS = PRS, y.train = train$fam$affection)\nbeta &lt;- as_FBM(matrix(M$beta.G))\npred.SCT &lt;- big_prodMat(G.test, beta)\n#AUC(pred = pred.SCT, test$fam$affection)\n</code></pre> <pre><code>AUC(pred = pred.SCT, test$fam$affection)\n</code></pre> <pre><code>## [1] 0.7816448\n</code></pre>"},{"location":"Scientific%20Journey/Report/report/#method-2-xgboost","title":"Method 2: XGBoost","text":"<p>XGBoost provide slightly better result compare to SCT. However this result heavily depend on booster and objective selection. Experiments shown that gblinear booster with count:poisson objective give the best results.</p> <p>For experimentation, I run 9 different max_depth and 9 nrounds. Each combination is repeated 10 times and the mean value calculated to rule out the randomess of gblinear booster</p> <pre><code>AUCs &lt;- matrix(,nrow = 10, ncol = 10)\nfor (i in seq(2 : 10)){\n  for (j in seq(2: 10)) {\n    temp &lt;- c()\n    for(t in seq(1:10)){\n      bstSparse &lt;- xgboost(data = ds[], \n                           label = y.train, \n                           booster=\"gblinear\",\n                           max_depth = i, \n                           eta = 1, \n                           nthread = 2, \n                           nrounds = j, \n                           lambda = 0.1,\n                           objective = \"count:poisson\"\n                           )\n      pred &lt;- predict(bstSparse, PRS_test[])\n      temp &lt;- append(temp,AUC(pred = pred, test$fam$affection))\n    }\n    AUCs[i,j] &lt;- mean(temp)\n  }\n}\n</code></pre> <pre><code>AUCs\n</code></pre> <pre><code>##            [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n##  [1,] 0.7810081 0.7814691 0.7819848 0.7820644 0.7821634 0.7819551\n##  [2,] 0.7815161 0.7816789 0.7819097 0.7820451 0.7820130 0.7820510\n##  [3,] 0.7811385 0.7817903 0.7820209 0.7821792 0.7821172 0.7820448\n##  [4,] 0.7812592 0.7814580 0.7820362 0.7819456 0.7820135 0.7819257\n##  [5,] 0.7811024 0.7814672 0.7818094 0.7819867 0.7820772 0.7820431\n##  [6,] 0.7813253 0.7816065 0.7818425 0.7822446 0.7821884 0.7819205\n##  [7,] 0.7810239 0.7817146 0.7817587 0.7819281 0.7822180 0.7819721\n##  [8,] 0.7811480 0.7820816 0.7818343 0.7820383 0.7820944 0.7819818\n##  [9,] 0.7815687 0.7814758 0.7816728 0.7818929 0.7820611 0.7820187\n## [10,]        NA        NA        NA        NA        NA        NA\n##            [,7]      [,8]      [,9] [,10]\n##  [1,] 0.7817276 0.7816945 0.7814971    NA\n##  [2,] 0.7819033 0.7817853 0.7816005    NA\n##  [3,] 0.7818634 0.7816886 0.7814594    NA\n##  [4,] 0.7819806 0.7818078 0.7813999    NA\n##  [5,] 0.7818562 0.7816829 0.7814363    NA\n##  [6,] 0.7819113 0.7817263 0.7814301    NA\n##  [7,] 0.7818913 0.7817029 0.7815198    NA\n##  [8,] 0.7819087 0.7816597 0.7814085    NA\n##  [9,] 0.7817407 0.7817544 0.7815585    NA\n## [10,]        NA        NA        NA    NA\n</code></pre> <pre><code>boxplot(AUCs)\n</code></pre> <p></p>"},{"location":"Scientific%20Journey/Report/report/#method-3-lassosum","title":"Method 3: Lassosum","text":"<p>Lassosum use L1 regularization to better fit the regression model.</p> <p>For basic L1 regularization of Linear Regression, 2 metaparameters are required: a learning rate  and a regularization parameter . However, lassosum does not require these parameters, as the software automatically scan the parameters space and select the best  and  </p> <pre><code>setwd(directory) \ncor &lt;- p2cor(p = sumstats$pval, n = 8000, sign=sumstats$beta)\n\nout &lt;- lassosum.pipeline(cor =cor, chr=sumstats$chromosome, pos = sumstats$physical.pos, \n                         A1 = sumstats$allele1, A2 = sumstats$allele2,\n                         ref.bfile = file_train, test.bfile = file_test,\n                         LDblocks = LDblocks)\n\nv &lt;- validate(out)\n\nout2 &lt;- subset(out, s=v$best.s, lambda = v$lambda)\nv2 &lt;- validate(out2)\nv2$best.validation.result\n</code></pre> <pre><code>AUC(v$best.pgs, v$pheno)\n</code></pre> <pre><code>## [1] 0.7308175\n</code></pre>"},{"location":"Scientific%20Journey/Report/report/#method-4-random-forest","title":"Method 4: Random Forest","text":"<p>Random forest also give good AUC (above 70%, but less than other methods except for keras). The result highly dependen on the number of trees (more trees equals more convergence), and the learning rate also contribute to an lesser extend.</p> <p>The code in this session will sweep through the parameters: 1 to 10 trees and alpha from 0.1 to 1 (0.1 increment).</p> <pre><code>PRS_test.df &lt;- as.data.frame(PRS_test[])\n\nf &lt;- as.formula(paste(\"y ~\", paste(cols[!cols %in% \"PRS.df\"], collapse = \" + \")))\n\nfor (j in seq(1 : 10)){\n  for (i in seq(1: 10)) {\n    rf &lt;- ranger(f, data = PRS.df, num.trees = j*100, verbose = TRUE, write.forest = TRUE, alpha = (i/10))\n    pred &lt;- predict(rf, PRS_test.df)\n    AUCs[i,j] &lt;- AUC(pred = pred$predictions, test$fam$affection)\n  }\n}\n</code></pre> <pre><code>AUCs\n</code></pre> <pre><code>##            [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n##  [1,] 0.7532205 0.7535048 0.7564268 0.7501722 0.7507132 0.7515333\n##  [2,] 0.7440113 0.7530198 0.7540833 0.7532506 0.7518778 0.7546351\n##  [3,] 0.7545732 0.7500895 0.7514272 0.7556325 0.7545590 0.7539930\n##  [4,] 0.7549227 0.7515300 0.7500226 0.7548174 0.7503537 0.7543692\n##  [5,] 0.7488554 0.7515392 0.7508904 0.7535265 0.7544863 0.7536828\n##  [6,] 0.7501346 0.7512984 0.7525976 0.7547204 0.7520592 0.7536302\n##  [7,] 0.7496137 0.7509414 0.7527130 0.7549386 0.7512415 0.7553466\n##  [8,] 0.7435724 0.7513862 0.7545431 0.7513971 0.7525416 0.7559703\n##  [9,] 0.7513695 0.7512825 0.7537756 0.7543274 0.7507516 0.7562646\n## [10,] 0.7473798 0.7476314 0.7535859 0.7539036 0.7543751 0.7537062\n##            [,7]      [,8]      [,9]     [,10]\n##  [1,] 0.7541560 0.7521771 0.7544930 0.7532481\n##  [2,] 0.7547923 0.7526261 0.7558700 0.7539353\n##  [3,] 0.7526620 0.7529212 0.7531896 0.7540800\n##  [4,] 0.7532573 0.7556534 0.7528978 0.7538868\n##  [5,] 0.7518979 0.7547931 0.7558106 0.7535190\n##  [6,] 0.7544119 0.7547555 0.7558792 0.7548132\n##  [7,] 0.7524614 0.7543283 0.7537857 0.7549896\n##  [8,] 0.7533702 0.7535374 0.7533777 0.7520208\n##  [9,] 0.7569953 0.7558482 0.7558123 0.7530600\n## [10,] 0.7538342 0.7547547 0.7521612 0.7539888\n</code></pre> <pre><code>boxplot(AUCs)\n</code></pre> <p></p>"},{"location":"Scientific%20Journey/Report/report/#method-5-nerual-net-keras","title":"Method 5: Nerual net (Keras)","text":"<p>For some reason, keras cannot return good result; the prediction always biased to controls. Even with adjusted weights and oversampling, the bias is still there and AUC is always . I included it here in case I figure it out in the future.</p> <p>In this example, I put a 10:1 weight ratio between case and control.</p> <pre><code>y.train &lt;- train$fam$affection\n\nmodel &lt;- keras_model_sequential() \nmodel %&gt;%\n layer_dense(units = 1000, activation = \"softmax\", input_shape = c(ncol(ds)), kernel_regularizer=regularizer_l1(0.02)) %&gt;%\n # layer_dense(units = 1000, activation = \"softmax\", input_shape = c(2800)) %&gt;%\n # layer_dropout(rate=0.1) %&gt;%\n layer_dense(units = 2, activation = \"softmax\", kernel_regularizer=regularizer_l1(0.01))\n\nmodel %&gt;% compile(\n #loss = 'categorical_crossentropy',\n loss = 'mean_absolute_percentage_error',\n optimizer = 'SGD',\n metrics = c('accuracy')\n)\ny.train &lt;-  to_categorical(y.train)\n\nhistory &lt;- model$fit(\n ds, y.train,\n class_weight = list(1, 10),\n epochs = as.integer(10), \n batch_size = as.integer(28)\n)\nAUC(predict_classes(model, PRS_test[]), test$fam$affection)\n</code></pre> <pre><code>## [1] 0.5\n</code></pre> <ol> <li> <p>Original Rmd documment provided here \u21a9</p> </li> </ol>"},{"location":"Thought/FabLab-recruit/","title":"FabLab USTH Recruit","text":""},{"location":"Thought/FabLab-recruit/#pipette-2","title":"Pipette 2","text":"<ul> <li>Thuy\u1ebft tr\u00ecnh ch\u01b0a t\u1ed1t.</li> <li>L\u00e0m kh\u00e1 t\u1ed1t, ch\u1ee7 \u0111\u1ed9ng.</li> <li>Ng\u1ecdc, Ph\u01b0\u1ee3ng, Linh c\u00f3 ch\u1ee7 \u0111\u1ed9ng h\u01a1n c\u00e1c b\u1ea1n kh\u00e1c.</li> <li>C\u00f3 c\u1ed1 g\u1eafng thi\u1ebft k\u1ebf th\u00eam. Ch\u01b0a ho\u00e0n thi\u1ec7n nh\u01b0ng c\u00f3 \u00fd t\u01b0\u1edfng.</li> </ul>"},{"location":"Thought/FabLab-recruit/#pipette-1","title":"Pipette 1","text":"<ul> <li>Slide kh\u00e1 t\u1ed1t nh\u01b0ng c\u00f2n l\u1ed7i</li> </ul>"},{"location":"Thought/FabLab-recruit/#cansat-2","title":"Cansat 2","text":"<ul> <li>Kh\u00f4ng s\u1ed1ng s\u00f3t</li> <li>Th\u1eddi gian ng\u1eafn, ti\u1ebft ki\u1ec7m</li> <li>Slide t\u1ed1t</li> </ul>"},{"location":"Thought/FabLab-recruit/#cansat-1","title":"Cansat 1","text":"<ul> <li>Qu\u1ea3n l\u00fd th\u1eddi gian ch\u01b0a t\u1ed1t</li> <li>Slide kh\u00e1 t\u1ed1t, \u0111\u1eb9p nh\u01b0ng nhi\u1ec1u ch\u1eef</li> <li>Hi\u1ec1n Minh, Kh\u00e1nh... ch\u1ecbu kh\u00f3 l\u00e0m</li> <li>(b\u1ea1n l\u00e0m \u0111i\u1ec7n) thuy\u1ebft tr\u00ecnh ch\u01b0a t\u1ed1t</li> <li>Ch\u01b0a truy\u1ec1n \u0111\u01b0\u1ee3c t\u00edn hi\u1ec7u.</li> <li>Thi\u1ebft k\u1ebf t\u1ed1t, kh\u00f4ng th\u1eeba nhi\u1ec1u</li> <li>C\u00f3 t\u00ednh to\u00e1n</li> </ul>"},{"location":"Thought/FabLab-recruit/#xe","title":"Xe","text":"<ul> <li>Mua kit</li> </ul>"},{"location":"Thought/FabLab-recruit/#may-bay","title":"M\u00e1y bay","text":""},{"location":"Thought/Uncle/","title":"Two Snooty Uncles Eating to the Beat","text":"<p>A Short Story</p> <p>by Jane Doe</p> <p>Rick McCallister had always loved cosy Sidney with its robust, raspy rivers. It was a place where he felt unstable.</p> <p>He was a smart, tight-fisted, squash drinker with curvaceous moles and beautiful hands. His friends saw him as an envious, embarrassed elephant. Once, he had even helped an uptight blind person cross the road. That's the sort of man he was.</p> <p>Rick walked over to the window and reflected on his cold surroundings. The sleet rained like talking horses.</p> <p>Then he saw something in the distance, or rather someone. It was the figure of Helen Thornhill. Helen was a noble friend with ginger moles and slimy hands.</p> <p>Rick gulped. He was not prepared for Helen.</p> <p>As Rick stepped outside and Helen came closer, he could see the vacant glint in her eye. \"I am here because I want a pencil,\" Helen bellowed, in a mean tone. She slammed her fist against Rick's chest, with the force of 3526 hamsters. \"I frigging love you, Rick McCallister.\" Rick looked back, even more happy and still fingering the bendy piano. \"Helen, let's move in together,\" he replied.</p> <p>They looked at each other with jumpy feelings, like two knowing, kindhearted koalas drinking at a very down to earth funeral, which had classical music playing in the background and two snooty uncles eating to the beat.</p> <p>Rick regarded Helen's ginger moles and slimy hands. He held out his hand. \"Let's not fight,\" he whispered, gently.</p> <p>\"Hmph,\" pondered Helen.</p> <p>\"Please?\" begged Rick with puppy dog eyes.</p> <p>Helen looked sleepy, her body blushing like a gentle, giant gun.</p> <p>Then Helen came inside for a nice beaker of squash.</p> <p>THE END.</p> <pre><code>Auto Praise for Two Snooty Uncles Eating to the Beat\n\n\"I feel like I know Rick McCallister. In a way, it feels as though I've always known him.\"\n    - The Daily Tale\n\n\"About as enjoyable as being hailed on whilst taking in washing that has been targeted by seagulls with the squits.\"\n    - Enid Kibbler\n\n\"Saying the sleet rained like talking horses is just the kind of literary device that makes this brilliant.\"\n    - Hit the Spoof\n\n\"I could do better.\"\n    - Zob Gloop\n</code></pre>"},{"location":"Thought/mosig-review/","title":"Master Program review: MoSIG year 1","text":""},{"location":"Thought/mosig-review/#abstract","title":"Abstract","text":""},{"location":"Thought/mosig-review/#context","title":"Context","text":"<p>I graduated from University of Science and Technology of Hanoi in the end of 2017. For about 8 months that follows my graduation, I was working in USTH's ICTLab in a bioinformatics project related to plant genomics (Oryza Japonaise in particular). In June 2018 I was offered a place in ENSIMAG's MoSIG (Master of Science in Informatics at Grenoble) programme.</p>"},{"location":"Thought/mosig-review/#the-program","title":"The program","text":"<p>The MoSIG program is a joint program between Grenoble INP's ENSIMAG and Universit\u00e9 Grenoble Alpes' UFR IM\u00b2AG. The 2 years LMD Master program is considered one of the best program in France and Europe for Computer Science and Mathematics, and is entirely taught in English.</p>"},{"location":"Thought/mosig-review/#academic","title":"Academic","text":"<p>The first year of MoSIG aim to provide you with a background knowledge </p>"},{"location":"Thought/mosig-review/#the-internship","title":"The internship","text":""},{"location":"Thought/mosig-review/#student-life","title":"Student life","text":""},{"location":"Thought/mosig-review/#conclusion","title":"Conclusion","text":""},{"location":"Thought/nov-28-17/","title":"Random note #1","text":"<p>Xin ng\u1eaft ch\u01b0\u01a1ng tr\u00ecnh Bioinformatics h\u00e0ng ng\u00e0y \u0111\u1ec3 mang \u0111\u1ebfn nh\u1eefng suy ngh\u0129 r\u1eddi r\u1ea1c (sau 2 chai bia) c\u1ee7a b\u1ea3n th\u00e2n.</p>"},{"location":"Thought/nov-28-17/#khoa-hoc-va-tri-to-mo","title":"Khoa h\u1ecdc v\u00e0 tr\u00ed t\u00f2 m\u00f2","text":"<p>H\u1ecdc \u1edf USTH m\u1ed9t th\u1eddi gian m\u00ecnh nh\u1eadn ra r\u1eb1ng tr\u00ed t\u00f2 m\u00f2 c\u1ee7a sinh vi\u00ean th\u1eadt k\u00ec l\u1ea1. R\u1ea5t nhi\u1ec1u ng\u01b0\u1eddi b\u1ea1n c\u1ee7a m\u00ecnh \u0111\u1ebfn v\u1edbi USTH v\u1edbi m\u1ed9t tinh th\u1ea7n r\u1ea5t c\u1ea7u ti\u1ebfn v\u00e0 mong mu\u1ed1n h\u1ecdc h\u1ecfi r\u1ea5t m\u1ea1nh. M\u1ecdi ng\u01b0\u1eddi \u0111\u1ec1u r\u1ea5t gi\u1ecfi v\u00e0 h\u1ea5p th\u1ee5 r\u1ea5t nhi\u1ec1u ki\u1ebfn th\u1ee9c khoa h\u1ecdc v\u1edbi t\u1ed1c \u0111\u1ed9 l\u00e0m m\u00ecnh ph\u1ea3i ng\u1ea1c nhi\u00ean. Nh\u01b0ng \u0111i\u1ec1u n\u00e0y d\u1eebng l\u1ea1i \u1edf chuy\u00ean m\u00f4n h\u1eb9p c\u1ee7a c\u00e1c b\u1ea1n. Trong v\u00e0i n\u0103m h\u1ecdc \u1edf USTH, vi\u1ec7c ph\u1ed1i h\u1ee3p gi\u1eefa c\u00e1c ng\u00e0nh c\u00f3 li\u00ean quan r\u1ea5t kh\u00f3, v\u00e0 ch\u1ec9 c\u00f3 m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng \u00edt c\u00e1c b\u1ea1n h\u1ecdc c\u1ee7a m\u00ecnh tr\u1edf th\u00e0nh c\u00e1c chuy\u00ean gia \u0111a ng\u00e0nh.</p> <p>\u0110i\u1ec1u n\u00e0y c\u00f3 l\u1ebd b\u1eaft ngu\u1ed3n t\u1eeb m\u1ee5c ti\u00eau h\u1ecdc th\u00e0nh t\u00edch cao c\u1ee7a c\u00e1c sinh vi\u00ean \u0111\u1ec9nh cao, nh\u1eefng ng\u01b0\u1eddi c\u00f3 kh\u1ea3 n\u0103ng nh\u1ea5t \u0111\u1ec3 tr\u1edf th\u00e0nh c\u00e1c chuy\u00ean gia n\u00e0y (hay c\u00e1c nh\u00e0 b\u00e1c h\u1ecdc ha ha). \u0110\u00f4i khi m\u00ecnh ch\u01b0a nh\u1eadn th\u1ea5y s\u1ef1 ham th\u00edch ki\u1ebfn th\u1ee9c n\u00f3i chung c\u1ee7a anh em xung quanh. Khi h\u1ecdc m\u1ed9t ki\u1ebfn th\u1ee9c m\u1edbi m\u1ecdi ng\u01b0\u1eddi lu\u00f4n \u0111\u1eb7t m\u1ed9t c\u00e2u h\u1ecfi - c\u00e2u h\u1ecfi n\u00e0y hay xu\u1ea5t hi\u1ec7n trong c\u00e1c s\u00e1ch selfhelp, m\u00e0 theo m\u00ecnh l\u00e0 ph\u1ea3n t\u00e1c d\u1ee5ng kh\u1ee7ng khi\u1ebfp: \"H\u1ecdc c\u00e1i n\u00e0y \u0111\u1ec3 l\u00e0m g\u00ec\".</p> <p>Th\u1eadt ra m\u00ecnh l\u00e0 ai \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 c\u00e1c b\u1ea1n \u0111\u1ed3ng trang l\u1ee9a c\u1ee7a m\u00ecnh. Ch\u1ec9 l\u00e0... \u0111\u00f4i khi m\u00ecnh th\u1ea5y th\u1eadt c\u00f4 \u0111\u1ed9c tr\u00ean con \u0111\u01b0\u1eddng t\u00ecm ki\u1ebfm tri th\u1ee9c. May c\u0169ng c\u00f3 m\u1ed9t v\u00e0i ng\u01b0\u1eddi b\u1ea1n chia s\u1ebb, nh\u01b0ng ph\u1ea7n l\u1edbn m\u1ecdi ng\u01b0\u1eddi r\u01a1i v\u00e0o nh\u00f3m m\u00ecnh m\u00f4 t\u1ea3 \u1edf tr\u00ean.</p>"},{"location":"Thought/nov-28-17/#oi-ieu-nho-nhoi","title":"\u0110\u00f4i \u0111i\u1ec1u nh\u1ecf nhoi","text":"<p>H\u00f4m nay m\u00ecnh c\u00f3 n\u00f3i 1 2 c\u00e2u v\u1ec1 c\u00e1ch \u0111\u1ec3 m\u00ecnh \u0111\u1ecdc. \u0110\u1ecdc m\u1ed9t b\u00e0i th\u01a1, \u0111\u1ecdc m\u1ed9t cu\u1ed1n s\u00e1ch hay \u0111\u1ecdc m\u1ed9t b\u00e0i b\u00e1o khoa h\u1ecdc, m\u00ecnh \u0111\u1ec1u c\u00f3 m\u1ed9t \"gi\u1ea3i thu\u1eadt\" chung. \u0110\u1ecdc 3 l\u1ea7n. Chi ti\u1ebft c\u00f3 th\u1ec3 xem \u1edf b\u00e0i n\u00e0y</p> <p>Gi\u1ea3i thu\u1eadt \u0111\u1ec3 \u0111\u1ecdc n\u00f3i chung \u0111\u01a1n gi\u1ea3n th\u1ebf n\u00e0y:</p> <ol> <li>L\u1ea7n \u0111\u1ea7u \u0111\u1ecdc cho bi\u1ebft</li> <li>L\u1ea7n hai \u0111\u1ecdc cho hi\u1ec3u</li> <li>L\u1ea7n ba \u0111\u1ecdc cho vui</li> </ol>"},{"location":"Thought/nov-28-17/#the-gioi-nhu-toi-thay","title":"Th\u1ebf gi\u1edbi nh\u01b0 t\u00f4i th\u1ea5y","text":"<p>M\u1ed9t s\u1ed1 ng\u01b0\u1eddi bi\u1ebft m\u00ecnh c\u00f3 th\u1ec3 s\u1ebd bi\u1ebft m\u00ecnh c\u00f3 ch\u1ee5p \u1ea3nh, cho vui. \u0110\u00f3 l\u00e0 m\u1ed9t trong s\u1ed1 nh\u1eefng th\u00fa vui gi\u1eef cho m\u00ecnh c\u00f3 m\u1ed9t tr\u1ea1ng th\u00e1i c\u00e2n b\u1eb1ng v\u00e0 t\u1ec9nh t\u00e1o trong cu\u1ed9c s\u1ed1ng.</p> <p>Nhi\u1ebfp \u1ea3nh v\u1edbi m\u00ecnh c\u0169ng l\u00e0 m\u1ed9t s\u1ef1 ph\u1ea3n \u00e1nh c\u00e1ch m\u00ecnh t\u00ecm hi\u1ec3u cu\u1ed9c s\u1ed1ng v\u00e0 khoa h\u1ecdc. M\u1ed9t c\u00e1ch nh\u00ecn m\u1edbi, c\u00e1ch nh\u00ecn kh\u00e1c v\u1ec1 cu\u1ed9c s\u1ed1ng, v\u1ec1 s\u1ef1 v\u1eadt xung quanh m\u00ecnh: \u0111\u01b0a th\u00eam m\u1ed9t l\u1edbp k\u00ednh v\u00e0o gi\u1eefa m\u00ecnh v\u00e0 s\u1ef1 v\u1eadt, nh\u00ecn m\u1ecdi th\u1ee9 t\u1eeb m\u1ed9t g\u00f3c nh\u00ecn kh\u00e1c.</p> <p>Ti\u00eau \u0111\u1ec1 ph\u1ea7n n\u00e0y l\u1ea5y t\u1eeb m\u1ed9t quy\u1ec3n s\u00e1ch c\u1ee7a Einstein Th\u1ebf gi\u1edbi nh\u01b0 t\u00f4i th\u1ea5y</p>"},{"location":"Thought/on-writing/","title":"V\u1ec1 chuy\u1ec7n vi\u1ebft l\u00e1ch","text":"<p>Okay, v\u1eady l\u00e0 m\u00ecnh \u0111\u00e3 kh\u00f4ng th\u00e0nh c\u00f4ng (l\u1eafm) trong m\u1ee5c ti\u00eau vi\u1ebft b\u00e0i tr\u01b0\u1edbc ng\u00e0y sinh nh\u1eadt m\u00ecnh (th\u1eadt ra c\u0169ng vi\u1ebft nhi\u1ec1u h\u01a1n s\u1ed1 post l\u00ean \u0111\u00e2y). M\u1ed9t s\u1ed1 l\u00fd do v\u00e0 b\u00e0i h\u1ecdc l\u00e0:</p> <ol> <li> <p>\u0110\u1ea7u ti\u00ean l\u00e0 do m\u00ecnh \u0111\u00e3 r\u1ea5t b\u1eadn r\u1ed9n trong th\u1eddi gian m\u1ed9t th\u00e1ng v\u1eeba r\u1ed3i. \u0110\u00e2y l\u00e0 m\u1ed9t l\u00fd do c\u0169 m\u00e8m v\u00e0 c\u1ed5 l\u1ed7 nh\u01b0ng r\u1ea5t th\u1eadt. Gi\u1eefa vi\u1ec7c nghi\u00ean c\u1ee9u t\u1ea1i ICTLab - IRD, nghi\u00ean c\u1ee9u t\u1ea1i FabLab USTH, qu\u1ea3n tr\u1ecb v\u00e0 h\u01b0\u1edbng d\u1eabn c\u00e1c em m\u1edbi v\u00e0o FabLab, h\u01b0\u1edbng d\u1eabn c\u00e1c b\u1ea1n Biologist, th\u1eddi gian d\u00e0nh cho b\u1ea3n th\u00e2n m\u00ecnh kh\u00f4ng c\u00f3 nhi\u1ec1u. Ph\u1ea7n l\u1edbn th\u1eddi gian ri\u00eang n\u00e0y m\u00ecnh d\u00e0nh cho b\u1ea1n g\u00e1i m\u00ecnh.</p> </li> <li> <p>Th\u00e0nh th\u1ef1c m\u00e0 n\u00f3i, \u0111\u1ed9ng l\u1ef1c \u0111\u1ec3 vi\u1ebft c\u1ee7a m\u00ecnh gi\u1ea3m s\u00fat trong th\u1eddi gian v\u1eeba qua. M\u1ed9t l\u00fd do l\u1edbn \u0111\u1ec3 m\u00ecnh b\u1eaft \u0111\u1ea7u vi\u1ebft l\u00e0 do s\u1ef1 m\u1ea5t c\u00e2n b\u1eb1ng v\u00e0 b\u00ed b\u00e1ch trong cu\u1ed9c s\u1ed1ng n\u1ed9i t\u00e2m d\u1eabn \u0111\u1ebfn nhu c\u1ea7u gi\u1ea3i t\u1ecfa ra b\u00ean ngo\u00e0i. N\u00f3i v\u1eady kh\u00f4ng c\u00f3 ngh\u0129a l\u00e0 k\u1ec3 l\u1ec3 hay kh\u00f4ng bu\u1ed3n ch\u00e1n l\u00e0 s\u1ebd kh\u00f4ng vi\u1ebft. Tuy v\u1eady gi\u1eefa m\u1ed9t l\u1ecbch c\u00f4ng vi\u1ec7c ch\u1eadt ch\u1ed9i v\u00e0 t\u00e2m tr\u00ed b\u1ecb chi\u1ebfm d\u1ee5ng (b\u1edfi l\u1ecbch c\u00f4ng vi\u1ec7c tr\u00ean), ng\u01b0\u1eddi ta c\u0169ng kh\u00f3 c\u00f3 th\u1ec3 ng\u1ed3i vi\u1ebft m\u1ed9t b\u00e0i ra h\u1ed3n.</p> </li> <li> <p>M\u1ee5c \u0111\u00edch ch\u00ednh c\u1ee7a trang n\u00e0y l\u00e0 n\u01a1i \u0111\u1ec3 chia s\u1ebb v\u1ec1 khoa h\u1ecdc k\u1ef9 thu\u1eadt - c\u00f9ng m\u1ed9t s\u1ed1 b\u00e0i vi\u1ebft ng\u1eafn. M\u00e0 th\u1eadt ra trong th\u1eddi gian v\u1eeba r\u1ed3i m\u00ecnh c\u0169ng kh\u00f4ng c\u00f3 nhi\u1ec1u \u0111i\u1ec1u \u0111\u1ec3 chia s\u1ebb l\u1eafm. D\u1ed1i v\u1edbi k\u1ef9 thu\u1eadt tin sinh, m\u00ecnh \u0111ang ph\u1ea3i v\u1eadt l\u1ed9n v\u1edbi vi\u1ec7c ph\u00e1t tri\u1ec3n c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng S4 cho R. \u0110i\u1ec1u n\u00e0y kh\u00e1 kh\u00f3 v\u1edbi m\u1ed9t ng\u01b0\u1eddi h\u1ecdc l\u1eadp tr\u00ecnh theo ki\u1ec3u formal nh\u01b0 m\u00ecnh (s\u1ebd c\u1ea7n c\u1ea3 m\u1ed9t b\u00e0i ri\u00eang \u0111\u1ec3 n\u00f3i v\u1ec1 qu\u00e1 tr\u00ecnh v\u1edbi S4). V\u1edbi c\u00e1c c\u00f4ng vi\u1ec7c \u1edf Fab Lab cu\u1ed1i c\u00f9ng m\u00ecnh \u0111\u00e3 l\u00e0m cho chi\u1ebfc m\u00e1y in nh\u1ecf ch\u1ea1y \u0111\u01b0\u1ee3c - th\u00eam m\u1ed9t m\u1eabu m\u00e1y in trong kho t\u00e0ng c\u1ee7a nh\u00f3m. Ti\u1ebfp theo s\u1ebd l\u00e0 m\u00e1y CNC v\u00e0 c\u00e1c c\u00f4ng ngh\u1ec7 h\u1ed7 tr\u1ee3 cho in 3D (farm, server etc.). C\u00f4ng cu\u1ed9c nghi\u00ean c\u1ee9u khoa h\u1ecdc c\u0169ng c\u00f2n \u0111ang ph\u1ea3i m\u00f2 m\u1eabm trong m\u1ed9t n\u1ed9i dung \u0111\u1ea7y m\u1edbi m\u1ebb v\u00e0 th\u1eed th\u00e1ch \u0111\u1ed1i v\u1edbi m\u00ecnh. C\u1ea7n ph\u1ea3i s\u1ee5c s\u1ea1o, \u0111\u00e0o b\u1edbi \u0111\u1ec3 t\u00ecm c\u00e1c kh\u00eda c\u1ea1nh hay v\u00e0 th\u1eed th\u00e1ch - c\u0169ng nh\u01b0 gi\u1ee5c s\u1ebfp \u0111\u1ec3 ti\u1ebfn \u0111\u1ebfn nh\u1eefng m\u1ea3ng th\u00fa v\u1ecb trong chuy\u00ean m\u00f4n. Hi v\u1ecdng s\u1ebd ra th\u00e0nh qu\u1ea3 tr\u01b0\u1edbc th\u00e1ng 3 - \u0111\u1ec3 c\u00f3 th\u1ec3 y\u00ean t\u00e2m b\u01b0\u1edbc ti\u1ebfp</p> </li> <li> <p>Tuy nh\u1eefng l\u00fd do tr\u00ean s\u1ebd kh\u00f4ng m\u1ea5t \u0111i, v\u00e0 nh\u1eefng d\u1ef1 \u0111\u1ecbnh s\u1eafp t\u1edbi ch\u1ec9 c\u00f3 nhi\u1ec1u th\u00eam, m\u00ecnh v\u1eabn th\u1ef1c s\u1ef1 mu\u1ed1n c\u1ea7m b\u00fat (hay b\u00e0n ph\u00edm). C\u00f3 th\u1ec3 nh\u1eefng ng\u00e0y th\u00e1ng t\u1edbi s\u1ebd kh\u00f4ng vi\u1ebft \u0111\u01b0\u1ee3c nhi\u1ec1u, vi\u1ebft \u0111\u01b0\u1ee3c th\u01b0\u1eddng xuy\u00ean, nh\u01b0ng s\u1ebd l\u00e0 m\u1ed9t vi\u1ec7c m\u00ecnh duy tr\u00ec. \u0110\u00e2y l\u00e0 m\u1ed9t g\u00f3c ri\u00eang c\u1ee7a m\u00ecnh \u0111\u1ec3 chia s\u1ebb. Mu\u1ed1n \u0111\u01b0\u1ee3c nh\u01b0 v\u1eady m\u1ed9t s\u1ed1 \u0111i\u1ec1u sau m\u00ecnh s\u1ebd ph\u1ea3i \u00e1p d\u1ee5ng v\u1edbi b\u1ea3n th\u00e2n:</p> <ol> <li>Ng\u1eebng vi\u1ec7c \u0111\u1eb7t m\u1ee5c ti\u00eau, l\u1ecbch vi\u1ebft. \u0110i\u1ec1u n\u00e0y ch\u1ec9 c\u00f3 l\u1ee3i v\u1edbi m\u1ed9t s\u1ed1 vi\u1ec7c, m\u1ed9t s\u1ed1 ng\u01b0\u1eddi c\u1ee5 th\u1ec3. N\u00f3 s\u1ebd kh\u00f4ng hi\u1ec7u qu\u1ea3 n\u1ebfu vi\u1ec7c \u0111ang l\u00e0m kh\u00f4ng mang l\u1ea1i incentive ho\u1eb7c l\u00e0m m\u1ea5t h\u1ee9ng th\u00fa. Ch\u1ec9 c\u00f4ng vi\u1ec7c m\u1edbi n\u00ean c\u00f3 \u00e1p l\u1ef1c nh\u01b0 v\u1eady. Vi\u1ebft, tr\u01b0\u1edbc h\u1ebft, l\u00e0 vui.</li> <li>Vi\u1ebft nh\u00e1p tr\u01b0\u1edbc b\u1eb1ng tay, ra gi\u1ea5y. S\u1edf d\u0129 m\u00ecnh r\u1ea5t d\u1ec5 c\u1ee5t h\u1ee9ng (v\u00e0 c\u00f3 nhi\u1ec1u b\u00e0i vi\u1ebft m\u1ed9t n\u1eeda) l\u00e0 do \u0111ang c\u00f3 \u00fd t\u01b0\u1edfng, c\u00f3 m\u1ea1ch hay nh\u01b0ng l\u1ea1i kh\u00f4ng c\u00f3 s\u1eb5n m\u00e1y t\u00ednh, m\u00e0 v\u1ec1 sau l\u1ea1i qu\u00ean m\u1ea5t, r\u1ea5t kh\u00f3 ch\u1ecbu. Vi\u1ebft c\u0169ng s\u1ebd bu\u1ed9c l\u1eddi v\u0103n ph\u1ea3i c\u1ea9n th\u1eadn v\u00e0 trau chu\u1ed1t h\u01a1n.</li> <li>C\u1ea7n c\u00f3 c\u00e1c \u0111\u1ec3 c\u00f3 ph\u1ea3n h\u1ed3i v\u00e0 t\u01b0\u01a1ng t\u00e1c. C\u00f3 th\u1ec3 s\u1ebd repost l\u00ean Wordpress v\u00e0 Facebook. Vi\u1ebft m\u00e0 kh\u00f4ng \u0111\u01b0\u1ee3c \u0111\u1ecdc c\u0169ng c\u1ea3m th\u1ea5y thi\u1ebfu thi\u1ebfu. Ai \u0111\u00f3 c\u00f3 th\u1ec3 b\u1ea3o ta s\u1ed1ng \u1ea3o, nh\u01b0ng th\u1eadt ra trong th\u1ebf gi\u1edbi \u1ea3o c\u00e0ng c\u1ea7n m\u1edf to m\u1eaft c\u0103ng r\u1ed9ng tai \u0111\u1ec3 m\u00e0 nghe, m\u00e0 h\u1ecdc. Confirmation bias l\u00e0 c\u00f3 th\u1eadt.</li> </ol> </li> </ol>"},{"location":"Thought/the-death-of-expertise-vi/","title":"C\u00e1i ch\u1ebft c\u1ee7a chuy\u00ean m\u00f4n","text":"<p>Tom Nichols</p> <p>L\u1eddi ng\u01b0\u1eddi d\u1ecbch:</p> <p>\u0110\u00e2y l\u00e0 m\u1ed9t b\u00e0i vi\u1ebft kh\u00e1 hay c\u1ee7a t\u00e1c gi\u1ea3 Tom Nichols. Tuy n\u00f3 \u0111\u00e3 c\u0169 nh\u01b0ng th\u1eddi gian ch\u1ec9 ch\u1ee9ng minh c\u00e0ng l\u00fac n\u00f3 c\u00e0ng c\u00f3 ngh\u0129a.</p> <p>T\u00f4i (ho\u1eb7c \u00edt ra t\u00f4i t\u1ef1 nh\u1eadn) l\u00e0 m\u1ed9t chuy\u00ean vi\u00ean. Kh\u00f4ng ph\u1ea3i v\u1ec1 t\u1ea5t c\u1ea3 m\u1ecdi th\u1ee9, m\u00e0 l\u00e0 trong m\u1ed9t l\u0129nh v\u1ef1c h\u1eb9p c\u1ee7a ki\u1ebfn th\u1ee9c con ng\u01b0\u1eddi: Khoa h\u1ecdc X\u00e3 h\u1ed9i v\u00e0 ch\u00ednh s\u00e1ch c\u00f4ng. Khi t\u00f4i \u0111\u01b0a ra l\u1eadp lu\u1eadn v\u1ec1 m\u1ed9t ch\u1ee7 \u0111\u1ec1, t\u00f4i k\u1ef3 v\u1ecdng \u00fd ki\u1ebfn c\u1ee7a t\u00f4i s\u1ebd c\u00f3 tr\u1ecdng l\u01b0\u1ee3ng h\u01a1n nh\u1eefng ng\u01b0\u1eddi kh\u00f4ng c\u00f3 chuy\u00ean m\u00f4n.</p> <p>T\u00f4i kh\u00f4ng th\u1ec3 ng\u1edd \u0111\u01b0\u1ee3c r\u1eb1ng nh\u1eefng c\u00e2u tr\u00ean th\u1eadt scandal. H\u00f3a ra ch\u00fang l\u00e0 nh\u1eefng c\u00e2u n\u00f3i g\u00e2y ra ch\u1ec9 tr\u00edch. Ng\u00e0y nay, m\u1ecdi s\u1ef1 tuy\u00ean b\u1ed1 v\u1ec1 m\u1eb7t chuy\u00ean m\u00f4n \u0111\u1ec1 g\u00e2y ra m\u1ed9t c\u01a1n ch\u1ea5n \u0111\u1ed9ng d\u1eef d\u1ed9i t\u1eeb m\u1ed9t g\u00f3c c\u1ee7a coogn ch\u00fang Hoa K\u1ef3, nh\u1eefng ng\u01b0\u1eddi ngay l\u1eadp t\u1ee9c ch\u1ec9 tay r\u1eb1ng l\u1eadp lu\u1eadn n\u00e0y ho\u00e0n to\u00e0n ch\u1ec9 l\u00e0 ng\u1ee5y bi\u1ec7n , l\u00e0 d\u1ea5u hi\u1ec7u c\u1ee7a nh\u1eefng tay b\u00e0n gi\u1ea5y \u0111\u00e1ng khinh b\u1ec9, v\u00e0 l\u00e0 m\u1ed9t n\u1ed7 l\u1ef1c \u0111\u00e1ng x\u1ea5u h\u1ed5 \u0111\u1ec3 d\u00f9ng uy t\u00edn d\u1eadp t\u1eaft nh\u1eefng cu\u1ed9c tranh lu\u1eadn d\u00e2n ch\u1ee7.</p> <p>M\u1eb7c d\u00f9 n\u1ec1n d\u00e2n ch\u1ee7, nh\u01b0 t\u00f4i \u0111\u00e3 n\u00f3i trong m\u1ed9t ti\u1ec3u lu\u1eadn v\u1ec1 C.S. Lewis v\u00e0 v\u1ee5 vi\u1ec7c Snowden, th\u1ef1c ch\u1ea5t l\u00e0 m\u1ed9t h\u1ec7 th\u1ed1ng ch\u00ednh quy\u1ec1n, ch\u1ee9 kh\u00f4ng ph\u1ea3i l\u00e0 m\u1ed9t tr\u1ea1ng th\u00e1i c\u00f4ng b\u1eb1ng nh\u01b0 qu\u00fd v\u1ecb h\u00ecnh dung. \u0110i\u1ec1u \u0111\u00f3 c\u00f3 ngh\u0129a ch\u00fang ta t\u1eadn h\u01b0\u1edfng nh\u1eefng \u0111\u1eb7c quy\u1ec1n t\u01b0\u01a1ng \u0111\u01b0\u01a1ng tr\u01b0\u1edbc ch\u00ednh quy\u1ec1n v\u00e0 gi\u1eefa m\u1ed7i c\u00e1 nh\u00e2n. C\u00f3 quy\u1ec1n l\u1ef1c ngang h\u00e0ng, \u0111i\u1ec1u n\u00e0y kh\u00f4ng c\u00f3 ngh\u0129a hai ng\u01b0\u1eddi ngang h\u00e0ng v\u1ec1 t\u00e0i n\u0103ng, kh\u1ea3 n\u0103ng hay ki\u1ebfn th\u1ee9c. N\u00f3 ch\u1eafc ch\u1eafn kh\u00f4ng c\u00f3 ngh\u0129a \u00fd ki\u1ebfn c\u1ee7a m\u1ed7i c\u00e1 nh\u00e2n \u0111\u1ec1u c\u00f3 gi\u00e1 tr\u1ecb nh\u01b0 m\u1ecdi c\u00e1i nh\u00e2n kh\u00e1c. V\u1eady m\u00e0, \u0111\u00f3 l\u1ea1i l\u00e0 m\u1ed9t c\u00e1ch nh\u00ecn nh\u1eadn c\u1ee7a kh\u00e1 nhi\u1ec1u ng\u01b0\u1eddi, b\u1ea5t ch\u1ea5p s\u1ef1 v\u00f4 l\u00fd c\u1ee7a n\u00f3</p>"},{"location":"Thought/the-death-of-expertise-vi/#chuyen-gi-ang-dien-ra","title":"Chuy\u1ec7n g\u00ec \u0111ang di\u1ec5n ra?","text":"<p>T\u00f4i e r\u1eb1ng ch\u00fang ta \u0111ang ch\u1ee9ng ki\u1ebfn c\u00e1i ch\u1ebft c\u1ee7a chuy\u00ean m\u00f4n: S\u1ef1 s\u1ee5p \u0111\u1ed5 c\u1ee7a b\u1ea5t c\u1ee9 s\u1ef1 ph\u00e2n chia n\u00e0o gi\u1eefa nh\u1eefng ng\u01b0\u1eddi c\u00f3 chuy\u00ean m\u00f4n v\u00e0 qu\u1ea7n ch\u00fang, h\u1ecdc sinh v\u00e0 gi\u00e1o vi\u00ean, nh\u1eefng ng\u01b0\u1eddi th\u1eafc m\u1eafc v\u00e0 nh\u1eefng ng\u01b0\u1eddi hi\u1ec3u r\u00f5 - n\u00f3i m\u1ed9t c\u00e1ch kh\u00e1c, gi\u1eefa nh\u1eefng ng\u01b0\u1eddi \u0111\u1ea1t \u0111\u01b0\u1ee3c th\u00e0nh t\u1ef1u trong m\u1ed9t l\u0129nh v\u1ef1c c\u1ee5 th\u1ec3 v\u00e0 nh\u1eefng k\u1ebb b\u1ea5t \u0111\u1eafc ch\u00ed. S\u1ef1 s\u1ee5p \u0111\u1ed5 n\u00e0y kh\u00f4ng \u0111\u1ed3ng ngh\u0129a v\u1edbi s\u1ef1 s\u1ee5p \u0111\u1ed5 th\u1ef1c s\u1ef1 c\u1ee7a ki\u1ebfn th\u1ee9c chuy\u00ean ng\u00e0nh - nh\u1eefng hi\u1ec3u bi\u1ebft v\u1ec1 s\u1ef1 vi\u1ec7c, s\u1ef1 v\u1eadt c\u1ee5 th\u1ec3 \u0111\u1eb7t ra s\u1ef1 kh\u00e1c bi\u1ec7t gi\u1eefa nh\u1eefng c\u00e1 nh\u00e2n trong x\u00e3 h\u1ed9i. S\u1ebd lu\u00f4n lu\u00f4n c\u00f3 nh\u1eefng b\u00e1c s\u0129, lu\u1eadt s\u01b0, k\u1ef9 s\u01b0 v\u00e0 nh\u1eefng chuy\u00ean gia trong t\u1eebng l\u0129nh v\u1ef1c. \u0110i\u1ec1u t\u00f4i lo s\u1ee3 l\u00e0 s\u1ef1 b\u1ea5t c\u00f4ng nh\u1eadn v\u1ec1 m\u1eb7t chuy\u00ean m\u00f4n, th\u1ee9 s\u1ebd thanh \u0111\u1ed5i c\u00e1ch ch\u00fang ta s\u1ed1ng v\u00e0 suy ngh\u0129.</p> <p>\u0110\u00e2y l\u00e0 m\u1ed9t \u0111i\u1ec1u t\u1ed3i t\u1ec7. \u0110\u00fang, c\u00e1c chuy\u00ean gia c\u0169ng c\u00f3 th\u1ec3 c\u00f3 sai s\u00f3t, t\u1eeb s\u1ef1 c\u1ed1 thalidomide cho \u0111\u1ebfn v\u1ee5 n\u1ed5 t\u00e0u Challenger \u0111\u1ec1u nh\u1eafc nh\u1edf ch\u00fang ta v\u1ec1 s\u1ef1 th\u1eadt n\u00e0y. Tuy v\u1eady, c\u00e1c chuy\u00ean gia c\u00f3 t\u1ec9 l\u1ec7 \u0111\u00fang cao h\u01a1n nh\u1eefng ng\u01b0\u1eddi b\u00ecnh th\u01b0\u1eddng. B\u00e1c s\u0129, d\u00f9 h\u1ecd c\u00f3 th\u1ec3 c\u00f3 sai s\u00f3t, v\u1eabn ngon h\u01a1n nh\u1eefng b\u00e0 \u0111\u1ed3ng ch\u1eefa b\u1ec7nh b\u1eb1ng ngo\u1ea1i c\u1ea3m hay ru\u1ed9t g\u00e0 \u0111\u1eb7c ch\u1ebf b\u1edfi b\u00e0 c\u00f4 b\u00ean ngo\u1ea1i. T\u1eeb ch\u1ed1i quan \u0111i\u1ec3m chuy\u00ean m\u00f4n v\u00e0 thay th\u1ebf n\u00f3 b\u1eb1ng \u00fd t\u01b0\u1edfng m\u1ecdi \u00fd ki\u1ebfn \u0111\u1ec1u c\u00f3 tr\u1ecdng l\u01b0\u1ee3ng ngang nhau l\u00e0 m\u1ed9t \u0111i\u1ec1u ng\u1edb ng\u1ea9n.</p> <p>T\u1ec7 h\u01a1n n\u1eefa, n\u00f3 c\u00f2n nguy hi\u1ec3m. C\u00e1i ch\u1ebft c\u1ee7a chuy\u00ean m\u00f4n l\u00e0 s\u1ef1 ch\u1ed1i b\u1ecf kh\u00f4ng ch\u1ec9 v\u1edbi ki\u1ebfn th\u1ee9c, m\u00e0 c\u00f2n v\u1edbi con \u0111\u01b0\u1eddng ch\u00fang ta theo \u0111u\u1ed5i h\u1ecdc v\u1ea5n v\u00e0 t\u00ecm ra hi\u1ec3u bi\u1ebft - n\u1ec1n t\u1ea3ng c\u1ee7a n\u1ec1n v\u0103n minh Ph\u01b0\u01a1ng T\u00e2y. \u0110\u00fang, t\u00f4i n\u00f3i \u0111\u1ebfn n\u1ec1n v\u0103n minh Ph\u01b0\u01a1ng T\u00e2y: c\u00e1i n\u1ec1n v\u0103n minh ph\u1ee5 h\u1ec7, ph\u00e1t x\u00edt, ph\u00e2n bi\u1ec7t ch\u1ee7ng t\u1ed9c, nh\u1eefng k\u1ebb \u0111\u00e3 t\u1ea1o ra bom nguy\u00ean t\u1eed, xe Edsel, n\u01b0\u1edbc t\u0103ng l\u1ef1c, m\u1eafc ch\u1ee9ng b\u00e9o ph\u00ec nh\u01b0ng c\u0169ng h\u1ea1 c\u00e1nh m\u00e1y bay trong \u0111\u00eam t\u1ed1i v\u00e0 vi\u1ebft ra nh\u1eefng v\u0103n b\u1ea3n t\u1ea7m c\u1ee1 Hi\u1ebfn ch\u01b0\u01a1ng Li\u00ean H\u1ee3p Qu\u1ed1c.</p> <p>\u0110i\u1ec1u n\u00e0y kh\u00f4ng ch\u1ec9 li\u00ean quan \u0111\u1ebfn ch\u00ednh tr\u1ecb, d\u00f9 th\u1ebf \u0111\u00e3 qu\u00e1 t\u1ec7. Kh\u00f4ng, th\u1ef1c t\u1ebf c\u00f2n t\u1ec7 h\u01a1n nhi\u1ec1u: </p> <p>This isn\u2019t just about politics, which would be bad enough. No, it\u2019s worse than that: the perverse effect of the death of expertise is that without real experts, everyone is an expert on everything. To take but one horrifying example, we live today in an advanced post-industrial country that is now fighting a resurgence of whooping cough \u2014 a scourge nearly eliminated a century ago \u2014 merely because otherwise intelligent people have been second-guessing their doctors and refusing to vaccinate their kids after reading stuff written by people who know exactly zip about medicine. (Yes, I mean people like Jenny McCarthy).</p>"},{"location":"Thought/the-death-of-expertise/","title":"The Death Of Expertise","text":"<p>Tom Nichols</p> <p>I am (or at least think I am) an expert. Not on everything, but in a particular area of human knowledge, specifically social science and public policy. When I say something on those subjects, I expect that my opinion holds more weight than that of most other people.</p> <p>I never thought those were particularly controversial statements. As it turns out, they\u2019re plenty controversial. Today, any assertion of expertise produces an explosion of anger from certain quarters of the American public, who immediately complain that such claims are nothing more than fallacious appeals to authority, sure signs of dreadful elitism, and an obvious effort to use credentials to stifle the dialogue required by a real democracy.</p> <p>But democracy, as I wrote in an essay about C.S. Lewis and the Snowden affair, denotes a system of government, not an actual state of equality. It means that we enjoy equal rights versus the government, and in relation to each other. Having equal rights does not mean having equal talents, equal abilities, or equal knowledge.  It assuredly does not mean that everyone\u2019s opinion about anything is as good as anyone else\u2019s. And yet, this is now enshrined as the credo of a fair number of people despite being obvious nonsense.</p>"},{"location":"Thought/the-death-of-expertise/#whats-going-on-here","title":"What\u2019s going on here?","text":"<p>I fear we are witnessing the death of expertise: a Google-fueled, Wikipedia-based, blog-sodden collapse of any division between professionals and laymen, students and teachers, knowers and wonderers \u2013 in other words, between those of any achievement in an area and those with none at all. By this, I do not mean the death of actual expertise, the knowledge of specific things that sets some people apart from others in various areas. There will always be doctors, lawyers, engineers, and other specialists in various fields. Rather, what I fear has died is any acknowledgement of expertise as anything that should alter our thoughts or change the way we live.</p> <p>What has died is any acknowledgement of expertise as anything that should alter our thoughts or change the way we live.</p> <p>This is a very bad thing. Yes, it\u2019s true that experts can make mistakes, as disasters from thalidomide to the Challenger explosion tragically remind us. But mostly, experts have a pretty good batting average compared to laymen: doctors, whatever their errors, seem to do better with most illnesses than faith healers or your Aunt Ginny and her special chicken gut poultice. To reject the notion of expertise, and to replace it with a sanctimonious insistence that every person has a right to his or her own opinion, is silly.</p> <p>Worse, it\u2019s dangerous. The death of expertise is a rejection not only of knowledge, but of the ways in which we gain knowledge and learn about things. Fundamentally, it\u2019s a rejection of science and rationality, which are the foundations of Western civilization itself. Yes, I said Western civilization: that paternalistic, racist, ethnocentric approach to knowledge that created the nuclear bomb, the Edsel, and New Coke, but which also keeps diabetics alive, lands mammoth airliners in the dark, and writes documents like the Charter of the United Nations.</p> <p>This isn\u2019t just about politics, which would be bad enough. No, it\u2019s worse than that: the perverse effect of the death of expertise is that without real experts, everyone is an expert on everything. To take but one horrifying example, we live today in an advanced post-industrial country that is now fighting a resurgence of whooping cough \u2014 a scourge nearly eliminated a century ago \u2014 merely because otherwise intelligent people have been second-guessing their doctors and refusing to vaccinate their kids after reading stuff written by people who know exactly zip about medicine. (Yes, I mean people like Jenny McCarthy).</p> <p>In politics, too, the problem has reached ridiculous proportions. People in political debates no longer distinguish the phrase you\u2019re wrong from the phrase you\u2019re stupid. To disagree is to insult. To correct another is to be a hater. And to refuse to acknowledge alternative views, no matter how fantastic or inane, is to be closed-minded.</p>"},{"location":"Thought/the-death-of-expertise/#how-conversation-became-exhausting","title":"How conversation became exhausting","text":"<p>Critics might dismiss all this by saying that everyone has a right to participate in the public sphere. That\u2019s true. But every discussion must take place within limits and above a certain baseline of competence. And competence is sorely lacking in the public arena. People with strong views on going to war in other countries can barely find their own nation on a map; people who want to punish Congress for this or that law can\u2019t name their own member of the House.</p> <p>People with strong views on going to war in other countries can barely find their own nation on a map.</p> <p>None of this ignorance stops people from arguing as though they are research scientists. Tackle a complex policy issue with a layman today, and you will get snippy and sophistic demands to show ever increasing amounts of proof or evidence for your case, even though the ordinary interlocutor in such debates isn\u2019t really equipped to decide what constitutes evidence or to know it when it\u2019s presented. The use of evidence is a specialized form of knowledge that takes a long time to learn, which is why articles and books are subjected to peer review and not to everyone review, but don\u2019t tell that to someone hectoring you about the how things really work in Moscow or Beijing or Washington.</p> <p>This subverts any real hope of a conversation, because it is simply exhausting \u2014 at least speaking from my perspective as the policy expert in most of these discussions \u2014 to have to start from the very beginning of every argument and establish the merest baseline of knowledge, and then constantly to have to negotiate the rules of logical argument. (Most people I encounter, for example, have no idea what a non-sequitur is, or when they\u2019re using one; nor do they understand the difference between generalizations and stereotypes.) Most people are already huffy and offended before ever encountering the substance of the issue at hand.</p> <p>Once upon a time \u2014 way back in the Dark Ages before the 2000s \u2014 people seemed to understand, in a general way, the difference between experts and laymen. There was a clear demarcation in political food fights, as objections and dissent among experts came from their peers \u2014 that is, from people equipped with similar knowledge. The public, largely, were spectators.</p> <p>This was both good and bad. While it strained out the kook factor in discussions (editors controlled their letters pages, which today would be called moderating), it also meant that sometimes public policy debate was too esoteric, conducted less for public enlightenment and more as just so much dueling jargon between experts.</p> <p>If experts go back to only talking to each other, that\u2019s bad for democracy.</p> <p>No one \u2014 not me, anyway \u2014 wants to return to those days. I like the 21st century, and I like the democratization of knowledge and the wider circle of public participation. That greater participation, however, is endangered by the utterly illogical insistence that every opinion should have equal weight, because people like me, sooner or later, are forced to tune out people who insist that we\u2019re all starting from intellectual scratch. (Spoiler: We\u2019re not.) And if that happens, experts will go back to only talking to each other. And that\u2019s bad for democracy.</p>"},{"location":"Thought/the-death-of-expertise/#the-downside-of-no-gatekeepers","title":"The downside of no gatekeepers","text":"<p>How did this peevishness about expertise come about, and how can it have gotten so immensely foolish?</p> <p>Some of it is purely due to the globalization of communication. There are no longer any gatekeepers: the journals and op-ed pages that were once strictly edited have been drowned under the weight of self-publishable blogs. There was once a time when participation in public debate, even in the pages of the local newspaper, required submission of a letter or an article, and that submission had to be written intelligently, pass editorial review, and stand with the author\u2019s name attached. Even then, it was a big deal to get a letter in a major newspaper.</p> <p>Now, anyone can bum rush the comments section of any major publication. Sometimes, that results in a free-for-all that spurs better thinking. Most of the time, however, it means that anyone can post anything they want, under any anonymous cover, and never have to defend their views or get called out for being wrong.</p> <p>Another reason for the collapse of expertise lies not with the global commons but with the increasingly partisan nature of U.S. political campaigns. There was once a time when presidents would win elections and then scour universities and think-tanks for a brain trust; that\u2019s how Henry Kissinger, Samuel Huntington, Zbigniew Brzezinski and others ended up in government service while moving between places like Harvard and Columbia.</p> <p>This is the code of the samurai, not the intellectual, and it privileges the campaign loyalist over the expert.</p> <p>Those days are gone. To be sure, some of the blame rests with the increasing irrelevance of overly narrow research in the social sciences. But it is also because the primary requisite of seniority in the policy world is too often an answer to the question: What did you do during the campaign? This is the code of the samurai, not the intellectual, and it privileges the campaign loyalist over the expert.</p> <p>I have a hard time, for example, imagining that I would be called to Washington today in the way I was back in 1990, when the senior Senator from Pennsylvania asked a former U.S. Ambassador to the UN who she might recommend to advise him on foreign affairs, and she gave him my name. Despite the fact that I had no connection to Pennsylvania and had never worked on his campaigns, he called me at the campus where I was teaching, and later invited me to join his personal staff.</p> <p>Universities, without doubt, have to own some of this mess. The idea of telling students that professors run the show and know better than they do strikes many students as something like uppity lip from the help, and so many profs don\u2019t do it. (One of the greatest teachers I ever had, James Schall, once wrote many years ago that students have obligations to teachers, including trust, docility, effort, and thinking, an assertion that would produce howls of outrage from the entitled generations roaming campuses today.) As a result, many academic departments are boutiques, in which the professors are expected to be something like intellectual valets. This produces nothing but a delusion of intellectual adequacy in children who should be instructed, not catered to.</p>"},{"location":"Thought/the-death-of-expertise/#the-confidence-of-the-dumb","title":"The confidence of the dumb","text":"<p>There\u2019s also that immutable problem known as human nature. It has a name now: it\u2019s called the Dunning-Kruger effect, which says, in sum, that the dumber you are, the more confident you are that you\u2019re not actually dumb. And when you get invested in being aggressively dumb\u2026well, the last thing you want to encounter are experts who disagree with you, and so you dismiss them in order to maintain your unreasonably high opinion of yourself. (There\u2019s a lot of that loose on social media, especially.)</p> <p>All of these are symptoms of the same disease: a manic reinterpretation of democracy in which everyone must have their say, and no one must be disrespected. (The verb to disrespect is one of the most obnoxious and insidious innovations in our language in years, because it really means to fail to pay me the impossibly high requirement of respect I demand.) This yearning for respect and equality, even\u2014perhaps especially\u2014if unearned, is so intense that it brooks no disagreement. It represents the full flowering of a therapeutic culture where self-esteem, not achievement, is the ultimate human value, and it\u2019s making us all dumber by the day.</p> <p>Thus, at least some of the people who reject expertise are not really, as they often claim, showing their independence of thought. They are instead rejecting anything that might stir a gnawing insecurity that their own opinion might not be worth all that much.</p>"},{"location":"Thought/the-death-of-expertise/#experts-the-servants-not-masters-of-a-democracy","title":"Experts: the servants, not masters, of a democracy","text":"<p>So what can we do? Not much, sadly, since this is a cultural and generational issue that will take a long time come right, if it ever does. Personally, I don\u2019t think technocrats and intellectuals should rule the world: we had quite enough of that in the late 20th century, thank you, and it should be clear now that intellectualism makes for lousy policy without some sort of political common sense. Indeed, in an ideal world, experts are the servants, not the masters, of a democracy.</p> <p>But when citizens forgo their basic obligation to learn enough to actually govern themselves, and instead remain stubbornly imprisoned by their fragile egos and caged by their own sense of entitlement, experts will end up running things by default. That\u2019s a terrible outcome for everyone.</p> <p>Expertise is necessary, and it\u2019s not going away. Unless we return it to a healthy role in public policy, we\u2019re going to have stupider and less productive arguments every day. So here, presented without modesty or political sensitivity, are some things to think about when engaging with experts in their area of specialization.</p> <ol> <li>We can all stipulate: the expert isn\u2019t always right.</li> <li>But an expert is far more likely to be right than you are. On a question of factual interpretation or evaluation, it shouldn\u2019t engender insecurity or anxiety to think that an expert\u2019s view is likely to be better-informed than yours. (Because, likely, it is.)</li> <li>Experts come in many flavors. Education enables it, but practitioners in a field acquire expertise through experience; usually the combination of the two is the mark of a true expert in a field. But if you have neither education nor experience, you might want to consider exactly what it is you\u2019re bringing to the argument.</li> <li>In any discussion, you have a positive obligation to learn at least enough to make the conversation possible. The University of Google doesn\u2019t count. Remember: having a strong opinion about something isn\u2019t the same as knowing something.</li> <li>And yes, your political opinions have value. Of course they do: you\u2019re a member of a democracy and what you want is as important as what any other voter wants. As a layman, however, your political analysis, has far less value, and probably isn\u2019t \u2014 indeed, almost certainly isn\u2019t \u2014 as good as you think it is.</li> </ol> <p>And how do I know all this? Just who do I think I am?</p> <p>Well, of course: I\u2019m an expert.</p>"},{"location":"Thought/tiresome/","title":"M\u1ecfi","text":"<pre><code>Ai,\n\nN\u00e0o \u0111\u00e2u mu\u1ed1n gi\u00e0\n\nTh\u1eddi gian b\u1ea1c \u00e1c\n</code></pre> <p>T\u00e2m tr\u1ea1ng ng\u00e0y h\u00f4m nay c\u1ee7a m\u00ecnh kh\u00f4ng t\u1ed1t. C\u00f3 l\u1ebd kh\u00f4ng n\u00ean vi\u1ebft nh\u01b0ng m\u00e0 t\u1eeb h\u00f4m qua \u0111\u00e3 vi\u1ebft r\u1ed3i n\u00ean k\u1ec7.</p>"},{"location":"Thought/tiresome/#mot-trang-thai-cua-tri-nao","title":"M\u1ed9t tr\u1ea1ng th\u00e1i c\u1ee7a tr\u00ed n\u00e3o","text":"<p>A state of mind.</p> <p>Nh\u1eefng l\u00fac n\u00e0y l\u00e0 nh\u1eefng l\u00fac m\u00ecnh c\u1ea7n con ng\u01b0\u1eddi. \u0110\u00fang h\u01a1n l\u00e0 c\u1ea7n \u0111\u00fang ng\u01b0\u1eddi.</p> <p>T\u00f4i l\u00e0 m\u1ed9t con ng\u01b0\u1eddi c\u00f4 \u0111\u1ed9c. C\u00f4 \u0111\u1ed9c kh\u00f4ng ph\u1ea3i do l\u1ed7i c\u1ee7a t\u00f4i, m\u00e0 th\u1eadt ra l\u1ea1i ch\u00ednh l\u00e0 do t\u00f4i. Do ch\u00ednh b\u1ea3n th\u00e2n m\u00ecnh l\u00e0 m\u00ecnh, n\u00ean m\u00ecnh b\u1ecb c\u00f4 \u0111\u1ed9c. S\u1ed1ng m\u1ed9t cu\u1ed9c \u0111\u1eddi kh\u00f4ng \u0111\u1ec1 ph\u00f2ng b\u1ea5t c\u1ee9 ai, m\u1edf l\u00f2ng v\u1edbi t\u1ea5t c\u1ea3 m\u1ecdi ng\u01b0\u1eddi n\u00ean t\u1ea5t th\u1ea3y m\u1ecdi ng\u01b0\u1eddi l\u1ea1i \u0111\u1ec1 ph\u00f2ng. C\u00f3 l\u1ebd c\u00f3 m\u1ed9t s\u1ef1 c\u00e2n b\u1eb1ng \u1edf \u0111\u00e2y?</p>"},{"location":"Thought/tiresome/#toi-luon-rat-muon-hat-nhung-khuc-ca-bong-chay","title":"T\u00f4i lu\u00f4n r\u1ea5t mu\u1ed1n h\u00e1t nh\u1eefng kh\u00fac ca b\u1ecfng ch\u00e1y","text":"<pre><code>H\u00f2a m\u00ecnh v\u00e0o d\u00f2ng ng\u01b0\u1eddi t\u1eebng ng\u00e0y kh\u00f4ng l\u1eabn m\u00ecnh v\u1edbi ai\n\nKh\u00f4ng cho m\u00ecnh nhi\u1ec1u \u0111\u1eafn \u0111o khi \u0111\u1ee9ng tr\u01b0\u1edbc nh\u1eefng l\u1ed1i r\u1ebd\n\nB\u00ecnh th\u1ea3n trong gian nan, tin \u1edf ch\u00ednh m\u00ecnh\n</code></pre> <p>V\u00e0o nh\u1eefng l\u00fac ch\u00e1n n\u1ea3n th\u1ebf n\u00e0y t\u00f4i l\u1ea1i t\u00ecm v\u1ec1 ch\u00fa L\u1eadp. L\u1eddi ca c\u1ee7a ch\u00fa l\u00e0 tu\u1ed5i tr\u1ebb c\u1ee7a t\u00f4i, l\u00e0 ti\u1ebfng l\u00f2ng c\u1ee7a t\u00f4i.</p> <p>Kh\u00f4ng c\u00f3 nh\u1ea1c c\u1ee7a B\u1ee9c t\u01b0\u1eddng v\u00e0 Tr\u1ea7n L\u1eadp, kh\u00f4ng hi\u1ec3u t\u00f4i s\u1ebd s\u1ed1ng \u0111\u1ebfn gi\u1edd n\u00e0y b\u1eb1ng c\u00e1ch n\u00e0o. M\u1ed7i khi t\u00f4i c\u1ea3m th\u1ea5y tuy\u1ec7t v\u1ecdng, m\u1ed7i khi cu\u1ed9c \u0111\u1eddi h\u1eaft h\u1ee7i nh\u1eefng \u0111i\u1ec1u nh\u1ecf nhoi m\u00e0 t\u00f4i vun v\u00e9n trong cu\u1ed9c s\u1ed1ng, ch\u1ec9 c\u00f3 ch\u00fa L\u1eadp m\u1edbi hi\u1ec3u \u0111\u01b0\u1ee3c kh\u00e1t khao \u0111\u01b0\u1ee3c s\u1ed1ng m\u1ed9t cu\u1ed9c \u0111\u1eddi nh\u1ecf b\u00e9.</p> <p>M\u1ed7i l\u00fac tr\u1edf n\u00ean tuy\u1ec7t v\u1ecdng, t\u00f4i th\u01b0\u1eddng ngh\u0129 \u0111\u1ebfn s\u1ef1 k\u1ebft th\u00fac. N\u00f3i nh\u01b0 m\u1ed9t cu\u1ed1n ti\u1ec3u thuy\u1ebft th\u00ec k\u1ebft th\u00fac l\u1ea1i m\u00e0 m\u1ed9t s\u1ef1 b\u1eaft \u0111\u1ea7u m\u1edbi. Nh\u1ea3m nh\u00ed. K\u1ebft th\u00fac l\u00e0 h\u1ebft. L\u00e0 gi\u1ea3i tho\u00e1t kh\u1ecfi nh\u1eefng c\u00f9m x\u00edch v\u00e2y quanh c\u1ed5. L\u00e0 tr\u1ed1n ch\u1ea1y kh\u1ecfi nh\u1eefng ni\u1ec1m vui v\u00e0 \u0111am m\u00ea cu\u1ed9c s\u1ed1ng. M\u1ed9t ng\u00e0y 15 tu\u1ed5i, t\u00f4i ng\u1ed3i b\u00ean ngo\u00e0i lan can s\u00e2n th\u01b0\u1ee3ng nh\u00e0 t\u00f4i. M\u1ed9t n\u1ed7i \u00e1m \u1ea3nh d\u00e2ng l\u00ean t\u1eeb l\u1ed3ng ng\u1ef1c: n\u1ebfu c\u00f3 m\u1ed9t c\u00e1i g\u00ec \u0111\u00f3 k\u00e9o ch\u00e2n m\u00ecnh xu\u1ed1ng... \u0110\u00f3 l\u00e0 n\u1ed7i \u00e1m \u1ea3nh th\u01b0\u1eddng tr\u1ef1c nh\u1ea5t c\u1ee7a t\u00f4i. L\u00e0 c\u00e1ch m\u00e0 trong nh\u1eefng c\u01a1n m\u01a1 c\u1ee7a m\u1ed9t gi\u1ea5c ng\u1ee7 ch\u1eadp ch\u1eddn t\u00f4i th\u1ea5y s\u1ef1 k\u1ebft th\u00fac c\u1ee7a m\u00ecnh. \u0110\u1ebfn gi\u1edd t\u00f4i c\u0169ng kh\u00f4ng c\u00f2n nh\u1edb m\u00ecnh c\u00f3 th\u1ef1c s\u1ef1 tuy\u1ec7t v\u1ecdng v\u00e0o c\u00e1i th\u1eddi \u0111i\u1ec3m \u0111\u01b0a c\u1ea3 hai ch\u00e2n ra kh\u1ecfi lan can, hay ch\u1ec9 l\u00e0 s\u1ef1 ng\u00f4ng cu\u1ed3ng c\u1ee7a m\u1ed9t tu\u1ed5i tr\u1ebb b\u1ecb k\u00ecm n\u00e9n?</p> <p>M\u1ed9t ng\u00e0y n\u00e0o \u0111\u00f3 t\u00f4i s\u1ebd r\u1eddi xa kh\u1ecfi \u0111\u00e2y. \u0110\u00f3 v\u1edbi t\u00f4i c\u0169ng nh\u01b0 l\u00e0 m\u1ed9t c\u00e1i ch\u1ebft nho nh\u1ecf. T\u00f4i y\u00eau m\u1ea3nh \u0111\u1ea5t n\u00e0y, t\u00f4i y\u00eau th\u00e0nh ph\u1ed1 c\u1ee7a t\u00f4i, v\u1edbi nh\u1eefng \u0111i\u1ec1u nho nh\u1ecf m\u00e0 ch\u1ec9 c\u00f2n m\u1ed9t m\u00ecnh t\u00f4i g\u00ecn gi\u1eef. V\u1edbi nh\u1eefng \u0111i\u1ec1u t\u1ed1t \u0111\u1eb9p m\u00e0 ch\u1ec9 m\u00ecnh t\u00f4i t\u1ef1 h\u00e0o, v\u1edbi nh\u1eefng l\u1ecbch s\u1eed m\u00e0 ch\u1ec9 m\u00ecnh t\u00f4i ghi nh\u1edb. Nh\u01b0ng ch\u01b0a m\u1ed9t ng\u00e0y t\u00f4i ngh\u0129 t\u00f4i s\u1ebd x\u00e2y d\u1ef1ng m\u1ed9t cu\u1ed9c s\u1ed1ng \u1edf th\u00e0nh ph\u1ed1 n\u00e0y. M\u1ed9t ng\u00e0y n\u00e0o \u0111\u00f3 H\u00e0 N\u1ed9i s\u1ebd kh\u00f4ng c\u00f2n m\u1ed9t thanh ni\u00ean th\u1ea5p b\u00e9 \u0111i l\u1ea1i tr\u00ean nh\u1eefng v\u1ec9a h\u00e8, nh\u00f2m v\u00e0o nh\u1eefng con ng\u00f5, \u0111\u01b0a m\u00e1y \u1ea3nh m\u00ecnh v\u00e0o nh\u1eefng khung h\u00ecnh, nh\u1eefng c\u1ea3nh v\u1eadt m\u00e0 kh\u00f4ng ai \u0111\u1ec3 \u00fd, kh\u00f4ng ai ch\u00fa \u00fd \u0111\u1ebfn.</p> <p>Nh\u01b0ng c\u0169ng gi\u1ed1ng nh\u01b0 H\u00e0 N\u1ed9i \u0111\u00e3 kh\u00f4ng c\u00f2n m\u1ed9t c\u1ee5 gi\u00e0 vi\u1ebft t\u1ea3n v\u0103n, hay m\u1ed9t ngh\u1ec7 s\u0129 y\u00eau xe, c\u00f3 l\u1ebd m\u1ed9t H\u00e0 N\u1ed9i kh\u00f4ng c\u00f3 m\u00ecnh s\u1ebd ch\u1eb3ng l\u00e0 m\u1ed9t s\u1ef1 bi\u1ebfn chuy\u1ec3n g\u00ec. Nh\u01b0ng t\u1eebng l\u1edbp ng\u01b0\u1eddi H\u00e0 N\u1ed9i r\u1eddi \u0111i mang theo m\u1ed9t m\u1ea3nh c\u1ee7a qu\u00ea h\u01b0\u01a1ng, m\u00e0 H\u00e0 N\u1ed9i th\u00ec thay \u0111\u1ed5i nhanh ch\u00f3ng.</p> <p>Th\u00f4i th\u00ec c\u0169ng l\u00e0 m\u1ed9t m\u1ea3nh k\u00ed \u1ee9c d\u00f4ng d\u00e0i \u0111\u1ec3 l\u1ea1i \u1edf \u0111\u00e2y.</p>"},{"location":"USTH/note%2010-Jun/","title":"note 10 Jun","text":""},{"location":"USTH/note%2010-Jun/#pipeline","title":"Pipeline","text":"<ol> <li>Retrive</li> </ol>"},{"location":"USTH/note%2010-Jun/#process","title":"Process","text":"<ol> <li>L\u1ea5y K (Kegg_ko) -&gt; tra c\u1ee9u KEGG</li> <li>L\u1ea5y module &amp; pathway</li> <li>Tr\u1ea3 v\u1ec1 b\u1ea3ng r\u00fat g\u1ecdn (g\u1ed3m name v\u00e0 c\u00e1c id)</li> </ol>"},{"location":"USTH/process/","title":"Pipeline and process of Python package","text":""},{"location":"USTH/process/#pipeline","title":"Pipeline","text":"<ol> <li> <p>Process incoming query sequence</p> <ol> <li>Single simple query # Elaborate<ol> <li>1 database with one gene</li> </ol> </li> <li>Bulk query<ol> <li>Identify databases</li> <li>Collasping similar ids/query string into list of unique id</li> <li> <p>Create query matrix</p> <p>Query matrix is a table of unique ids/query parameters and database. Each cell constitude to an individual single query</p> </li> </ol> </li> </ol> </li> <li> <p>Fetch information about databases</p> <ol> <li>Database information is stored in a database (metadatabase)</li> <li>Differential between database is intuitively indexed base on both name and function of the database<ol> <li>In current version, each function of the databases if accessed differently is considered separated database. For example funricegene db is divided into 3 different databases' entry.</li> <li>In future revision, function of the databases will be integrated into the description of database using template metaprogramming technique.</li> </ol> </li> <li>Part of html page that correspond to the data is exacted per definition store in database<ol> <li>Current version define the area of data and parse datas into cell according to list of headers</li> <li>Future version will define individual cells and actively search for each headers' content</li> </ol> </li> <li>Database definition is stored in database objects. Functions: # in future revision     #list of accessor and query function here. # in future revision</li> </ol> </li> <li> <p>Running the actual query</p> <ol> <li>Powered by BeautifulSoup, Requests and CSV module</li> <li>Requests built using information stored in db</li> <li>Possiblity for paralelization<ol> <li>Different IP address/proxy server etc.</li> <li>Survey on how different databases/server handle parallel query (limitation, DDOS etc.)</li> </ol> </li> </ol> </li> <li> <p>Post-processing of data</p> <ol> <li> <p>Text clean up</p> <p>Option for * Regular Expression * Python script (In future version) * Gawk (In future version)     1. Trimming and filterings</p> <p>Option for * Regular Expression * Python script (In future version) * (R ?) (In future version)     1. Exporting 1. Single query:     * CSV     * Excel     * JSON     * RDF 2. Query Matrix:     * Flat CSV file</p> <pre><code>    Cell with multiple data will be delimited with `;`\n* Structured folder\n\n    Each folder correspond to one database in the bulk query\n1. Chain query(?)\n</code></pre> </li> </ol> </li> </ol>"},{"location":"USTH/process/#database-object","title":"Database Object","text":"<ol> <li>Storable object</li> <li>Attributes:</li> <li>Functions:<ul> <li>Accessors</li> <li>Execute Query</li> </ul> </li> </ol>"},{"location":"USTH/process/#result-return","title":"Result return","text":"<ol> <li>CSV</li> </ol>"},{"location":"setups/setups/","title":"My current setup","text":""},{"location":"setups/setups/#personal-setup","title":"Personal setup","text":"<p>These are machines that I build and paid with my own money - Framework Laptop (2022)\\      - i5 1240p   - 32GB DDR4 2333Mhz \\   Savaged from old machines   - 500GB Gen 4 NVMe \\   Again, savaged from old machines!   - 2xUSB C; USB A; DP \\   The beauty of the Framework laptop is that you can choose your own IO setup!</p> <p>Currently, Ubuntu 22.04 with kernel 6.0 is the main OS on this machine. Kernel 6.0 with its new scheduler work pretty well with new  gen Intel chip. There are still some driver problem but I chart it to an Intel problem rather than a Framework specific prob.</p>"},{"location":"setups/setups/#_1","title":"My current setup","text":"<ul> <li>Desktop (circa 2020)</li> <li>Ryzen 5 3500x</li> <li>32GB DDR4 (3200Mhz)</li> <li>RTX3060 8GB</li> <li>Fractal Design Node 202</li> </ul> <p>Nothing to serious about this machine except 1) it's small and 2) play my games</p>"},{"location":"setups/setups/#work-setup","title":"Work setup","text":"<ul> <li>Macbook Air M1 (2022)</li> <li>M1</li> <li>RAM 8GB</li> <li>SSD 256GB\\   Nothing special about this. Macbook Air base model is great since they are 1. cheaper 2. VERY long lasting battery. My personal Framework laptop will blow it out the window in term of performance though</li> </ul>"}]}