{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"My Personal Website/Blog thinggy My name is Lam, Dang. I'm a master student at UGA's MoSIG program (Grenoble, France). My speciality including Software Engineering, Artificial Intelligence and Data Retrieval. For M1 internship I am working under Dr. Michael Blum's supervision to develop new method to calculate Polygenic Risk Score. For the last year I had been working on a bioinformatic project under supervision of Dr. Pierre Larmande at USTH's ICTLab, developing an engine to query from multiple genomics databases. Beside working in informatics, I was the founder of a Fab Lab and Technology Club at USTH. Builder of several machines and design, I am well versed in (very informal) mechanical design and CAD/CAM. Favorite flavor of CAD: Fusion 360.","title":"Home"},{"location":"#my-personal-websiteblog-thinggy","text":"My name is Lam, Dang. I'm a master student at UGA's MoSIG program (Grenoble, France). My speciality including Software Engineering, Artificial Intelligence and Data Retrieval. For M1 internship I am working under Dr. Michael Blum's supervision to develop new method to calculate Polygenic Risk Score. For the last year I had been working on a bioinformatic project under supervision of Dr. Pierre Larmande at USTH's ICTLab, developing an engine to query from multiple genomics databases. Beside working in informatics, I was the founder of a Fab Lab and Technology Club at USTH. Builder of several machines and design, I am well versed in (very informal) mechanical design and CAD/CAM. Favorite flavor of CAD: Fusion 360.","title":"My Personal Website/Blog thinggy"},{"location":"CV/","text":"DANG Vu Lam M1 MoSIG, Universite Grenoble Alpes Email : dangv@etu.univ-grenoble-alpes.fr Personal Contact : lam.dv@live.com Phone number : (+33) 6 68 36 02 29 Github | Linkedin | Webpage RESEARCH INTEREST My current research interest including, but not limited to: Neural Network: Deep Learning and Deep Neural Net, Extreme Learning Machine. Optimization algorithms: Swarm Intelligent, Swarm Optimization and Metaheuristic. Data Science: Advance Data Structure, Indexing, Knowledge Representation. Application of Neural Networks: Bioinformatics, Robotics, Image Processing, Pattern Recognition. Mathematics: Linear Algebra, Algorithm and Complexity, Graph Theory. EDUCATION Universtiy Grenoble Alpes Sep. 2018 - Current Master of Science in Informatics UFR IMAG Universtiy of Science and Technology of Hanoi Oct. 2013 - Oct 2017 Bachelor of Science and Technology Falcuty of Information and Communication Technology Thesis topic : Extreme Learning Machine Nguyen Tat Thanh High School Hanoi National University of Education Sep 2010 - Sep 2013 WORKING EXPRIENCES rRice project, IRD Oct 2017 \u2013 Aug 2018 Under the supervision of Dr. Pierre Larmande and Dr. Ho Bich Hai, I helped develop an R package and accompanying Python library to mine genomics data across multiple databases. Position : Programmer, Researcher. ICTLab, University of Science and Technology of Hanoi Apr 2017 \u2013 Oct 2017 As part of the collaboration between CVUT (Prague, Czech Republic) and USTH (Hanoi, Vietnam), I have done an internship under the supervision of Professor Basterrech (CVUT) that resulted in my bachelor thesis. We developed metaheuristic and swarm optimization algorithms for improving Extreme Learning Neural Networks. Position : Intern Researcher. FabLab USTH May 2015 \u2013 Aug 2018 FabLab are global network of Digital Fabrication Laboratory and workshops with the purpose of enabling invention, innovation and hacking of physical world by providing access to digital fabrication tools. In 2015, I organized the FabLab at USTH as a student club with the aim to advocate and support said goals and inciting hacking mentality for students at USTH Position : Founding Member, Mentor. FabLab Vietnam Foundation May 2015 \u2013 2016 FabLab Vietnam Foundation is a consortium of all FabLabs operates in Vietnam. At the time consists of 5 Labs, our purpose is to promote the development of FabLabs style facilities and Maker Movement in Vietnam. In 2015, the Foundation received a grant of $100,000 from IPP to further expand the movement. Position : Member, Representation of FabLab USTH. Dream Project Incubator September 2015 \u2013 2016 Dream Project Incubator aims to transform ambitious, self-driven young Vietnamese into better thinkers and doers via intellectual engagement and mentorship. Every year the project award 4 scholarship for ambitious Vietnamese whose age are between 19 and 25 for a 3 months summer trip to MIT, USA. After the conclusion of my USA trip, I served in the board of alumini, and as jury member and organizer for 2016 program Position : Alumni, Organizer, Jury. HONOR/ACTIVITIES Dream Project Incubator Jun 2015 Dream Project Incubator is a program setup by MIT PhDs and PhD candidates in collaboration with Boston Global Forum. I was awarded a 3 months stay in Boston for my work with FabLab USTH and FabLab Vietnam Foundation, and the chances to meet and confer with top minds from MIT and Harvard University. 3rd prize National gifted students\u2019 competition in Informatics 2007, 2008 and 2009 Held by Vietnam Ministry of Information Technology (now Ministry of Information and Communication) SKILLS Mathematics Linear Algebra; Graph Theory; Numerical and Combinatorial Optimization; Algorithms Design and Computational Complexity Theory. Sofware Engineering Python 3.0; C, C++, C#; Java; NodeJS; R, Matlab, Tensorflow; Ruby on Rails; Arduino and Processing. Hardware, Engineering, Design and Fabrication CAD Design (CATIA, Solidwork, Fusion 360); Metal Working; CNC machining; 3D printing; Electronics. Language English \u2003IELTS Academic 7.5 (as of 2018) \u2003Completed my Bachelor program in English Vietnamese \u2003First language French \u2003Beginner ^ In Vietnamese, the last name is DANG","title":"Curriculum Vitae"},{"location":"CV/#dang-vu-lam","text":"M1 MoSIG, Universite Grenoble Alpes Email : dangv@etu.univ-grenoble-alpes.fr Personal Contact : lam.dv@live.com Phone number : (+33) 6 68 36 02 29 Github | Linkedin | Webpage","title":"DANG Vu Lam"},{"location":"CV/#research-interest","text":"My current research interest including, but not limited to: Neural Network: Deep Learning and Deep Neural Net, Extreme Learning Machine. Optimization algorithms: Swarm Intelligent, Swarm Optimization and Metaheuristic. Data Science: Advance Data Structure, Indexing, Knowledge Representation. Application of Neural Networks: Bioinformatics, Robotics, Image Processing, Pattern Recognition. Mathematics: Linear Algebra, Algorithm and Complexity, Graph Theory.","title":"RESEARCH INTEREST"},{"location":"CV/#education","text":"","title":"EDUCATION"},{"location":"CV/#universtiy-grenoble-alpes","text":"Sep. 2018 - Current Master of Science in Informatics UFR IMAG","title":"Universtiy Grenoble Alpes"},{"location":"CV/#universtiy-of-science-and-technology-of-hanoi","text":"Oct. 2013 - Oct 2017 Bachelor of Science and Technology Falcuty of Information and Communication Technology Thesis topic : Extreme Learning Machine","title":"Universtiy of Science and Technology of Hanoi"},{"location":"CV/#nguyen-tat-thanh-high-school","text":"Hanoi National University of Education Sep 2010 - Sep 2013","title":"Nguyen Tat Thanh High School"},{"location":"CV/#working-expriences","text":"","title":"WORKING EXPRIENCES"},{"location":"CV/#rrice-project-ird","text":"Oct 2017 \u2013 Aug 2018 Under the supervision of Dr. Pierre Larmande and Dr. Ho Bich Hai, I helped develop an R package and accompanying Python library to mine genomics data across multiple databases. Position : Programmer, Researcher.","title":"rRice project, IRD"},{"location":"CV/#ictlab-university-of-science-and-technology-of-hanoi","text":"Apr 2017 \u2013 Oct 2017 As part of the collaboration between CVUT (Prague, Czech Republic) and USTH (Hanoi, Vietnam), I have done an internship under the supervision of Professor Basterrech (CVUT) that resulted in my bachelor thesis. We developed metaheuristic and swarm optimization algorithms for improving Extreme Learning Neural Networks. Position : Intern Researcher.","title":"ICTLab, University of Science and Technology of Hanoi"},{"location":"CV/#fablab-usth","text":"May 2015 \u2013 Aug 2018 FabLab are global network of Digital Fabrication Laboratory and workshops with the purpose of enabling invention, innovation and hacking of physical world by providing access to digital fabrication tools. In 2015, I organized the FabLab at USTH as a student club with the aim to advocate and support said goals and inciting hacking mentality for students at USTH Position : Founding Member, Mentor.","title":"FabLab USTH"},{"location":"CV/#fablab-vietnam-foundation","text":"May 2015 \u2013 2016 FabLab Vietnam Foundation is a consortium of all FabLabs operates in Vietnam. At the time consists of 5 Labs, our purpose is to promote the development of FabLabs style facilities and Maker Movement in Vietnam. In 2015, the Foundation received a grant of $100,000 from IPP to further expand the movement. Position : Member, Representation of FabLab USTH.","title":"FabLab Vietnam Foundation"},{"location":"CV/#dream-project-incubator","text":"September 2015 \u2013 2016 Dream Project Incubator aims to transform ambitious, self-driven young Vietnamese into better thinkers and doers via intellectual engagement and mentorship. Every year the project award 4 scholarship for ambitious Vietnamese whose age are between 19 and 25 for a 3 months summer trip to MIT, USA. After the conclusion of my USA trip, I served in the board of alumini, and as jury member and organizer for 2016 program Position : Alumni, Organizer, Jury.","title":"Dream Project Incubator"},{"location":"CV/#honoractivities","text":"","title":"HONOR/ACTIVITIES"},{"location":"CV/#dream-project-incubator_1","text":"Jun 2015 Dream Project Incubator is a program setup by MIT PhDs and PhD candidates in collaboration with Boston Global Forum. I was awarded a 3 months stay in Boston for my work with FabLab USTH and FabLab Vietnam Foundation, and the chances to meet and confer with top minds from MIT and Harvard University.","title":"Dream Project Incubator"},{"location":"CV/#3rd-prize-national-gifted-students-competition-in-informatics","text":"2007, 2008 and 2009 Held by Vietnam Ministry of Information Technology (now Ministry of Information and Communication)","title":"3rd prize National gifted students\u2019 competition in Informatics"},{"location":"CV/#skills","text":"Mathematics Linear Algebra; Graph Theory; Numerical and Combinatorial Optimization; Algorithms Design and Computational Complexity Theory. Sofware Engineering Python 3.0; C, C++, C#; Java; NodeJS; R, Matlab, Tensorflow; Ruby on Rails; Arduino and Processing. Hardware, Engineering, Design and Fabrication CAD Design (CATIA, Solidwork, Fusion 360); Metal Working; CNC machining; 3D printing; Electronics. Language English \u2003IELTS Academic 7.5 (as of 2018) \u2003Completed my Bachelor program in English Vietnamese \u2003First language French \u2003Beginner ^ In Vietnamese, the last name is DANG","title":"SKILLS"},{"location":"Log/Nov 28/","text":"Nov 28 2017 Meeting with master bio students Python: Basic data structure Binary tree B tree List, Linked list Object oriented Confident with the language: Some people can work with the language, need test on the problem solving skill and data structure analysis Problem statement: Technical Problem Query data from different databases, in different domaians Databases have different data types and data structure Need to combine the data Biological Problem Gene function analyses (Rice genenome) * Look for the gene match the phenotype Seminar Heterogeneity of databases","title":"Nov 28 2017"},{"location":"Log/Nov 28/#nov-28-2017","text":"","title":"Nov 28 2017"},{"location":"Log/Nov 28/#meeting-with-master-bio-students","text":"Python: Basic data structure Binary tree B tree List, Linked list Object oriented Confident with the language: Some people can work with the language, need test on the problem solving skill and data structure analysis Problem statement: Technical Problem Query data from different databases, in different domaians Databases have different data types and data structure Need to combine the data Biological Problem Gene function analyses (Rice genenome) * Look for the gene match the phenotype","title":"Meeting with master bio students"},{"location":"Log/Nov 28/#seminar","text":"Heterogeneity of databases","title":"Seminar"},{"location":"Master/Rec-let/","text":"Ho Bich Hai Institute of Information Technology\\ University of Science and Techonology of Hanoi\\ Vietnam Academy of Science and Technology Subject: Recommendation letter for MoSIG application To Whom It May Concern, My name is Ho Bich Hai. It is my pleasure to recommend Lam Dang to your program, Master of Science in Informatics at Grenoble. Mr. Dang has been a remarkably driven assistant and a valuable asset to our project illustrated by my points below. About the project, we have been working on unified access to genomics databases regarding rice ( oryza sativa ). Lam has rewritten execution code in Python that resulted in a product beyond satisfaction. In addition to his excellent work, there are other qualities about his workstyle that I would like to note in this letter. First, I would like to note Lam's outstanding capability to communicate with me as his supervisor. Our workflow entails outlining details and executing upon tight schedule and strict deadlines. Lam's ability to communicate well has made this process remarkably easy for both of us and enhanced productivity. Regarding my work nature, I need to work remotely often. Still, Lam manages to report his progress to me on a frequent basis and through Asana, he made it easy for me to keep track with his work. Second, his initiative is far beyond expectation. In our group discussions, Lam has been the most proactive in contributing ideas to solve those problems and discussing solutions with team members. One time he initiated to offer a function to resolve a member's difficulty to merge two interdependent parts of work from two Python and R scripts, which allowed the member to proceed with his next step. This is only one circumstance that exemplifies his tendency to go far beyond his own work which is already superb. In short, Lam has both excellent attention to details and a grasp for big picture that facilitates any project he is a part of. Your program will equip him with more cutting-edge skillsets in Data Science and further his success. Therefore, I strongly recommend him for your Master's program. Sincerely, Ho Bich Hai","title":"Rec let"},{"location":"Master/Rec-let/#subject-recommendation-letter-for-mosig-application","text":"To Whom It May Concern, My name is Ho Bich Hai. It is my pleasure to recommend Lam Dang to your program, Master of Science in Informatics at Grenoble. Mr. Dang has been a remarkably driven assistant and a valuable asset to our project illustrated by my points below. About the project, we have been working on unified access to genomics databases regarding rice ( oryza sativa ). Lam has rewritten execution code in Python that resulted in a product beyond satisfaction. In addition to his excellent work, there are other qualities about his workstyle that I would like to note in this letter. First, I would like to note Lam's outstanding capability to communicate with me as his supervisor. Our workflow entails outlining details and executing upon tight schedule and strict deadlines. Lam's ability to communicate well has made this process remarkably easy for both of us and enhanced productivity. Regarding my work nature, I need to work remotely often. Still, Lam manages to report his progress to me on a frequent basis and through Asana, he made it easy for me to keep track with his work. Second, his initiative is far beyond expectation. In our group discussions, Lam has been the most proactive in contributing ideas to solve those problems and discussing solutions with team members. One time he initiated to offer a function to resolve a member's difficulty to merge two interdependent parts of work from two Python and R scripts, which allowed the member to proceed with his next step. This is only one circumstance that exemplifies his tendency to go far beyond his own work which is already superb. In short, Lam has both excellent attention to details and a grasp for big picture that facilitates any project he is a part of. Your program will equip him with more cutting-edge skillsets in Data Science and further his success. Therefore, I strongly recommend him for your Master's program. Sincerely, Ho Bich Hai","title":"Subject: Recommendation letter for MoSIG application"},{"location":"Master/grenoble/","text":"Grenoble Application/Motivation letter What is your long-terme professional ambition I wish to be a expert in the field of computer science, able to solve complex problem, both technical and theoretical, across multiple fields with the help of data science tools. How will the master academic program you apply for help you to reach this ambition As a science student, I have interest across many different fields of expertise, and Data Science is the tool and methodology for me to understand complex problem in the world. I strongly believe that with a diverse course structure offered by Grenoble INP will not only strengthen my understanding of mathematics, algorithm and computer science but also prepare me to apply my expertise to other fields and give me a head start toward a long and successful scientific career. Is there any academic group you would like to work with and why ? Have you made contacts with any professors or researchers tbd Any other important information we should kno about your candidature As a student at USTH, I does not limited myself to the expertise of my speciality. During my 2nd year of the bachelor program, I have founded a FabLab (FabLab USTH) and subsequentially awarded a 3 month scholarship to Boston, MA, USA to confer with the people who originated the concept of FabLab at MIT and other FabLab around the world - within the framework of FAB11 conference and DPI programme. FabLab USTH is one of the few FabLab in the world that was founded and currently maintained exclusively by students, aiming at multidisciplinary research and development.","title":"Grenoble Application/Motivation letter"},{"location":"Master/grenoble/#grenoble-applicationmotivation-letter","text":"","title":"Grenoble Application/Motivation letter"},{"location":"Master/grenoble/#what-is-your-long-terme-professional-ambition","text":"I wish to be a expert in the field of computer science, able to solve complex problem, both technical and theoretical, across multiple fields with the help of data science tools.","title":"What is your long-terme professional ambition"},{"location":"Master/grenoble/#how-will-the-master-academic-program-you-apply-for-help-you-to-reach-this-ambition","text":"As a science student, I have interest across many different fields of expertise, and Data Science is the tool and methodology for me to understand complex problem in the world. I strongly believe that with a diverse course structure offered by Grenoble INP will not only strengthen my understanding of mathematics, algorithm and computer science but also prepare me to apply my expertise to other fields and give me a head start toward a long and successful scientific career.","title":"How will the master academic program you apply for help you to reach this ambition"},{"location":"Master/grenoble/#is-there-any-academic-group-you-would-like-to-work-with-and-why-have-you-made-contacts-with-any-professors-or-researchers","text":"tbd","title":"Is there any academic group you would like to work with and why ? Have you made contacts with any professors or researchers"},{"location":"Master/grenoble/#any-other-important-information-we-should-kno-about-your-candidature","text":"As a student at USTH, I does not limited myself to the expertise of my speciality. During my 2nd year of the bachelor program, I have founded a FabLab (FabLab USTH) and subsequentially awarded a 3 month scholarship to Boston, MA, USA to confer with the people who originated the concept of FabLab at MIT and other FabLab around the world - within the framework of FAB11 conference and DPI programme. FabLab USTH is one of the few FabLab in the world that was founded and currently maintained exclusively by students, aiming at multidisciplinary research and development.","title":"Any other important information we should kno about your candidature"},{"location":"Master/grenoble_2/","text":"Grenoble Application/Motivation letter What is your long-terme professional ambition I wish to be an expert in computer science and gain competence to solve complex interdisciplinary problems, both technical and theoretical ones, with data science tools. How will the master academic program you apply for help you to reach this ambition As a student of science, I have interests across many different fields of expertise, and Data Science is the tool and methodology for me to understand complex problems in the world. I strongly believe that such a dynamic course structure offered by Grenoble INP will not only strengthen my understanding of mathematics, algorithm, and computer science, but also prepare me for application to other fields and for a long and successful scientific career. Is there any academic group you would like to work with and why ? Have you made contacts with any professors or researchers tbd Any other important information we should kno about your candidature As a student at USTH, I have not limited my learning to computer science. During the 2nd year of my undergraduate career, I founded a FabLab (FabLab USTH) and was subsequently awarded a 3-month scholarship to Boston, MA, USA to confer with the people who initiated the concept of FabLab at MIT and other FabLabs around the world - within the framework of the FAB11 conference and the DPI programme. FabLab USTH is one of the few FabLabs in the world that was founded and currently maintained exclusively by students, aiming at multidisciplinary research and development.","title":"Grenoble Application/Motivation letter"},{"location":"Master/grenoble_2/#grenoble-applicationmotivation-letter","text":"","title":"Grenoble Application/Motivation letter"},{"location":"Master/grenoble_2/#what-is-your-long-terme-professional-ambition","text":"I wish to be an expert in computer science and gain competence to solve complex interdisciplinary problems, both technical and theoretical ones, with data science tools.","title":"What is your long-terme professional ambition"},{"location":"Master/grenoble_2/#how-will-the-master-academic-program-you-apply-for-help-you-to-reach-this-ambition","text":"As a student of science, I have interests across many different fields of expertise, and Data Science is the tool and methodology for me to understand complex problems in the world. I strongly believe that such a dynamic course structure offered by Grenoble INP will not only strengthen my understanding of mathematics, algorithm, and computer science, but also prepare me for application to other fields and for a long and successful scientific career.","title":"How will the master academic program you apply for help you to reach this ambition"},{"location":"Master/grenoble_2/#is-there-any-academic-group-you-would-like-to-work-with-and-why-have-you-made-contacts-with-any-professors-or-researchers","text":"tbd","title":"Is there any academic group you would like to work with and why ? Have you made contacts with any professors or researchers"},{"location":"Master/grenoble_2/#any-other-important-information-we-should-kno-about-your-candidature","text":"As a student at USTH, I have not limited my learning to computer science. During the 2nd year of my undergraduate career, I founded a FabLab (FabLab USTH) and was subsequently awarded a 3-month scholarship to Boston, MA, USA to confer with the people who initiated the concept of FabLab at MIT and other FabLabs around the world - within the framework of the FAB11 conference and the DPI programme. FabLab USTH is one of the few FabLabs in the world that was founded and currently maintained exclusively by students, aiming at multidisciplinary research and development.","title":"Any other important information we should kno about your candidature"},{"location":"Master/MathForCS/hanoi-tower/","text":"%Mathematic for Computer Science: Hanoi's Tower Problem %Vu-Lam DANG %Oct 26, 2018 Hanoi Tower is a classical mathematics/computer science problem. Given a set of n disks and k pegs, the goal of the puzzle is to move a stack of disks from the initial (or Departure) peg to the target (or Arrival) peg. Furthermore, the disks must be stack in an increasing order by weight. Solution for n = 5 and k = 3 Initial state: 1 -> [1 -> 2 -> 3 -> 4 -> 5] 2 -> [] 3 -> [] Move list: (1,3) (1,2) (3,2) (1,3) (2,1) (2,3) (1,3) (1,2) (3,2) (3,1) (2,1) (3,2) (1,3) (1,2) (3,2) (1,3) (2,1) (2,3) (1,3) (2,1) (3,2) (3,1) (2,1) (2,3) (1,3) (1,2) (3,2) (1,3) (2,1) (2,3) (1,3) Total move: 31 Classical Problem In order to solve the problem for [5,3] , we divide the problem into subproblems. First, we move 4 disks to the intermidiate peg. The 5^{th} disk (the largest disk) can now be move to the destination peg. The 4 disks stack is then moved onto the destination peg. This algorithm give us a recursive solution to Hanoi Tower problem. Each iteration give us 2 subproblems (one for move n-1 disks to intermediate peg, and one more to move said stack to the destination). Thus, the complexity for this solution is \\Theta(2^n) . In fact, because each iteration the algorithm call exactly 2 subproblems, the number of move required to move disks from one peg to another is strickly 2^n - 1 (minus one because the original state is called once). Coding of the position In order to encode a state of the game we can ultilize 3 linked list to decode the state of the pegs. The head of the list refer to the disk on top of said stack (FILO), thus one can avoid puting a larger disk on top of the smaller disk. In this case, memory space complexity is \\Theta(n) In order to encode a move we simply need the original and destination pegs, or 2 variables. The disk at the head of the original peg is detached and transfer to the destination peg. Example: State: 1 -> [1 -> 2 -> 3] 2 -> [] 3 -> [4 -> 5] Move(1,3) 1 -> [2 -> 3] 2 -> [] 3 -> [1 -> 4 -> 5] Move(1,2) 1 -> [3] 2 -> [2] 3 -> [1 -> 4 -> 5] Move(3,2) 1 -> [3] 2 -> [1 -> 2] 3 -> [4 -> 5] Variation: Unbounded number of pegs For a variation where k > n the puzzle become trivial. A simple solution where all n-1 disks are distributed throughout the intermediate pegs, leave room for the n^{th} disk to arrive at the destination peg before recollecting the stack at the destination peg is completed in 2n-1 move, will have time complexity of \\Theta(n) . Example: n=5, k=6 Initial state: 1 -> [1 -> 2 -> 3 -> 4 -> 5] //Departure 2 -> [] 3 -> [] 4 -> [] 5 -> [] 6 -> [] //Arrival Move list: (1,2) (1,3) (1,4) (1,5) (1,6) //Dispertion Intermediate state: 1 -> [] //Departure 2 -> [1] 3 -> [2] 4 -> [3] 5 -> [4] 6 -> [5] //Arrival Move list: (2,6) (3,6) (4,6) (5,6) //Recollection Final state 1 -> [] //Departure 2 -> [] 3 -> [] 4 -> [] 5 -> [] 6 -> [1 -> 2 -> 3 -> 4 -> 5] //Arrival Improved solutions The two cases H(n, \\Theta(n)) = \\Theta(n) and H(n, \\Theta(2^n))= \\Theta(2^n) represent 2 extreme of the input space for Hanoi's Tower problem, the best and worst case respectively. These 2 cases orcur when k > n or k = 3 respectively, as shown in previous sections. Consider a game where n=\\Delta(k-1) where \\Delta(k) is the triangular number of k . An algorithm to solve H(\\Delta(k-1), k) is stated as following: For each immidate peg j between k-2 and 1 , build a j -high column of disks until only one disk left (the largest disk) in the Departure stack - this disk is now moved to the arrival peg. Finally, each immidate peg j between 1 and k-2 is deconstructed and stacked up at the final destination. A graphical demonstration: Original state 1:=====.....======= (n disks) // Departure 2: // Arrival 3: . . k: Immidiate state: 1:= // Departure 2: // Arrival 3:=====.....===== // j=k-2 disks 4:====.....===== // j=k-3 disks 5:===.....===== // j=k-4 disks . . . k:= // j=1 disks Final state: 1: // Departure 2:=====.....======= (n disks) // Arrival 3: . . k: Remark : For j from k-2 to 1, the member disks of the next column is larger than the previous column. For example j = k-2 will have k-2 disks from 1 to k-2; j = k-3 will have k-3 disks from k-2 to 2 k -5 From section 4, we show that the construction of a n disks high stack with k>n stack have complexity of \\Theta(n) . Therefore, for each immidiate stack in the previous algorithm have its complexity of \\Theta(j) where j is the height of the stack (and also the position of the peg). Thus, the total complexity of this case is \\sum_{j=1}^{k-2}\\Theta(j) = \\Theta(n) . In conclusion, for n \\leq \\Delta(k-1) with k is a constant will have a linear cost solution (complexity of \\Theta(n) ). Furthermore, if n > \\Delta(k-1) the solution will no longer be linear, as it will require stack higher than j disks, which in turn require more costly subproblem similar to the classical problem.","title":"Hanoi tower"},{"location":"Master/MathForCS/hanoi-tower/#solution-for-n-5-and-k-3","text":"Initial state: 1 -> [1 -> 2 -> 3 -> 4 -> 5] 2 -> [] 3 -> [] Move list: (1,3) (1,2) (3,2) (1,3) (2,1) (2,3) (1,3) (1,2) (3,2) (3,1) (2,1) (3,2) (1,3) (1,2) (3,2) (1,3) (2,1) (2,3) (1,3) (2,1) (3,2) (3,1) (2,1) (2,3) (1,3) (1,2) (3,2) (1,3) (2,1) (2,3) (1,3) Total move: 31","title":"Solution for n = 5 and k = 3"},{"location":"Master/MathForCS/hanoi-tower/#classical-problem","text":"In order to solve the problem for [5,3] , we divide the problem into subproblems. First, we move 4 disks to the intermidiate peg. The 5^{th} disk (the largest disk) can now be move to the destination peg. The 4 disks stack is then moved onto the destination peg. This algorithm give us a recursive solution to Hanoi Tower problem. Each iteration give us 2 subproblems (one for move n-1 disks to intermediate peg, and one more to move said stack to the destination). Thus, the complexity for this solution is \\Theta(2^n) . In fact, because each iteration the algorithm call exactly 2 subproblems, the number of move required to move disks from one peg to another is strickly 2^n - 1 (minus one because the original state is called once).","title":"Classical Problem"},{"location":"Master/MathForCS/hanoi-tower/#coding-of-the-position","text":"In order to encode a state of the game we can ultilize 3 linked list to decode the state of the pegs. The head of the list refer to the disk on top of said stack (FILO), thus one can avoid puting a larger disk on top of the smaller disk. In this case, memory space complexity is \\Theta(n) In order to encode a move we simply need the original and destination pegs, or 2 variables. The disk at the head of the original peg is detached and transfer to the destination peg. Example: State: 1 -> [1 -> 2 -> 3] 2 -> [] 3 -> [4 -> 5] Move(1,3) 1 -> [2 -> 3] 2 -> [] 3 -> [1 -> 4 -> 5] Move(1,2) 1 -> [3] 2 -> [2] 3 -> [1 -> 4 -> 5] Move(3,2) 1 -> [3] 2 -> [1 -> 2] 3 -> [4 -> 5]","title":"Coding of the position"},{"location":"Master/MathForCS/hanoi-tower/#variation-unbounded-number-of-pegs","text":"For a variation where k > n the puzzle become trivial. A simple solution where all n-1 disks are distributed throughout the intermediate pegs, leave room for the n^{th} disk to arrive at the destination peg before recollecting the stack at the destination peg is completed in 2n-1 move, will have time complexity of \\Theta(n) . Example: n=5, k=6 Initial state: 1 -> [1 -> 2 -> 3 -> 4 -> 5] //Departure 2 -> [] 3 -> [] 4 -> [] 5 -> [] 6 -> [] //Arrival Move list: (1,2) (1,3) (1,4) (1,5) (1,6) //Dispertion Intermediate state: 1 -> [] //Departure 2 -> [1] 3 -> [2] 4 -> [3] 5 -> [4] 6 -> [5] //Arrival Move list: (2,6) (3,6) (4,6) (5,6) //Recollection Final state 1 -> [] //Departure 2 -> [] 3 -> [] 4 -> [] 5 -> [] 6 -> [1 -> 2 -> 3 -> 4 -> 5] //Arrival","title":"Variation: Unbounded number of pegs"},{"location":"Master/MathForCS/hanoi-tower/#improved-solutions","text":"The two cases H(n, \\Theta(n)) = \\Theta(n) and H(n, \\Theta(2^n))= \\Theta(2^n) represent 2 extreme of the input space for Hanoi's Tower problem, the best and worst case respectively. These 2 cases orcur when k > n or k = 3 respectively, as shown in previous sections. Consider a game where n=\\Delta(k-1) where \\Delta(k) is the triangular number of k . An algorithm to solve H(\\Delta(k-1), k) is stated as following: For each immidate peg j between k-2 and 1 , build a j -high column of disks until only one disk left (the largest disk) in the Departure stack - this disk is now moved to the arrival peg. Finally, each immidate peg j between 1 and k-2 is deconstructed and stacked up at the final destination. A graphical demonstration: Original state 1:=====.....======= (n disks) // Departure 2: // Arrival 3: . . k: Immidiate state: 1:= // Departure 2: // Arrival 3:=====.....===== // j=k-2 disks 4:====.....===== // j=k-3 disks 5:===.....===== // j=k-4 disks . . . k:= // j=1 disks Final state: 1: // Departure 2:=====.....======= (n disks) // Arrival 3: . . k: Remark : For j from k-2 to 1, the member disks of the next column is larger than the previous column. For example j = k-2 will have k-2 disks from 1 to k-2; j = k-3 will have k-3 disks from k-2 to 2 k -5 From section 4, we show that the construction of a n disks high stack with k>n stack have complexity of \\Theta(n) . Therefore, for each immidiate stack in the previous algorithm have its complexity of \\Theta(j) where j is the height of the stack (and also the position of the peg). Thus, the total complexity of this case is \\sum_{j=1}^{k-2}\\Theta(j) = \\Theta(n) . In conclusion, for n \\leq \\Delta(k-1) with k is a constant will have a linear cost solution (complexity of \\Theta(n) ). Furthermore, if n > \\Delta(k-1) the solution will no longer be linear, as it will require stack higher than j disks, which in turn require more costly subproblem similar to the classical problem.","title":"Improved solutions"},{"location":"Master/class notes/Fundamental ML/notes/","text":"Class Notes: Fundamental Machine Learning Homework 1 1 ERM principle Given a training set S = (x, y)^m_{i=0} and a class of function F Emperical Risk Minimization principle: Find f by minimizing the unbiased estimator of its generalization error L(f) on a given training set: \\hat{L}(f,S) = \\frac 1 m \\sum_{i=1}^m l(f(x_i)y_i) - Problem: for a finite set S, \\hat{L}(f,S) come to 0; however, for infinite set, error always equal 1. Quantitative Learning Model 1^{st} learning model: biological neuron ( 19^{st} century). MuCilloch & Pitts formal neuron (1943) Linear prediction function. Rosenblatt's Perceptron The equation \\sum_{y=1}^d w_jx_y + w_0 = 0 represent an hyperplane in the vector space of dimension d Aim of learning when we are doing classification is to find weight (w_1, w_2, ...) in order to have a positive response for examples of class +1 and negative response for examples of class -1. Any vector x in the vectoral space has a unique decomposition \\vec x = \\vec{x_p} + \\vec{x_H} \\vec{W_N} \\cdot \\vec x = \\sum_{j=1}^d w_j x_j = \\vec {w_N} \\cdot (\\vec {x_p} + \\vec {x_H} ) = (\\vec {w_N} \\cdot \\vec {x_p} ) + (\\vec {w_N} \\cdot \\vec {x_H} ) The idea of Rosenblatt: There are 2 types of signals that are drawn on the perceptron board. Signals correspoding to the form of interest, to which are associated the out put +1, and the other signal assoc. the output -1. Find the weights of the hyperplane by minimizing the distance between the classified signals on the hyperplanes. Learning perceptronparameters Objective function: \\hat L (w) = - \\sum _{i' \\in I} y_{i'} ((\\bar w, x_{i'})+ w_0) Update rule: gradient descent \\forall t \\ge 1, w^{(t)} \\leftarrow w^{(t-1)} - \\eta \\nabla_{w^{(t-1)}}\\hat L(w^{(t-1)}) Derivatives with respect to the parameters \\frac {\\delta \\hat L (w)}{\\delta x_0} = - \\sum_{i' \\in I} y_{i'} \\Delta \\hat L (\\bar w) ) - \\sum _{i' \\in I} y_{i'}X_{i'} Stochastic gradient descent \\forall (X,y), if y ((\\bar w, X)+w_0) \\le 0\\ then \\begin{pmatrix} w_0\\\\ \\bar w \\end{pmatrix} \\leftarrow \\begin{pmatrix} w_0\\\\ \\bar w \\end{pmatrix} + \\eta \\begin{pmatrix} y\\\\ yX \\end{pmatrix} Graphical depiction of online update rule email to: Massih-Reza.Amini@imag.fr \u21a9","title":"Fundamental ML"},{"location":"Master/class notes/Fundamental ML/notes/#class-notes-fundamental-machine-learning","text":"Homework 1 1","title":"Class Notes: Fundamental Machine Learning"},{"location":"Master/class notes/Fundamental ML/notes/#erm-principle","text":"Given a training set S = (x, y)^m_{i=0} and a class of function F Emperical Risk Minimization principle: Find f by minimizing the unbiased estimator of its generalization error L(f) on a given training set: \\hat{L}(f,S) = \\frac 1 m \\sum_{i=1}^m l(f(x_i)y_i) - Problem: for a finite set S, \\hat{L}(f,S) come to 0; however, for infinite set, error always equal 1.","title":"ERM principle"},{"location":"Master/class notes/Fundamental ML/notes/#quantitative-learning-model","text":"1^{st} learning model: biological neuron ( 19^{st} century). MuCilloch & Pitts formal neuron (1943) Linear prediction function. Rosenblatt's Perceptron The equation \\sum_{y=1}^d w_jx_y + w_0 = 0 represent an hyperplane in the vector space of dimension d Aim of learning when we are doing classification is to find weight (w_1, w_2, ...) in order to have a positive response for examples of class +1 and negative response for examples of class -1. Any vector x in the vectoral space has a unique decomposition \\vec x = \\vec{x_p} + \\vec{x_H} \\vec{W_N} \\cdot \\vec x = \\sum_{j=1}^d w_j x_j = \\vec {w_N} \\cdot (\\vec {x_p} + \\vec {x_H} ) = (\\vec {w_N} \\cdot \\vec {x_p} ) + (\\vec {w_N} \\cdot \\vec {x_H} ) The idea of Rosenblatt: There are 2 types of signals that are drawn on the perceptron board. Signals correspoding to the form of interest, to which are associated the out put +1, and the other signal assoc. the output -1. Find the weights of the hyperplane by minimizing the distance between the classified signals on the hyperplanes. Learning perceptronparameters Objective function: \\hat L (w) = - \\sum _{i' \\in I} y_{i'} ((\\bar w, x_{i'})+ w_0) Update rule: gradient descent \\forall t \\ge 1, w^{(t)} \\leftarrow w^{(t-1)} - \\eta \\nabla_{w^{(t-1)}}\\hat L(w^{(t-1)}) Derivatives with respect to the parameters \\frac {\\delta \\hat L (w)}{\\delta x_0} = - \\sum_{i' \\in I} y_{i'} \\Delta \\hat L (\\bar w) ) - \\sum _{i' \\in I} y_{i'}X_{i'} Stochastic gradient descent \\forall (X,y), if y ((\\bar w, X)+w_0) \\le 0\\ then \\begin{pmatrix} w_0\\\\ \\bar w \\end{pmatrix} \\leftarrow \\begin{pmatrix} w_0\\\\ \\bar w \\end{pmatrix} + \\eta \\begin{pmatrix} y\\\\ yX \\end{pmatrix} Graphical depiction of online update rule email to: Massih-Reza.Amini@imag.fr \u21a9","title":"Quantitative Learning Model"},{"location":"Scientific Journey/Where to start with Python/","text":"Where to start with Python The other day there was another student asked me how to start with infomatics and computer science (and of course, AI). My answer is, of course, with Python. Now, why is Python the perfect language for starter in Comp Sci as well as AI? Simply put, Python got it right between readability, simplicity and performance. When I first started with programming, there was Pascal, C and BASIC, all was heavy hitter and formal programming languages. They are not really readability and friendly, and often it is easier to rewrite than reread - this is still the common practise for small to medium projects. However they are close to native code and can be used to perform time critical and intricate codes. Once we get to a certain proficient with programming, it is always recommended to start learning a \"real\" programming language - the modern languages are C++, Java and of course, Python On the other end of the spectrum, there is Scratch and to some extend, JavaScript and Lua Script. These so called scripting programming language is easier to grasp and eaiser to learn, with less formalism and more abundent","title":"Where to start with Python"},{"location":"Scientific Journey/Where to start with Python/#where-to-start-with-python","text":"The other day there was another student asked me how to start with infomatics and computer science (and of course, AI). My answer is, of course, with Python. Now, why is Python the perfect language for starter in Comp Sci as well as AI? Simply put, Python got it right between readability, simplicity and performance. When I first started with programming, there was Pascal, C and BASIC, all was heavy hitter and formal programming languages. They are not really readability and friendly, and often it is easier to rewrite than reread - this is still the common practise for small to medium projects. However they are close to native code and can be used to perform time critical and intricate codes. Once we get to a certain proficient with programming, it is always recommended to start learning a \"real\" programming language - the modern languages are C++, Java and of course, Python On the other end of the spectrum, there is Scratch and to some extend, JavaScript and Lua Script. These so called scripting programming language is easier to grasp and eaiser to learn, with less formalism and more abundent","title":"Where to start with Python"},{"location":"Scientific Journey/day-one-ish/","text":"(Nh\u1eefng) ng\u00e0y \u0111\u1ea7u ti\u00ean Th\u1eadt ra m\u00ecnh \u0111\u00e3 l\u00e0m trong d\u1ef1 \u00e1n n\u00e0y \u0111\u01b0\u1ee3c \u0111\u1ebfn nay ch\u1eafc c\u0169ng g\u1ea7n 3 tu\u1ea7n r\u1ed3i. Kh\u00f4ng th\u1ebf n\u00f3i l\u00e0 ng\u00e0y \u0111\u1ea7u ti\u00ean \u0111\u01b0\u1ee3c, nh\u1eefng th\u00ec may ra. Trong b\u01b0\u1edbc \u0111\u1ea7u c\u1ee7a d\u1ef1 \u00e1n \u0111ang l\u00e0m, c\u00f4ng vi\u1ec7c ch\u00ednh c\u1ee7a m\u00ecnh l\u00e0 ph\u00e1t tri\u1ec3n m\u1ed9t module/engine Python \u0111\u1ec3 thu th\u1eadp d\u1eef li\u1ec7u t\u1eeb m\u1ed9t v\u00e0i c\u01a1 s\u1edf d\u1eef li\u1ec7u. Vi\u1ec7c n\u00e0y \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n b\u1eb1ng m\u1ed9t s\u1ed1 th\u01b0 vi\u1ec7n c\u01a1 b\u1ea3n c\u1ee7a Python. C\u00e1c th\u01b0 vi\u1ec7n n\u00e0y bao g\u1ed3m pandas (x\u1eed l\u00fd d\u1eef li\u1ec7u d\u1ea1ng b\u1ea3ng), bs4 (x\u1eed l\u00fd d\u1eef li\u1ec7u DOM), requests (th\u1ef1c thi truy v\u1ea5n HTTPS) v\u00e0 json (x\u1eed l\u00fd d\u1eef li\u1ec7u d\u1ea1ng JSON v\u00e0 chuy\u1ec3n h\u00f3a d\u1eef li\u1ec7u d\u1ea1ng t\u1eeb \u0111i\u1ec3n - Dictionary sang d\u1ea1ng JSON tr\u1ea3 v\u1ec1) Ph\u01b0\u01a1ng ph\u00e1p ban \u0111\u1ea7u C\u00e1ch truy\u1ec1n th\u1ed1ng \u0111\u1ec3 th\u1ef1c hi\u1ec7n c\u00e1c truy v\u1ea5n n\u00e0y l\u00e0 vi\u1ebft c\u00e1c \u0111o\u1ea1n script \u0111\u1ec3 truy c\u1eadp v\u00e0o c\u00e1c trang web c\u1ee7a c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u n\u00f3i tr\u00ean v\u00e0 l\u1ea5y k\u1ebft qu\u1ea3 tr\u1ea3 v\u1ec1. V\u1edbi ph\u01b0\u01a1ng ph\u00e1p n\u00e0y, m\u1ed7i c\u01a1 s\u1edf d\u1eef li\u1ec7u m\u1edbi s\u1ebd c\u00f3 m\u1ed9t \u0111o\u1ea1n script ri\u00eang \u0111\u1ec3 th\u1ef1c thi truy v\u1ea5n v\u00e0 x\u1eed l\u00fd d\u1eef li\u1ec7u tr\u1ea3 v\u1ec1. Ph\u01b0\u01a1ng ph\u00e1p n\u00e0y kh\u00e1 \u0111\u01a1n gi\u1ea3n v\u00e0 hi\u1ec7u qu\u1ea3 v\u1edbi m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng nh\u1ecf c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u \u00edt li\u00ean h\u1ec7 v\u1edbi nhau. Code c\u0169ng s\u1ebd minh b\u1ea1ch v\u00e0 t\u00ednh bao \u0111\u00f3ng \u0111\u01b0\u1ee3c \u0111\u1ea3m b\u1ea3o. V\u1ea5n \u0111\u1ec1 v\u1edbi ph\u01b0\u01a1ng ph\u00e1p n\u00e0y b\u1eaft \u0111\u1ea7u n\u1ea3y sinh khi m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng l\u1edbn c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u00f3 quan h\u1ec7 ch\u1ed3ng ch\u00e9o c\u1ea7n \u0111\u01b0\u1ee3c x\u1eed l\u00fd. S\u1ebd kh\u00f4ng c\u00f3 c\u00e1ch n\u00e0o hi\u1ec7u qu\u1ea3 \u0111\u1ec3 t\u00ecm hi\u1ec3u m\u1ed1i quan h\u1ec7 gi\u1eefa c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u n\u00e0y, DB n\u00e0o truy v\u1ea5n t\u1eeb \u0111\u00e2u nh\u1eefng th\u00f4ng tin n\u00e0o... L\u01b0\u1ee3ng code tr\u00f9ng l\u1eb7p s\u1ebd tr\u1edf n\u00ean r\u1ea5t l\u1edbn khi \u0111\u1ea1t con s\u1ed1 10 - 12 c\u01a1 s\u1edf d\u1eef li\u1ec7u. Ph\u1ea7n l\u1edbn kh\u1ed1i l\u01b0\u1ee3ng code s\u1ebd l\u00e0 gi\u1ed1ng nhau: Th\u1ef1c hi\u1ec7n c\u00e1c truy v\u1ea5n HTTPS, ph\u00e2n t\u00e1ch d\u1eef li\u1ec7u tr\u1ea3 v\u1ec1, ghi d\u1eef li\u1ec7u v\u00e0o c\u00e1c t\u1eeb \u0111i\u1ec3n c\u1ee7a Python v\u00e0 xu\u1ea5t ch\u00fang ra d\u01b0\u1edbi d\u1ea1ng JSON. Ng\u01b0\u1eddi s\u1eed d\u1ee5ng c\u00e1c module n\u00e0y l\u1ea1i l\u00e0 c\u00e1c nh\u00e0 sinh h\u1ecdc v\u1edbi y\u00eau c\u1ea7u m\u1ed9t giao di\u1ec7n d\u1ec5 d\u00e0ng gi\u1eefa R v\u00e0 Python, trong khi h\u1ec7 th\u1ed1ng n\u00e0y kh\u00f4ng c\u00f3 c\u00e1ch n\u00e0o s\u1eed d\u1ee5ng t\u1eeb kh\u00f3a nh\u01b0 l\u00e0 1 c\u00e1ch \u0111\u1ec3 truy c\u1eadp c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u. C\u00e1c v\u1ea5n \u0111\u1ec1 n\u00e0y \u0111\u00f2i h\u1ecfi t\u00e1i c\u1ea5u tr\u00fac l\u1ea1i module Python sang m\u1ed9t ki\u1ebfn tr\u00fac m\u1edbi m\u1ea1nh v\u00e0 t\u1ed5ng quan h\u01a1n. \u0110\u1eb7c t\u1ea3 c\u01a1 s\u1edf d\u1eef li\u1ec7u \u0110\u1ed1i m\u1eb7t v\u1edbi c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u, m\u1ed9t c\u00e1ch t\u1ef1 nhi\u00ean ta ngh\u0129 \u0111\u1ebfn vi\u1ec7c gom c\u00e1c ph\u1ea7n code tr\u00f9ng l\u1eb7p gi\u1eefa c\u00e1c sript truy v\u1ea5n c\u01a1 s\u1edf d\u1eef li\u1ec7u v\u1edbi nhau th\u00e0nh m\u1ed9t m\u00e1y truy v\u1ea5n (query engine) m\u1ea1nh v\u00e0 t\u1ed5ng quan. C\u00e1c th\u00e0nh ph\u1ea7n li\u00ean quan \u0111\u1ebfn truy v\u1ea5n m\u1ea1ng, HTTPS \u0111\u01b0\u1ee3c h\u1ed7 tr\u1ee3 b\u1edfi th\u01b0 vi\u1ec7n requests ho\u00e0n to\u00e0n l\u00e0 tr\u00f9ng nhau qua c\u00e1c th\u00e0nh ph\u1ea7n, v\u00e0 \u0111\u00e3 \u0111\u01b0\u1ee3c gom l\u1ea1i v\u00e0o module helper - \u0111\u00e2y l\u00e0 c\u00e1ch \u0111\u1eb7t t\u00ean theo quy \u01b0\u1edbc d\u00e0nh cho m\u1ed9t module ch\u1ee9a c\u00e1c h\u00e0m ph\u1ee5 tr\u1ee3, kh\u00f4ng s\u1eed d\u1ee5ng nh\u01b0 m\u1ed9t th\u00e0nh ph\u1ea7n c\u1ee7a c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng. Vi\u1ec7c c\u1ea7n l\u00e0m l\u00e0 vi\u1ebft c\u00e1c exception th\u00f4ng b\u00e1o l\u1ed7i t\u1eeb ph\u00eda server \u0111\u1ec3 ng\u01b0\u1eddi d\u00f9ng c\u00f3 th\u1ec3 ti\u1ebfn h\u00e0nh s\u1eeda ch\u1eefa l\u1ed7i. C\u00e1c th\u00e0nh ph\u1ea7n ph\u00e2n t\u00e1ch d\u1eef li\u1ec7u c\u1ea7n \u0111\u01b0\u1ee3c t\u1ed5 ch\u1ee9c l\u1ea1i ho\u00e0n to\u00e0n \u0111\u1ec3 c\u00f3 th\u1ec3: Ph\u00e2n gi\u1ea3i d\u1eef li\u1ec7u ch\u00ednh x\u00e1c gi\u1eefa c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u00f3 c\u00f9ng c\u00f4ng ngh\u1ec7 tr\u1ea3 v\u1ec1, Chuy\u1ec3n \u0111\u1ed5i ch\u1ebf \u0111\u1ed9 ph\u00e2n t\u00e1ch d\u1eef li\u1ec7u gi\u1eefa c\u00e1c c\u00f4ng ngh\u1ec7 kh\u00e1c nhau. \u0110\u1ec3 ph\u00e2n bi\u1ec7t gi\u1eefa c\u00e1c ch\u1ebf \u0111\u1ed9 ph\u00e2n t\u00e1ch kh\u00e1c nhau \u0111\u00f2i h\u1ecfi c\u1ea7n c\u00f3 m\u1ed9t ph\u01b0\u01a1ng ph\u00e1p \u0111\u1ec3 ph\u00e2n bi\u1ec7t gi\u1eefa c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u, ph\u00e2n t\u00edch v\u00e0 ph\u00e2n lo\u1ea1i ch\u00fang v\u00e0o c\u00e1c nh\u00f3m kh\u00e1c nhau. \u0110i\u1ec1u n\u00e0y c\u0169ng c\u00f3 l\u1ee3i cho c\u00e1c truy v\u1ea5n HTTPS, v\u00ec ch\u00fang ta c\u1ea7n ph\u1ea3i c\u00f3 c\u00e1c \u0111\u01b0\u1eddng d\u1eabn \u0111\u1ebfn c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u. C\u00e1c truy v\u1ea5n HTTPS c\u0169ng \u0111\u01b0\u1ee3c ph\u00e2n lo\u1ea1i th\u00e0nh 2 method: POST v\u00e0 GET, \u0111i\u1ec1u n\u00e0y c\u0169ng c\u1ea7n \u0111\u01b0\u1ee3c ghi ch\u00fa l\u1ea1i v\u00e0 chuy\u1ec3n \u0111\u1ed5i gi\u1eefa 2 ch\u1ebf \u0111\u1ed9. B\u00e0i to\u00e1n \u0111\u1eb7t ra tr\u01b0\u1edbc m\u1eaft \u0111\u00f2i h\u1ecfi thi\u1ebft k\u1ebf m\u1ed9t c\u01a1 s\u1edf d\u1eef li\u1ec7u ph\u1ee5 tr\u1ee3, l\u01b0u tr\u1eef th\u00f4ng tin v\u1ec1 t\u1ea5t c\u1ea3 c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u00f3 trong truy v\u1ea5n. \u0110\u00e2y l\u00e0 m\u1ed9t d\u1ea1ng si\u00eau d\u1eef li\u1ec7u (metadata). T\u1eeb kh\u00f3a n\u00e0y c\u00f3 v\u1ebb cao si\u00eau, nh\u01b0ng th\u1ef1c t\u1ebf ch\u00fang kh\u00e1 \u0111\u01a1n gi\u1ea3n. Ch\u00fang ch\u1ee9a c\u00e1c \u0111\u1eb7c t\u1ea3 v\u1ec1 c\u1ea5u tr\u00fac, c\u00e1c th\u00f4ng tin c\u1ea7n thi\u1ebft v\u00e0 li\u00ean quan \u0111\u1ebfn c\u01a1 s\u1edf d\u1eef li\u1ec7u ch\u00fang ta \u0111ang c\u1ea7n truy v\u1ea5n. M\u00ecnh kh\u00f4ng mu\u1ed1n c\u00f4ng b\u1ed1 \u0111\u1eb7c t\u1ea3 n\u00e0y ra trong b\u00e0i n\u00e0y, v\u00ec \u0111\u00e3 \u0111\u1ee7 n\u1eb7ng v\u1ec1 k\u1ef9 thu\u1eadt. Tuy v\u1eady m\u1ed9t \u0111i\u1ec1u c\u1ea7n \u0111\u1eb7c bi\u1ec7t v\u1edbi vi\u1ec7c s\u1eed d\u1ee5ng c\u00e1c metadata n\u00e0y l\u00e0 n\u00f3 cho ph\u00e9p ta m\u1edf r\u1ed9ng s\u1ed1 l\u01b0\u1ee3ng c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u00f3 th\u1ec3 truy v\u1ea5n l\u00ean theo c\u1ea5p s\u1ed1 c\u1ed9ng. M\u1ed7i c\u01a1 s\u1edf d\u1eef li\u1ec7u m\u1edbi gi\u1edd ch\u1ec9 c\u1ea7n vi\u1ebft c\u00e1c m\u00f4 t\u1ea3 n\u00e0y \u0111\u1ec3 c\u00f3 th\u1ec3 truy c\u1eadp theo t\u00ean v\u00e0 c\u00e1c th\u00f4ng s\u1ed1 truy v\u1ea5n ph\u00f9 h\u1ee3p. \u0110i\u1ec1u n\u00e0y th\u1ecfa m\u00e3n c\u00e1c y\u00eau c\u1ea7u v\u1ec1 t\u00ednh t\u1ed5ng quan c\u1ee7a engine truy v\u1ea5n d\u1eef li\u1ec7u \u0111ang \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng. \u0110\u00e2y c\u0169ng l\u00e0 l\u00fd do t\u1ea1i sao trong ng\u00e0nh h\u1ecdc m\u00e1y c\u00f3 nh\u1eefng n\u1ed9i dung r\u1ea5t quan tr\u1ecdng v\u1ec1 khai ph\u00e1 si\u00eau d\u1eef li\u1ec7u. C\u1ea5u tr\u00fac t\u1ed1i \u01b0u C\u00f4ng vi\u1ec7c thi\u1ebft k\u1ebf \u0111\u1eb7c t\u1ea3 c\u01a1 s\u1edf d\u1eef li\u1ec7u ph\u1ea3i \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n song song v\u1edbi qu\u00e1 tr\u00ecnh thi\u1ebft k\u1ebf ki\u1ebfn tr\u00fac m\u00e1y truy v\u1ea5n. M\u1ed7i ph\u1ea7n c\u1ee7a c\u1ea5u tr\u00fac \u0111\u1eb7c t\u1ea3 \u0111\u01b0\u1ee3c vi\u1ebft ra kh\u1edbp v\u1edbi gi\u1ea3i thu\u1eadt t\u01b0\u01a1ng \u1ee9ng c\u1ee7a m\u00e1y truy v\u1ea5n. Hi\u1ec7n t\u1ea1i c\u1ea5u tr\u00fac m\u00e1y truy v\u1ea5n \u1ed5n \u0111\u1ecbnh \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n theo gi\u1ea3i thu\u1eadt sau: Nh\u1eadn chu\u1ed7i t\u00ean c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u1ea7n truy v\u1ea5n Truy xu\u1ea5t c\u01a1 s\u1edf d\u1eef li\u1ec7u t\u01b0\u01a1ng \u1ee9ng v\u1edbi DOM cung c\u1ea5p b\u1edfi bs4 Truy v\u1ea5n c\u01a1 s\u1edf d\u1eef li\u1ec7u th\u00f4ng qua \u0111\u01b0\u1eddng d\u1eabn l\u01b0u tr\u1eef trong \u0111\u1eb7c t\u1ea3 X\u00e1c \u0111\u1ecbnh lo\u1ea1i d\u1eef li\u1ec7u tr\u1ea3 v\u1ec1 - \u0111\u01b0\u1ee3c l\u01b0u l\u1ea1i tr\u01b0\u1edbc trong \u0111\u1eb7c t\u1ea3 Ph\u00e2n gi\u1ea3i d\u1eef li\u1ec7u theo c\u1ea5u h\u00ecnh \u0111\u00e3 l\u01b0u tr\u1eef s\u1eb5n D\u1ecbch k\u1ebft qu\u1ea3 sang c\u1ea5u tr\u00fac JSON \u0111\u1ec3 R c\u00f3 th\u1ec3 ph\u00e2n t\u00edch K\u1ebft th\u00fac giai \u0111o\u1ea1n n\u00e0y c\u1ee7a d\u1ef1 \u00e1n, ta \u0111\u00e3 c\u00f3 th\u1ec3 truy c\u1eadp c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u v\u1edbi \u0111\u1ed9 \u1ed5n \u0111\u1ecbnh v\u00e0 \u0111\u1ed3ng nh\u1ea5t cao c\u1ea3 v\u1ec1 k\u1ebft qu\u1ea3 tr\u1ea3 v\u1ec1 l\u1eabn hi\u1ec7u n\u0103ng truy xu\u1ea5t. N\u1ec1n t\u1ea3ng n\u00e0y c\u0169ng d\u1ec5 d\u00e0ng \u0111\u01b0\u1ee3c \u0111\u01b0a v\u00e0o \u00e1p d\u1ee5ng trong c\u00e1c b\u00e0i to\u00e1n kh\u00e1c ngo\u00e0i ng\u00e0nh Tin sinh h\u1ecdc. B\u00e0i ti\u1ebfp theo s\u1ebd n\u00f3i v\u1ec1 R v\u00e0 c\u00e1c b\u00e0i \u0111\u1ea7u ti\u00ean li\u00ean quan \u0111\u1ebfn R. Gi\u1edd th\u00ec ngh\u1ec9 th\u00f4i :wink:","title":"(Nh\u1eefng) ng\u00e0y \u0111\u1ea7u ti\u00ean"},{"location":"Scientific Journey/day-one-ish/#nhung-ngay-au-tien","text":"Th\u1eadt ra m\u00ecnh \u0111\u00e3 l\u00e0m trong d\u1ef1 \u00e1n n\u00e0y \u0111\u01b0\u1ee3c \u0111\u1ebfn nay ch\u1eafc c\u0169ng g\u1ea7n 3 tu\u1ea7n r\u1ed3i. Kh\u00f4ng th\u1ebf n\u00f3i l\u00e0 ng\u00e0y \u0111\u1ea7u ti\u00ean \u0111\u01b0\u1ee3c, nh\u1eefng th\u00ec may ra. Trong b\u01b0\u1edbc \u0111\u1ea7u c\u1ee7a d\u1ef1 \u00e1n \u0111ang l\u00e0m, c\u00f4ng vi\u1ec7c ch\u00ednh c\u1ee7a m\u00ecnh l\u00e0 ph\u00e1t tri\u1ec3n m\u1ed9t module/engine Python \u0111\u1ec3 thu th\u1eadp d\u1eef li\u1ec7u t\u1eeb m\u1ed9t v\u00e0i c\u01a1 s\u1edf d\u1eef li\u1ec7u. Vi\u1ec7c n\u00e0y \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n b\u1eb1ng m\u1ed9t s\u1ed1 th\u01b0 vi\u1ec7n c\u01a1 b\u1ea3n c\u1ee7a Python. C\u00e1c th\u01b0 vi\u1ec7n n\u00e0y bao g\u1ed3m pandas (x\u1eed l\u00fd d\u1eef li\u1ec7u d\u1ea1ng b\u1ea3ng), bs4 (x\u1eed l\u00fd d\u1eef li\u1ec7u DOM), requests (th\u1ef1c thi truy v\u1ea5n HTTPS) v\u00e0 json (x\u1eed l\u00fd d\u1eef li\u1ec7u d\u1ea1ng JSON v\u00e0 chuy\u1ec3n h\u00f3a d\u1eef li\u1ec7u d\u1ea1ng t\u1eeb \u0111i\u1ec3n - Dictionary sang d\u1ea1ng JSON tr\u1ea3 v\u1ec1)","title":"(Nh\u1eefng) ng\u00e0y \u0111\u1ea7u ti\u00ean"},{"location":"Scientific Journey/day-one-ish/#phuong-phap-ban-au","text":"C\u00e1ch truy\u1ec1n th\u1ed1ng \u0111\u1ec3 th\u1ef1c hi\u1ec7n c\u00e1c truy v\u1ea5n n\u00e0y l\u00e0 vi\u1ebft c\u00e1c \u0111o\u1ea1n script \u0111\u1ec3 truy c\u1eadp v\u00e0o c\u00e1c trang web c\u1ee7a c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u n\u00f3i tr\u00ean v\u00e0 l\u1ea5y k\u1ebft qu\u1ea3 tr\u1ea3 v\u1ec1. V\u1edbi ph\u01b0\u01a1ng ph\u00e1p n\u00e0y, m\u1ed7i c\u01a1 s\u1edf d\u1eef li\u1ec7u m\u1edbi s\u1ebd c\u00f3 m\u1ed9t \u0111o\u1ea1n script ri\u00eang \u0111\u1ec3 th\u1ef1c thi truy v\u1ea5n v\u00e0 x\u1eed l\u00fd d\u1eef li\u1ec7u tr\u1ea3 v\u1ec1. Ph\u01b0\u01a1ng ph\u00e1p n\u00e0y kh\u00e1 \u0111\u01a1n gi\u1ea3n v\u00e0 hi\u1ec7u qu\u1ea3 v\u1edbi m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng nh\u1ecf c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u \u00edt li\u00ean h\u1ec7 v\u1edbi nhau. Code c\u0169ng s\u1ebd minh b\u1ea1ch v\u00e0 t\u00ednh bao \u0111\u00f3ng \u0111\u01b0\u1ee3c \u0111\u1ea3m b\u1ea3o. V\u1ea5n \u0111\u1ec1 v\u1edbi ph\u01b0\u01a1ng ph\u00e1p n\u00e0y b\u1eaft \u0111\u1ea7u n\u1ea3y sinh khi m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng l\u1edbn c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u00f3 quan h\u1ec7 ch\u1ed3ng ch\u00e9o c\u1ea7n \u0111\u01b0\u1ee3c x\u1eed l\u00fd. S\u1ebd kh\u00f4ng c\u00f3 c\u00e1ch n\u00e0o hi\u1ec7u qu\u1ea3 \u0111\u1ec3 t\u00ecm hi\u1ec3u m\u1ed1i quan h\u1ec7 gi\u1eefa c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u n\u00e0y, DB n\u00e0o truy v\u1ea5n t\u1eeb \u0111\u00e2u nh\u1eefng th\u00f4ng tin n\u00e0o... L\u01b0\u1ee3ng code tr\u00f9ng l\u1eb7p s\u1ebd tr\u1edf n\u00ean r\u1ea5t l\u1edbn khi \u0111\u1ea1t con s\u1ed1 10 - 12 c\u01a1 s\u1edf d\u1eef li\u1ec7u. Ph\u1ea7n l\u1edbn kh\u1ed1i l\u01b0\u1ee3ng code s\u1ebd l\u00e0 gi\u1ed1ng nhau: Th\u1ef1c hi\u1ec7n c\u00e1c truy v\u1ea5n HTTPS, ph\u00e2n t\u00e1ch d\u1eef li\u1ec7u tr\u1ea3 v\u1ec1, ghi d\u1eef li\u1ec7u v\u00e0o c\u00e1c t\u1eeb \u0111i\u1ec3n c\u1ee7a Python v\u00e0 xu\u1ea5t ch\u00fang ra d\u01b0\u1edbi d\u1ea1ng JSON. Ng\u01b0\u1eddi s\u1eed d\u1ee5ng c\u00e1c module n\u00e0y l\u1ea1i l\u00e0 c\u00e1c nh\u00e0 sinh h\u1ecdc v\u1edbi y\u00eau c\u1ea7u m\u1ed9t giao di\u1ec7n d\u1ec5 d\u00e0ng gi\u1eefa R v\u00e0 Python, trong khi h\u1ec7 th\u1ed1ng n\u00e0y kh\u00f4ng c\u00f3 c\u00e1ch n\u00e0o s\u1eed d\u1ee5ng t\u1eeb kh\u00f3a nh\u01b0 l\u00e0 1 c\u00e1ch \u0111\u1ec3 truy c\u1eadp c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u. C\u00e1c v\u1ea5n \u0111\u1ec1 n\u00e0y \u0111\u00f2i h\u1ecfi t\u00e1i c\u1ea5u tr\u00fac l\u1ea1i module Python sang m\u1ed9t ki\u1ebfn tr\u00fac m\u1edbi m\u1ea1nh v\u00e0 t\u1ed5ng quan h\u01a1n.","title":"Ph\u01b0\u01a1ng ph\u00e1p ban \u0111\u1ea7u"},{"location":"Scientific Journey/day-one-ish/#ac-ta-co-so-du-lieu","text":"\u0110\u1ed1i m\u1eb7t v\u1edbi c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u, m\u1ed9t c\u00e1ch t\u1ef1 nhi\u00ean ta ngh\u0129 \u0111\u1ebfn vi\u1ec7c gom c\u00e1c ph\u1ea7n code tr\u00f9ng l\u1eb7p gi\u1eefa c\u00e1c sript truy v\u1ea5n c\u01a1 s\u1edf d\u1eef li\u1ec7u v\u1edbi nhau th\u00e0nh m\u1ed9t m\u00e1y truy v\u1ea5n (query engine) m\u1ea1nh v\u00e0 t\u1ed5ng quan. C\u00e1c th\u00e0nh ph\u1ea7n li\u00ean quan \u0111\u1ebfn truy v\u1ea5n m\u1ea1ng, HTTPS \u0111\u01b0\u1ee3c h\u1ed7 tr\u1ee3 b\u1edfi th\u01b0 vi\u1ec7n requests ho\u00e0n to\u00e0n l\u00e0 tr\u00f9ng nhau qua c\u00e1c th\u00e0nh ph\u1ea7n, v\u00e0 \u0111\u00e3 \u0111\u01b0\u1ee3c gom l\u1ea1i v\u00e0o module helper - \u0111\u00e2y l\u00e0 c\u00e1ch \u0111\u1eb7t t\u00ean theo quy \u01b0\u1edbc d\u00e0nh cho m\u1ed9t module ch\u1ee9a c\u00e1c h\u00e0m ph\u1ee5 tr\u1ee3, kh\u00f4ng s\u1eed d\u1ee5ng nh\u01b0 m\u1ed9t th\u00e0nh ph\u1ea7n c\u1ee7a c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng. Vi\u1ec7c c\u1ea7n l\u00e0m l\u00e0 vi\u1ebft c\u00e1c exception th\u00f4ng b\u00e1o l\u1ed7i t\u1eeb ph\u00eda server \u0111\u1ec3 ng\u01b0\u1eddi d\u00f9ng c\u00f3 th\u1ec3 ti\u1ebfn h\u00e0nh s\u1eeda ch\u1eefa l\u1ed7i. C\u00e1c th\u00e0nh ph\u1ea7n ph\u00e2n t\u00e1ch d\u1eef li\u1ec7u c\u1ea7n \u0111\u01b0\u1ee3c t\u1ed5 ch\u1ee9c l\u1ea1i ho\u00e0n to\u00e0n \u0111\u1ec3 c\u00f3 th\u1ec3: Ph\u00e2n gi\u1ea3i d\u1eef li\u1ec7u ch\u00ednh x\u00e1c gi\u1eefa c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u00f3 c\u00f9ng c\u00f4ng ngh\u1ec7 tr\u1ea3 v\u1ec1, Chuy\u1ec3n \u0111\u1ed5i ch\u1ebf \u0111\u1ed9 ph\u00e2n t\u00e1ch d\u1eef li\u1ec7u gi\u1eefa c\u00e1c c\u00f4ng ngh\u1ec7 kh\u00e1c nhau. \u0110\u1ec3 ph\u00e2n bi\u1ec7t gi\u1eefa c\u00e1c ch\u1ebf \u0111\u1ed9 ph\u00e2n t\u00e1ch kh\u00e1c nhau \u0111\u00f2i h\u1ecfi c\u1ea7n c\u00f3 m\u1ed9t ph\u01b0\u01a1ng ph\u00e1p \u0111\u1ec3 ph\u00e2n bi\u1ec7t gi\u1eefa c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u, ph\u00e2n t\u00edch v\u00e0 ph\u00e2n lo\u1ea1i ch\u00fang v\u00e0o c\u00e1c nh\u00f3m kh\u00e1c nhau. \u0110i\u1ec1u n\u00e0y c\u0169ng c\u00f3 l\u1ee3i cho c\u00e1c truy v\u1ea5n HTTPS, v\u00ec ch\u00fang ta c\u1ea7n ph\u1ea3i c\u00f3 c\u00e1c \u0111\u01b0\u1eddng d\u1eabn \u0111\u1ebfn c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u. C\u00e1c truy v\u1ea5n HTTPS c\u0169ng \u0111\u01b0\u1ee3c ph\u00e2n lo\u1ea1i th\u00e0nh 2 method: POST v\u00e0 GET, \u0111i\u1ec1u n\u00e0y c\u0169ng c\u1ea7n \u0111\u01b0\u1ee3c ghi ch\u00fa l\u1ea1i v\u00e0 chuy\u1ec3n \u0111\u1ed5i gi\u1eefa 2 ch\u1ebf \u0111\u1ed9. B\u00e0i to\u00e1n \u0111\u1eb7t ra tr\u01b0\u1edbc m\u1eaft \u0111\u00f2i h\u1ecfi thi\u1ebft k\u1ebf m\u1ed9t c\u01a1 s\u1edf d\u1eef li\u1ec7u ph\u1ee5 tr\u1ee3, l\u01b0u tr\u1eef th\u00f4ng tin v\u1ec1 t\u1ea5t c\u1ea3 c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u00f3 trong truy v\u1ea5n. \u0110\u00e2y l\u00e0 m\u1ed9t d\u1ea1ng si\u00eau d\u1eef li\u1ec7u (metadata). T\u1eeb kh\u00f3a n\u00e0y c\u00f3 v\u1ebb cao si\u00eau, nh\u01b0ng th\u1ef1c t\u1ebf ch\u00fang kh\u00e1 \u0111\u01a1n gi\u1ea3n. Ch\u00fang ch\u1ee9a c\u00e1c \u0111\u1eb7c t\u1ea3 v\u1ec1 c\u1ea5u tr\u00fac, c\u00e1c th\u00f4ng tin c\u1ea7n thi\u1ebft v\u00e0 li\u00ean quan \u0111\u1ebfn c\u01a1 s\u1edf d\u1eef li\u1ec7u ch\u00fang ta \u0111ang c\u1ea7n truy v\u1ea5n. M\u00ecnh kh\u00f4ng mu\u1ed1n c\u00f4ng b\u1ed1 \u0111\u1eb7c t\u1ea3 n\u00e0y ra trong b\u00e0i n\u00e0y, v\u00ec \u0111\u00e3 \u0111\u1ee7 n\u1eb7ng v\u1ec1 k\u1ef9 thu\u1eadt. Tuy v\u1eady m\u1ed9t \u0111i\u1ec1u c\u1ea7n \u0111\u1eb7c bi\u1ec7t v\u1edbi vi\u1ec7c s\u1eed d\u1ee5ng c\u00e1c metadata n\u00e0y l\u00e0 n\u00f3 cho ph\u00e9p ta m\u1edf r\u1ed9ng s\u1ed1 l\u01b0\u1ee3ng c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u00f3 th\u1ec3 truy v\u1ea5n l\u00ean theo c\u1ea5p s\u1ed1 c\u1ed9ng. M\u1ed7i c\u01a1 s\u1edf d\u1eef li\u1ec7u m\u1edbi gi\u1edd ch\u1ec9 c\u1ea7n vi\u1ebft c\u00e1c m\u00f4 t\u1ea3 n\u00e0y \u0111\u1ec3 c\u00f3 th\u1ec3 truy c\u1eadp theo t\u00ean v\u00e0 c\u00e1c th\u00f4ng s\u1ed1 truy v\u1ea5n ph\u00f9 h\u1ee3p. \u0110i\u1ec1u n\u00e0y th\u1ecfa m\u00e3n c\u00e1c y\u00eau c\u1ea7u v\u1ec1 t\u00ednh t\u1ed5ng quan c\u1ee7a engine truy v\u1ea5n d\u1eef li\u1ec7u \u0111ang \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng. \u0110\u00e2y c\u0169ng l\u00e0 l\u00fd do t\u1ea1i sao trong ng\u00e0nh h\u1ecdc m\u00e1y c\u00f3 nh\u1eefng n\u1ed9i dung r\u1ea5t quan tr\u1ecdng v\u1ec1 khai ph\u00e1 si\u00eau d\u1eef li\u1ec7u.","title":"\u0110\u1eb7c t\u1ea3 c\u01a1 s\u1edf d\u1eef li\u1ec7u"},{"location":"Scientific Journey/day-one-ish/#cau-truc-toi-uu","text":"C\u00f4ng vi\u1ec7c thi\u1ebft k\u1ebf \u0111\u1eb7c t\u1ea3 c\u01a1 s\u1edf d\u1eef li\u1ec7u ph\u1ea3i \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n song song v\u1edbi qu\u00e1 tr\u00ecnh thi\u1ebft k\u1ebf ki\u1ebfn tr\u00fac m\u00e1y truy v\u1ea5n. M\u1ed7i ph\u1ea7n c\u1ee7a c\u1ea5u tr\u00fac \u0111\u1eb7c t\u1ea3 \u0111\u01b0\u1ee3c vi\u1ebft ra kh\u1edbp v\u1edbi gi\u1ea3i thu\u1eadt t\u01b0\u01a1ng \u1ee9ng c\u1ee7a m\u00e1y truy v\u1ea5n. Hi\u1ec7n t\u1ea1i c\u1ea5u tr\u00fac m\u00e1y truy v\u1ea5n \u1ed5n \u0111\u1ecbnh \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n theo gi\u1ea3i thu\u1eadt sau: Nh\u1eadn chu\u1ed7i t\u00ean c\u01a1 s\u1edf d\u1eef li\u1ec7u c\u1ea7n truy v\u1ea5n Truy xu\u1ea5t c\u01a1 s\u1edf d\u1eef li\u1ec7u t\u01b0\u01a1ng \u1ee9ng v\u1edbi DOM cung c\u1ea5p b\u1edfi bs4 Truy v\u1ea5n c\u01a1 s\u1edf d\u1eef li\u1ec7u th\u00f4ng qua \u0111\u01b0\u1eddng d\u1eabn l\u01b0u tr\u1eef trong \u0111\u1eb7c t\u1ea3 X\u00e1c \u0111\u1ecbnh lo\u1ea1i d\u1eef li\u1ec7u tr\u1ea3 v\u1ec1 - \u0111\u01b0\u1ee3c l\u01b0u l\u1ea1i tr\u01b0\u1edbc trong \u0111\u1eb7c t\u1ea3 Ph\u00e2n gi\u1ea3i d\u1eef li\u1ec7u theo c\u1ea5u h\u00ecnh \u0111\u00e3 l\u01b0u tr\u1eef s\u1eb5n D\u1ecbch k\u1ebft qu\u1ea3 sang c\u1ea5u tr\u00fac JSON \u0111\u1ec3 R c\u00f3 th\u1ec3 ph\u00e2n t\u00edch K\u1ebft th\u00fac giai \u0111o\u1ea1n n\u00e0y c\u1ee7a d\u1ef1 \u00e1n, ta \u0111\u00e3 c\u00f3 th\u1ec3 truy c\u1eadp c\u00e1c c\u01a1 s\u1edf d\u1eef li\u1ec7u v\u1edbi \u0111\u1ed9 \u1ed5n \u0111\u1ecbnh v\u00e0 \u0111\u1ed3ng nh\u1ea5t cao c\u1ea3 v\u1ec1 k\u1ebft qu\u1ea3 tr\u1ea3 v\u1ec1 l\u1eabn hi\u1ec7u n\u0103ng truy xu\u1ea5t. N\u1ec1n t\u1ea3ng n\u00e0y c\u0169ng d\u1ec5 d\u00e0ng \u0111\u01b0\u1ee3c \u0111\u01b0a v\u00e0o \u00e1p d\u1ee5ng trong c\u00e1c b\u00e0i to\u00e1n kh\u00e1c ngo\u00e0i ng\u00e0nh Tin sinh h\u1ecdc. B\u00e0i ti\u1ebfp theo s\u1ebd n\u00f3i v\u1ec1 R v\u00e0 c\u00e1c b\u00e0i \u0111\u1ea7u ti\u00ean li\u00ean quan \u0111\u1ebfn R. Gi\u1edd th\u00ec ngh\u1ec9 th\u00f4i :wink:","title":"C\u1ea5u tr\u00fac t\u1ed1i \u01b0u"},{"location":"Scientific Journey/day-two-documentation-markdown/","text":"Ghi ch\u00fa b\u1eb1ng Markdown Tr\u01b0\u1edbc khi \u0111\u1ebfn v\u1edbi R c\u00f3 m\u1ed9t s\u1ed1 k\u0129 n\u0103ng ch\u00fang ta c\u1ea7n bi\u1ebft khi tham gia m\u1ed9t d\u1ef1 \u00e1n CNTT. funfact: T\u1ea5t c\u1ea3 b\u00e0i vi\u1ebft tr\u00ean trang n\u00e0y \u0111\u1ec1u \u0111\u01b0\u1ee3c vi\u1ebft b\u1eb1ng Markdown . Git T\u1ea5t c\u1ea3 c\u00e1c d\u1ef1 \u00e1n CNTT c\u00f3 s\u1ef1 tham gia c\u1ee7a 1 s\u1ed1 ng\u01b0\u1eddi \u0111\u1ec1u s\u1eed d\u1ee5ng 1 ph\u1ea7n m\u1ec1m qu\u1ea3n l\u00fd phi\u00ean b\u1ea3n (version control). Ph\u1ed5 bi\u1ebfn nh\u1ea5t trong s\u1ed1 n\u00e0y l\u00e0 Git - \u0111\u01b0\u1ee3c d\u00f9ng trong c\u1ea3 gi\u1edbi khoa h\u1ecdc l\u1eabn gi\u1edbi c\u00f4ng nghi\u1ec7p. \u0110\u1ec3 s\u1eed d\u1ee5ng Git, ch\u00fang ta ch\u1ec9 \u0111\u01a1n gi\u1ea3n download Git . C\u00e1c c\u00e2u l\u1ec7nh git c\u00f3 th\u1ec3 tra c\u1ee9u r\u1ea5t \u0111\u01a1n gi\u1ea3n b\u1eb1ng c\u00e2u l\u1ec7nh git -- help . Tuy nhi\u00ean \u0111\u1ec3 s\u1eed d\u1ee5ng h\u00e0ng ng\u00e0y th\u00ec c\u00f3 m\u1ed9t s\u1ed1 c\u00e1c c\u00e2u l\u1ec7nh sau: git init \u0111\u1ec3 kh\u1edfi t\u1ea1o m\u1ed9t th\u01b0 m\u1ee5c git (\u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 m\u1ed9t repo) git add . \u0111\u1ec3 th\u00eam t\u1ea5t c\u1ea3 c\u00e1c thay \u0111\u1ed5i c\u1ee7a m\u00ecnh v\u00e0o repo git commit -m \"Message\" l\u01b0u c\u00e1c thay \u0111\u1ed5i n\u00e0y l\u1ea1i th\u00e0nh 1 version/commit git push \u0111\u1ea9y m\u1ed9t commit l\u00ean server github (ho\u1eb7c m\u1ed9t server c\u00e1 nh\u00e2n) git pull l\u1ea5y phi\u00ean b\u1ea3n m\u1edbi nh\u1ea5t c\u1ee7a branch/repo git clone \"repo-url\" t\u1ea3i m\u1ed9t repo github v\u1ec1 M\u1ed9t s\u1ed1 t\u1eeb kh\u00f3a c\u1ea7n t\u00ecm hi\u1ec3u th\u00eam l\u00e0 branch v\u00e0 fork. 2 kh\u00e1i ni\u1ec7m n\u00e0y gi\u00fap ta qu\u1ea3n l\u00fd c\u00e1c th\u00e0nh ph\u1ea7n c\u1ee7a d\u1ef1 \u00e1n d\u1ec5 h\u01a1n, v\u00ed d\u1ee5 nh\u01b0 khi l\u00e0m vi\u1ec7c v\u1edbi Markdown v\u00e0 MKDocs \u1edf d\u01b0\u1edbi, ch\u00fang ta s\u1ebd c\u00f3 1 branch \u0111\u1ec3 qu\u1ea3n l\u00fd c\u00e1c b\u00e0i vi\u1ebft v\u00e0 1 branch \u0111\u1ec3 c\u00f4ng b\u1ed1/render c\u00e1c b\u00e0i vi\u1ebft n\u00e0y. Markdown Tr\u01b0\u1edbc khi c\u00f3 Markdown, gi\u1edbi khoa h\u1ecdc th\u01b0\u1eddng s\u1eed d\u1ee5ng LaTex \u0111\u1ec3 so\u1ea1n th\u1ea3o c\u00e1c v\u0103n b\u1ea3n khoa h\u1ecdc. \u0110i\u1ec3m m\u1ea1nh c\u1ee7a LaTex l\u00e0 n\u00f3 r\u1ea5t nhi\u1ec1u plugin v\u00e0 c\u00f4ng c\u1ee5 h\u1ed7 tr\u1ee3, tuy nhi\u00ean c\u00fa ph\u00e1p c\u1ee7a LaTex r\u1ea5t ph\u1ee9c t\u1ea1p v\u00e0 kh\u00f3 \u0111\u1ec3 h\u1ecdc. \u0110\u1ec3 gi\u1ea3i quy\u1ebft v\u1ea5n n\u1ea1n n\u00e0y, Markdown ra \u0111\u1eddi nh\u01b0 m\u1ed9t ng\u00f4n ng\u1eef \u0111\u00e1nh d\u1ea5u nh\u1eb9 v\u00e0 d\u1ec5 s\u1eed d\u1ee5ng. To\u00e0n b\u1ed9 c\u00fa ph\u00e1p c\u1ee7a Markdown c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c t\u00f3m l\u01b0\u1ee3c trong m\u1ed9t hai trang A4 . V\u1ea5n \u0111\u1ec1 l\u00e0 khi ch\u00fang ta c\u00f3 m\u1ed9t b\u00e0i vi\u1ebft Markdown xong r\u1ed3i th\u00ec l\u00e0m g\u00ec? \u0110\u00f3n \u0111\u1ecdc ph\u1ea7n 3 (t\u1ed1i vi\u1ebft ti\u1ebfp). Github Pages v\u00e0 MKDocs \u0110\u1ec3 l\u00e0m m\u1ed9t trang web t\u01b0\u01a1ng t\u1ef1 trang n\u00e0y c\u00f3 r\u1ea5t nhi\u1ec1u l\u00fd do. M\u1ed9t trong s\u1ed1 nh\u1eefng l\u00fd do ph\u1ed5 bi\u1ebfn l\u00e0 \u0111\u1ec3 l\u00e0m t\u00e0i li\u1ec7u cho d\u1ef1 \u00e1n (project page). M\u1ed9t l\u00fd do kh\u00e1c l\u00e0 do th\u00edch vi\u1ebft (nh\u01b0 m\u00ecnh). Trang web n\u00e0y \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng tr\u00ean Github Pages v\u00e0 MKDocs. L\u00fd do l\u00e0 v\u00ec so v\u1edbi Wordpress (n\u1ec1n t\u1ea3ng tr\u01b0\u1edbc \u0111\u00e2y m\u00ecnh vi\u1ebft), MKDocs s\u1eed d\u1ee5ng Markdown cho ph\u00e9p vi\u1ebft r\u1ea5t nhanh v\u00e0 d\u1ec5 d\u00e0ng t\u00f9y ch\u1ec9nh. MKDocs c\u0169ng \u1ed5n \u0111\u1ecbnh v\u00e0 t\u1ed1i gi\u1ea3n h\u01a1n nhi\u1ec1u so v\u1edbi Wordpress. \u0110\u1ec3 c\u00e0i \u0111\u1eb7t MKDocs h\u1ebft s\u1ee9c \u0111\u01a1n gi\u1ea3n: pip install mkdocs . M\u00ecnh khuy\u1ebfn kh\u00edch m\u1ecdi ng\u01b0\u1eddi t\u1ef1 t\u00ecm hi\u1ec3u th\u00eam v\u1ec1 ph\u1ea7n m\u1ec1m n\u00e0y. \u0110\u1ec3 b\u1eaft \u0111\u1ea7u m\u1ed9t trang web MKDocs, ta di chuy\u1ec3n \u0111\u1ebfn folder ch\u1ee9a repo (xem ph\u1ea7n d\u01b0\u1edbi) ch\u1ee9a trang web c\u1ee7a ch\u00fang ta, v\u00e0 d\u00f9ng c\u00e2u l\u1ec7nh mkdocs init . C\u00e1c folder v\u00e0 file kh\u00e1c ta kh\u00f4ng c\u1ea7n qu\u00e1 ch\u00fa tr\u1ecdng, quan tr\u1ecdng nh\u1ea5t l\u00fac n\u00e0y l\u00e0 folder docs v\u00e0 file mkdocs.yml . Folder docs ch\u1ee9a t\u1ea5t c\u1ea3 t\u00e0i li\u1ec7u ch\u00fang ta s\u1ebd vi\u1ebft. File mkdocs.yml ch\u1ee9a c\u00e1c thi\u1ebft l\u1eadp cho trang web. Sau khi vi\u1ebft 1 t\u00e0i li\u1ec7u Markdown v\u00e0o folder docs , c\u00e2u l\u1ec7nh mkdocs build s\u1ebd x\u00e2y d\u1ef1ng trang web c\u1ee7a ta v\u00e0o folder site . Vi\u1ec7c c\u1ea7n l\u00e0m ti\u1ebfp theo l\u00e0 upload folder n\u00e0y l\u00ean server \u0111\u1ec3 cung c\u1ea5p trang web. \u0110\u00e2y l\u00e0 vi\u1ec7c c\u1ee7a c\u00e2u l\u1ec7nh mkdocs gh-deploy . S\u1ebd c\u1ea7n m\u1ed9t s\u1ed1 t\u00f9y ch\u1ec9nh trong file mkdocs.yml . \u0110i\u1ec1u n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c n\u00f3i th\u00eam \u1edf ph\u1ea7n d\u01b0\u1edbi. Server c\u1ee7a blog n\u00e0y \u0111\u01b0\u1ee3c h\u1ed7 tr\u1ee3 b\u1edfi Github Pages. D\u1ecbch v\u1ee5 n\u00e0y c\u1ee7a Github nh\u1eb1m cung c\u1ea5p 1 n\u1ec1n t\u1ea3ng website HTML t\u0129nh (static HTML pages), d\u00e0nh cho vi\u1ec7c c\u00f4ng b\u1ed1 t\u00e0i li\u1ec7u c\u1ee7a d\u1ef1 \u00e1n, ho\u1eb7c \u0111\u1ec3 m\u00ecnh l\u1ee3i d\u1ee5ng l\u00e0m web c\u00e1 nh\u00e2n. Th\u00f4ng th\u01b0\u1eddng Github Pages \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng b\u1eb1ng Jekyll. Tuy nhi\u00ean v\u00ec m\u00ecnh \u0111\u00e3 s\u1eed d\u1ee5ng Python, v\u00e0 kh\u00f4ng mu\u1ed1n c\u00e0i th\u00eam NodeJS n\u00ean tr\u00ean n\u00e0y ch\u00fang ta s\u1ebd d\u00f9ng MKDocs \u0111\u1ec3 x\u00e2y d\u1ef1ng 1 trang web. \u0110\u1ec3 \u0111\u0103ng k\u00fd 1 trang Github Pages c\u00e1 nh\u00e2n, ch\u00fang ta c\u1ea7n t\u1ea1o m\u1ed9t repo c\u00f3 t\u00ean github-username .github.io , v\u1edbi github-username l\u00e0 t\u00ean Github c\u1ee7a ch\u00fang ta. L\u00fac n\u00e0y, Github s\u1ebd t\u1ef1 t\u1ea1o m\u1ed9t trang v\u1edbi t\u00ean mi\u1ec1n github-username .github.io \u0111\u1ec3 ch\u00fang ta c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng. \u0110\u1ec3 post MKDocs l\u00ean trang c\u00e1 nh\u00e2n n\u00e0y, sau khi clone repo tr\u00ean v\u1ec1 v\u00e0 c\u00e0i \u0111\u1eb7t MKDocs t\u1ea1i folder c\u1ee7a repo, ch\u00fang ta c\u1ea7n ch\u1ec9nh s\u1eeda file mkdocs.yml . M\u1eb7c \u0111\u1ecbnh MKDocs s\u1ebd post b\u00e0i theo c\u1ea5u h\u00ecnh c\u1ee7a Github Pages d\u00e0nh cho t\u1ed5 ch\u1ee9c thay v\u00ec c\u00e1 nh\u00e2n. \u0110i\u1ec3m kh\u00e1c bi\u1ec7t gi\u1eefa 2 lo\u1ea1i trang n\u00e0y l\u00e0 trang t\u1ed5 ch\u1ee9c cho ph\u00e9p post b\u00e0i l\u00ean branch gp-deploy, trong khi trang c\u00e1 nh\u00e2n post b\u00e0i l\u00ean branch master ( xem th\u00eam v\u1ec1 git branch ). \u0110\u1ec3 c\u1ea5u h\u00ecnh cho \u0111\u00fang v\u1edbi trang c\u00e1 nh\u00e2n, ch\u1ec9 \u0111\u01a1n gi\u1ea3n th\u00eam m\u1ed9t d\u00f2ng: remote_branch: master V\u1eady th\u00f4i :smile: anh em ch\u1ecbu kh\u00f3 \u0111\u1ecdc th\u00eam t\u00e0i li\u1ec7u nha.","title":"Ghi ch\u00fa b\u1eb1ng Markdown"},{"location":"Scientific Journey/day-two-documentation-markdown/#ghi-chu-bang-markdown","text":"Tr\u01b0\u1edbc khi \u0111\u1ebfn v\u1edbi R c\u00f3 m\u1ed9t s\u1ed1 k\u0129 n\u0103ng ch\u00fang ta c\u1ea7n bi\u1ebft khi tham gia m\u1ed9t d\u1ef1 \u00e1n CNTT. funfact: T\u1ea5t c\u1ea3 b\u00e0i vi\u1ebft tr\u00ean trang n\u00e0y \u0111\u1ec1u \u0111\u01b0\u1ee3c vi\u1ebft b\u1eb1ng Markdown .","title":"Ghi ch\u00fa b\u1eb1ng Markdown"},{"location":"Scientific Journey/day-two-documentation-markdown/#git","text":"T\u1ea5t c\u1ea3 c\u00e1c d\u1ef1 \u00e1n CNTT c\u00f3 s\u1ef1 tham gia c\u1ee7a 1 s\u1ed1 ng\u01b0\u1eddi \u0111\u1ec1u s\u1eed d\u1ee5ng 1 ph\u1ea7n m\u1ec1m qu\u1ea3n l\u00fd phi\u00ean b\u1ea3n (version control). Ph\u1ed5 bi\u1ebfn nh\u1ea5t trong s\u1ed1 n\u00e0y l\u00e0 Git - \u0111\u01b0\u1ee3c d\u00f9ng trong c\u1ea3 gi\u1edbi khoa h\u1ecdc l\u1eabn gi\u1edbi c\u00f4ng nghi\u1ec7p. \u0110\u1ec3 s\u1eed d\u1ee5ng Git, ch\u00fang ta ch\u1ec9 \u0111\u01a1n gi\u1ea3n download Git . C\u00e1c c\u00e2u l\u1ec7nh git c\u00f3 th\u1ec3 tra c\u1ee9u r\u1ea5t \u0111\u01a1n gi\u1ea3n b\u1eb1ng c\u00e2u l\u1ec7nh git -- help . Tuy nhi\u00ean \u0111\u1ec3 s\u1eed d\u1ee5ng h\u00e0ng ng\u00e0y th\u00ec c\u00f3 m\u1ed9t s\u1ed1 c\u00e1c c\u00e2u l\u1ec7nh sau: git init \u0111\u1ec3 kh\u1edfi t\u1ea1o m\u1ed9t th\u01b0 m\u1ee5c git (\u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 m\u1ed9t repo) git add . \u0111\u1ec3 th\u00eam t\u1ea5t c\u1ea3 c\u00e1c thay \u0111\u1ed5i c\u1ee7a m\u00ecnh v\u00e0o repo git commit -m \"Message\" l\u01b0u c\u00e1c thay \u0111\u1ed5i n\u00e0y l\u1ea1i th\u00e0nh 1 version/commit git push \u0111\u1ea9y m\u1ed9t commit l\u00ean server github (ho\u1eb7c m\u1ed9t server c\u00e1 nh\u00e2n) git pull l\u1ea5y phi\u00ean b\u1ea3n m\u1edbi nh\u1ea5t c\u1ee7a branch/repo git clone \"repo-url\" t\u1ea3i m\u1ed9t repo github v\u1ec1 M\u1ed9t s\u1ed1 t\u1eeb kh\u00f3a c\u1ea7n t\u00ecm hi\u1ec3u th\u00eam l\u00e0 branch v\u00e0 fork. 2 kh\u00e1i ni\u1ec7m n\u00e0y gi\u00fap ta qu\u1ea3n l\u00fd c\u00e1c th\u00e0nh ph\u1ea7n c\u1ee7a d\u1ef1 \u00e1n d\u1ec5 h\u01a1n, v\u00ed d\u1ee5 nh\u01b0 khi l\u00e0m vi\u1ec7c v\u1edbi Markdown v\u00e0 MKDocs \u1edf d\u01b0\u1edbi, ch\u00fang ta s\u1ebd c\u00f3 1 branch \u0111\u1ec3 qu\u1ea3n l\u00fd c\u00e1c b\u00e0i vi\u1ebft v\u00e0 1 branch \u0111\u1ec3 c\u00f4ng b\u1ed1/render c\u00e1c b\u00e0i vi\u1ebft n\u00e0y.","title":"Git"},{"location":"Scientific Journey/day-two-documentation-markdown/#markdown","text":"Tr\u01b0\u1edbc khi c\u00f3 Markdown, gi\u1edbi khoa h\u1ecdc th\u01b0\u1eddng s\u1eed d\u1ee5ng LaTex \u0111\u1ec3 so\u1ea1n th\u1ea3o c\u00e1c v\u0103n b\u1ea3n khoa h\u1ecdc. \u0110i\u1ec3m m\u1ea1nh c\u1ee7a LaTex l\u00e0 n\u00f3 r\u1ea5t nhi\u1ec1u plugin v\u00e0 c\u00f4ng c\u1ee5 h\u1ed7 tr\u1ee3, tuy nhi\u00ean c\u00fa ph\u00e1p c\u1ee7a LaTex r\u1ea5t ph\u1ee9c t\u1ea1p v\u00e0 kh\u00f3 \u0111\u1ec3 h\u1ecdc. \u0110\u1ec3 gi\u1ea3i quy\u1ebft v\u1ea5n n\u1ea1n n\u00e0y, Markdown ra \u0111\u1eddi nh\u01b0 m\u1ed9t ng\u00f4n ng\u1eef \u0111\u00e1nh d\u1ea5u nh\u1eb9 v\u00e0 d\u1ec5 s\u1eed d\u1ee5ng. To\u00e0n b\u1ed9 c\u00fa ph\u00e1p c\u1ee7a Markdown c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c t\u00f3m l\u01b0\u1ee3c trong m\u1ed9t hai trang A4 . V\u1ea5n \u0111\u1ec1 l\u00e0 khi ch\u00fang ta c\u00f3 m\u1ed9t b\u00e0i vi\u1ebft Markdown xong r\u1ed3i th\u00ec l\u00e0m g\u00ec? \u0110\u00f3n \u0111\u1ecdc ph\u1ea7n 3 (t\u1ed1i vi\u1ebft ti\u1ebfp).","title":"Markdown"},{"location":"Scientific Journey/day-two-documentation-markdown/#github-pages-va-mkdocs","text":"\u0110\u1ec3 l\u00e0m m\u1ed9t trang web t\u01b0\u01a1ng t\u1ef1 trang n\u00e0y c\u00f3 r\u1ea5t nhi\u1ec1u l\u00fd do. M\u1ed9t trong s\u1ed1 nh\u1eefng l\u00fd do ph\u1ed5 bi\u1ebfn l\u00e0 \u0111\u1ec3 l\u00e0m t\u00e0i li\u1ec7u cho d\u1ef1 \u00e1n (project page). M\u1ed9t l\u00fd do kh\u00e1c l\u00e0 do th\u00edch vi\u1ebft (nh\u01b0 m\u00ecnh). Trang web n\u00e0y \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng tr\u00ean Github Pages v\u00e0 MKDocs. L\u00fd do l\u00e0 v\u00ec so v\u1edbi Wordpress (n\u1ec1n t\u1ea3ng tr\u01b0\u1edbc \u0111\u00e2y m\u00ecnh vi\u1ebft), MKDocs s\u1eed d\u1ee5ng Markdown cho ph\u00e9p vi\u1ebft r\u1ea5t nhanh v\u00e0 d\u1ec5 d\u00e0ng t\u00f9y ch\u1ec9nh. MKDocs c\u0169ng \u1ed5n \u0111\u1ecbnh v\u00e0 t\u1ed1i gi\u1ea3n h\u01a1n nhi\u1ec1u so v\u1edbi Wordpress. \u0110\u1ec3 c\u00e0i \u0111\u1eb7t MKDocs h\u1ebft s\u1ee9c \u0111\u01a1n gi\u1ea3n: pip install mkdocs . M\u00ecnh khuy\u1ebfn kh\u00edch m\u1ecdi ng\u01b0\u1eddi t\u1ef1 t\u00ecm hi\u1ec3u th\u00eam v\u1ec1 ph\u1ea7n m\u1ec1m n\u00e0y. \u0110\u1ec3 b\u1eaft \u0111\u1ea7u m\u1ed9t trang web MKDocs, ta di chuy\u1ec3n \u0111\u1ebfn folder ch\u1ee9a repo (xem ph\u1ea7n d\u01b0\u1edbi) ch\u1ee9a trang web c\u1ee7a ch\u00fang ta, v\u00e0 d\u00f9ng c\u00e2u l\u1ec7nh mkdocs init . C\u00e1c folder v\u00e0 file kh\u00e1c ta kh\u00f4ng c\u1ea7n qu\u00e1 ch\u00fa tr\u1ecdng, quan tr\u1ecdng nh\u1ea5t l\u00fac n\u00e0y l\u00e0 folder docs v\u00e0 file mkdocs.yml . Folder docs ch\u1ee9a t\u1ea5t c\u1ea3 t\u00e0i li\u1ec7u ch\u00fang ta s\u1ebd vi\u1ebft. File mkdocs.yml ch\u1ee9a c\u00e1c thi\u1ebft l\u1eadp cho trang web. Sau khi vi\u1ebft 1 t\u00e0i li\u1ec7u Markdown v\u00e0o folder docs , c\u00e2u l\u1ec7nh mkdocs build s\u1ebd x\u00e2y d\u1ef1ng trang web c\u1ee7a ta v\u00e0o folder site . Vi\u1ec7c c\u1ea7n l\u00e0m ti\u1ebfp theo l\u00e0 upload folder n\u00e0y l\u00ean server \u0111\u1ec3 cung c\u1ea5p trang web. \u0110\u00e2y l\u00e0 vi\u1ec7c c\u1ee7a c\u00e2u l\u1ec7nh mkdocs gh-deploy . S\u1ebd c\u1ea7n m\u1ed9t s\u1ed1 t\u00f9y ch\u1ec9nh trong file mkdocs.yml . \u0110i\u1ec1u n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c n\u00f3i th\u00eam \u1edf ph\u1ea7n d\u01b0\u1edbi. Server c\u1ee7a blog n\u00e0y \u0111\u01b0\u1ee3c h\u1ed7 tr\u1ee3 b\u1edfi Github Pages. D\u1ecbch v\u1ee5 n\u00e0y c\u1ee7a Github nh\u1eb1m cung c\u1ea5p 1 n\u1ec1n t\u1ea3ng website HTML t\u0129nh (static HTML pages), d\u00e0nh cho vi\u1ec7c c\u00f4ng b\u1ed1 t\u00e0i li\u1ec7u c\u1ee7a d\u1ef1 \u00e1n, ho\u1eb7c \u0111\u1ec3 m\u00ecnh l\u1ee3i d\u1ee5ng l\u00e0m web c\u00e1 nh\u00e2n. Th\u00f4ng th\u01b0\u1eddng Github Pages \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng b\u1eb1ng Jekyll. Tuy nhi\u00ean v\u00ec m\u00ecnh \u0111\u00e3 s\u1eed d\u1ee5ng Python, v\u00e0 kh\u00f4ng mu\u1ed1n c\u00e0i th\u00eam NodeJS n\u00ean tr\u00ean n\u00e0y ch\u00fang ta s\u1ebd d\u00f9ng MKDocs \u0111\u1ec3 x\u00e2y d\u1ef1ng 1 trang web. \u0110\u1ec3 \u0111\u0103ng k\u00fd 1 trang Github Pages c\u00e1 nh\u00e2n, ch\u00fang ta c\u1ea7n t\u1ea1o m\u1ed9t repo c\u00f3 t\u00ean github-username .github.io , v\u1edbi github-username l\u00e0 t\u00ean Github c\u1ee7a ch\u00fang ta. L\u00fac n\u00e0y, Github s\u1ebd t\u1ef1 t\u1ea1o m\u1ed9t trang v\u1edbi t\u00ean mi\u1ec1n github-username .github.io \u0111\u1ec3 ch\u00fang ta c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng. \u0110\u1ec3 post MKDocs l\u00ean trang c\u00e1 nh\u00e2n n\u00e0y, sau khi clone repo tr\u00ean v\u1ec1 v\u00e0 c\u00e0i \u0111\u1eb7t MKDocs t\u1ea1i folder c\u1ee7a repo, ch\u00fang ta c\u1ea7n ch\u1ec9nh s\u1eeda file mkdocs.yml . M\u1eb7c \u0111\u1ecbnh MKDocs s\u1ebd post b\u00e0i theo c\u1ea5u h\u00ecnh c\u1ee7a Github Pages d\u00e0nh cho t\u1ed5 ch\u1ee9c thay v\u00ec c\u00e1 nh\u00e2n. \u0110i\u1ec3m kh\u00e1c bi\u1ec7t gi\u1eefa 2 lo\u1ea1i trang n\u00e0y l\u00e0 trang t\u1ed5 ch\u1ee9c cho ph\u00e9p post b\u00e0i l\u00ean branch gp-deploy, trong khi trang c\u00e1 nh\u00e2n post b\u00e0i l\u00ean branch master ( xem th\u00eam v\u1ec1 git branch ). \u0110\u1ec3 c\u1ea5u h\u00ecnh cho \u0111\u00fang v\u1edbi trang c\u00e1 nh\u00e2n, ch\u1ec9 \u0111\u01a1n gi\u1ea3n th\u00eam m\u1ed9t d\u00f2ng: remote_branch: master V\u1eady th\u00f4i :smile: anh em ch\u1ecbu kh\u00f3 \u0111\u1ecdc th\u00eam t\u00e0i li\u1ec7u nha.","title":"Github Pages v\u00e0 MKDocs"},{"location":"Scientific Journey/intro/","text":"H\u00e0nh tr\u00ecnh khoa h\u1ecdc: Python v\u00e0 R Trong n\u1ed7 l\u1ef1c m\u1edbi nh\u1ea5t c\u1ee7a L\u00e2m \u0111\u1ec3 t\u00e1ch r\u1eddi m\u1eb7t h\u1ecdc t\u1eadp/h\u1ecdc thu\u1eadt v\u00e0 cu\u1ed9c s\u1ed1ng n\u1ed9i t\u00e2m, \u0111\u00e2y l\u00e0 m\u1ed9t s\u1ef1 th\u1ee5t l\u00f9i tai h\u1ea1i. T\u1eeb gi\u1edd \u0111\u1ebfn khi sinh nh\u1eadt 23 tu\u1ed5i c\u1ee7a b\u1ea3n th\u00e2n, m\u00ecnh s\u1ebd \u0111\u1eb7t m\u1ee5c ti\u00eau m\u1ed7i ng\u00e0y (c\u1ed1 g\u1eafng) vi\u1ebft \u00edt ra 1 b\u00e0i, v\u00e0 c\u00f4ng vi\u1ec7c l\u1edbn nh\u1ea5t m\u00ecnh \u0111ang c\u1ea7n ph\u1ea3i l\u00e0m l\u00e0 nghi\u00ean c\u1ee9u. D\u0129 nhi\u00ean, m\u1ed9t ng\u00e0y th\u00ec c\u00f3 24 ch\u1ee9 kh\u00f4ng ph\u1ea3i 48 ti\u1ebfng \u0111\u1ed3ng h\u1ed3, v\u00e0 m\u1ed9t ng\u01b0\u1eddi m\u1ed7i ng\u00e0y ch\u1ec9 c\u00f3 th\u1ec3 c\u00f3 t\u1eebng \u0111\u1ea5y th\u1eddi gian \u0111\u1ec3 ng\u1ed3i \u0111\u1ecdc v\u00e0 vi\u1ebft. V\u00ec v\u1eady, t\u1ed1t nh\u1ea5t l\u00e0 vi\u1ebft v\u1ec1 c\u00e1i m\u00ecnh \u0111ang \u0111\u1ecdc, v\u1eeba \u0111\u1ec3 ghi nh\u1edb v\u1eeba \u0111\u1ec3 l\u1ea5y th\u00eam c\u1ea3m h\u1ee9ng m\u00e0 ng\u1ed3i... vi\u1ebft. M\u1ed9t ch\u00fat v\u1ec1 c\u00f4ng vi\u1ec7c hi\u1ec7n t\u1ea1i. N\u00f3i cho oai, m\u00ecnh l\u00e0 m\u1ed9t researcher \u1edf ICTLab tr\u01b0\u1eddng USTH. N\u00f3i chu\u1ea9n ra m\u00ecnh l\u00e0 m\u1ed9t th\u1ee3 code cho th\u1ea7y Pierre \u0111\u1ec3 l\u1ea5y ti\u1ec1n v\u00e0 c\u01a1 h\u1ed9i sang b\u1ec3n :)). C\u00e1c ph\u1ea7n code li\u00ean quan \u0111\u1ebfn s\u1eed d\u1ee5ng c\u00f4ng c\u1ee5 ICT trong tin sinh h\u1ecdc, c\u1ee5 th\u1ec3 l\u00e0 g\u00ec th\u00ec... m\u00ecnh c\u0169ng ch\u1eb3ng bi\u1ebft ha ha. 2 c\u00f4ng c\u1ee5 ch\u00ednh c\u1ee7a m\u00ecnh v\u00e0 c\u1ee7a r\u1ea5t nhi\u1ec1u d\u1ef1 \u00e1n Khoa h\u1ecdc D\u1eef li\u1ec7u (Data Science) l\u00e0 R v\u00e0 Python. C\u00e1 nh\u00e2n m\u00ecnh l\u00e0 ng\u01b0\u1eddi \u0111\u00e3 kh\u00e1 c\u00f3 kinh nghi\u1ec7m s\u1eed d\u1ee5ng Python trong th\u1eddi gian kh\u00e1 l\u00e2u, nh\u01b0ng R l\u00e0 m\u1ed9t th\u1ee9 kh\u00e1c h\u1eb3n. Kh\u00f4ng nh\u01b0 Python, m\u1ed9t ng\u00f4n ng\u1eef \u0111a d\u1ee5ng (general purpose), R l\u00e0 m\u1ed9t c\u00f4ng c\u1ee5 kh\u00e1 \u0111\u1eb7c tr\u01b0ng cho ng\u00e0nh khoa h\u1ecdc d\u1eef li\u1ec7u. T\u1ea5t c\u1ea3 m\u1ecdi th\u1ee9 li\u00ean quan \u0111\u1ec3n R \u0111\u1ec1u nh\u1eb1m 1 m\u1ee5c ti\u00eau: khai ph\u00e1 d\u1eef li\u1ec7u. V\u00ec v\u1eady R l\u00e0 m\u1ed9t c\u00f4ng c\u1ee5 tuy\u1ec7t v\u1eddi cho m\u1ed9t tr\u01b0\u1eddng h\u1ee3p c\u1ee5 th\u1ec3 c\u1ee7a m\u1ed9t ng\u00e0nh khoa h\u1ecdc c\u1ee5 th\u1ec3. C\u00e1c gi\u1edbi h\u1ea1n v\u1ec1 kh\u1ea3 n\u0103ng \u1ee9ng d\u1ee5ng c\u1ee7a R trong c\u00e1c b\u00e0i to\u00e1n kh\u00e1c b\u1ed7ng tr\u1edf n\u00ean kh\u00f4ng quan tr\u1ecdng. (N\u00f3i nh\u01b0 v\u1eady kh\u00f4ng ph\u1ea3i l\u00e0 R kh\u00f4ng th\u1ec3 l\u00e0m \u0111\u01b0\u1ee3c nh\u1eefng vi\u1ec7c kh\u00e1c. \u00d4ng n\u1ed9i Hadley vi\u1ebft c\u1ea3 m\u1ea5y quy\u1ec3n s\u00e1ch v\u1ec1 R \u0111\u01b0\u1ee3c d\u00e0n trang b\u1eb1ng... R, s\u1eed d\u1ee5ng RMarkdown v\u00e0 Bookdown, 2 package ch\u00ednh \u1ed5ng ph\u00e1t tri\u1ec3n cho R. Ngo\u00e0i ra c\u0169ng c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng Sparkling \u0111\u1ec3 vi\u1ebft web service cho c\u00e1c routine R, kh\u00e1 l\u00e0 h\u1eefu d\u1ee5ng \u0111\u1ec3 \u0111\u01b0a c\u00e1c nghi\u00ean c\u1ee9u R v\u00e0o \u00e1p d\u1ee5ng th\u1ef1c t\u1ebf. V\u00ec y\u00eau c\u1ea7u c\u1ee7a c\u00f4ng vi\u1ec7c trong d\u1ef1 \u00e1n, t\u1eeb ng\u00e0y h\u00f4m nay m\u00ecnh s\u1ebd b\u1eaft \u0111\u1ea7u c\u00e0y cu\u1ed1c R \u0111\u1ebfn khi n\u00e0o R v\u00e0 Python c\u1ee7a m\u00ecnh \u0111\u1ea1t \u0111\u01b0\u1ee3c t\u00ecnh \u0111\u1ed9 t\u01b0\u01a1ng \u0111\u1ed1i \u0111\u1ed3ng nh\u1ea5t v\u1edbi nhau. Gi\u1ed1ng nh\u01b0 tay tr\u00e1i v\u00e0 tay ph\u1ea3i v\u1eady, nh\u01b0ng m\u00ecnh l\u00e0 ng\u01b0\u1eddi thu\u1eadn c\u1ea3 2 tay. C\u00e1c t\u00e0i li\u1ec7u, reference v\u00e0 v\u00ed d\u1ee5 s\u1ebd s\u1eed d\u1ee5ng b\u1ed9 c\u00f4ng c\u1ee5 R chu\u1ea9n, ch\u1ea1y tr\u00ean RStudio tr\u00ean n\u1ec1n Windows 10. T\u00e0i li\u1ec7u tham kh\u1ea3o ch\u00ednh s\u1ebd l\u00e0 R for Data Science c\u1ee7a Hadley (\u00f4ng tr\u00f9m ng\u00e0nh R) \u0111\u1ed1i v\u1edbi R v\u00e0 How to think like a Computer Scientist \u0111\u1ed1i v\u1edbi Python. Let's the journey begin.","title":"H\u00e0nh tr\u00ecnh khoa h\u1ecdc: Python v\u00e0 R"},{"location":"Scientific Journey/intro/#hanh-trinh-khoa-hoc-python-va-r","text":"Trong n\u1ed7 l\u1ef1c m\u1edbi nh\u1ea5t c\u1ee7a L\u00e2m \u0111\u1ec3 t\u00e1ch r\u1eddi m\u1eb7t h\u1ecdc t\u1eadp/h\u1ecdc thu\u1eadt v\u00e0 cu\u1ed9c s\u1ed1ng n\u1ed9i t\u00e2m, \u0111\u00e2y l\u00e0 m\u1ed9t s\u1ef1 th\u1ee5t l\u00f9i tai h\u1ea1i. T\u1eeb gi\u1edd \u0111\u1ebfn khi sinh nh\u1eadt 23 tu\u1ed5i c\u1ee7a b\u1ea3n th\u00e2n, m\u00ecnh s\u1ebd \u0111\u1eb7t m\u1ee5c ti\u00eau m\u1ed7i ng\u00e0y (c\u1ed1 g\u1eafng) vi\u1ebft \u00edt ra 1 b\u00e0i, v\u00e0 c\u00f4ng vi\u1ec7c l\u1edbn nh\u1ea5t m\u00ecnh \u0111ang c\u1ea7n ph\u1ea3i l\u00e0m l\u00e0 nghi\u00ean c\u1ee9u. D\u0129 nhi\u00ean, m\u1ed9t ng\u00e0y th\u00ec c\u00f3 24 ch\u1ee9 kh\u00f4ng ph\u1ea3i 48 ti\u1ebfng \u0111\u1ed3ng h\u1ed3, v\u00e0 m\u1ed9t ng\u01b0\u1eddi m\u1ed7i ng\u00e0y ch\u1ec9 c\u00f3 th\u1ec3 c\u00f3 t\u1eebng \u0111\u1ea5y th\u1eddi gian \u0111\u1ec3 ng\u1ed3i \u0111\u1ecdc v\u00e0 vi\u1ebft. V\u00ec v\u1eady, t\u1ed1t nh\u1ea5t l\u00e0 vi\u1ebft v\u1ec1 c\u00e1i m\u00ecnh \u0111ang \u0111\u1ecdc, v\u1eeba \u0111\u1ec3 ghi nh\u1edb v\u1eeba \u0111\u1ec3 l\u1ea5y th\u00eam c\u1ea3m h\u1ee9ng m\u00e0 ng\u1ed3i... vi\u1ebft. M\u1ed9t ch\u00fat v\u1ec1 c\u00f4ng vi\u1ec7c hi\u1ec7n t\u1ea1i. N\u00f3i cho oai, m\u00ecnh l\u00e0 m\u1ed9t researcher \u1edf ICTLab tr\u01b0\u1eddng USTH. N\u00f3i chu\u1ea9n ra m\u00ecnh l\u00e0 m\u1ed9t th\u1ee3 code cho th\u1ea7y Pierre \u0111\u1ec3 l\u1ea5y ti\u1ec1n v\u00e0 c\u01a1 h\u1ed9i sang b\u1ec3n :)). C\u00e1c ph\u1ea7n code li\u00ean quan \u0111\u1ebfn s\u1eed d\u1ee5ng c\u00f4ng c\u1ee5 ICT trong tin sinh h\u1ecdc, c\u1ee5 th\u1ec3 l\u00e0 g\u00ec th\u00ec... m\u00ecnh c\u0169ng ch\u1eb3ng bi\u1ebft ha ha. 2 c\u00f4ng c\u1ee5 ch\u00ednh c\u1ee7a m\u00ecnh v\u00e0 c\u1ee7a r\u1ea5t nhi\u1ec1u d\u1ef1 \u00e1n Khoa h\u1ecdc D\u1eef li\u1ec7u (Data Science) l\u00e0 R v\u00e0 Python. C\u00e1 nh\u00e2n m\u00ecnh l\u00e0 ng\u01b0\u1eddi \u0111\u00e3 kh\u00e1 c\u00f3 kinh nghi\u1ec7m s\u1eed d\u1ee5ng Python trong th\u1eddi gian kh\u00e1 l\u00e2u, nh\u01b0ng R l\u00e0 m\u1ed9t th\u1ee9 kh\u00e1c h\u1eb3n. Kh\u00f4ng nh\u01b0 Python, m\u1ed9t ng\u00f4n ng\u1eef \u0111a d\u1ee5ng (general purpose), R l\u00e0 m\u1ed9t c\u00f4ng c\u1ee5 kh\u00e1 \u0111\u1eb7c tr\u01b0ng cho ng\u00e0nh khoa h\u1ecdc d\u1eef li\u1ec7u. T\u1ea5t c\u1ea3 m\u1ecdi th\u1ee9 li\u00ean quan \u0111\u1ec3n R \u0111\u1ec1u nh\u1eb1m 1 m\u1ee5c ti\u00eau: khai ph\u00e1 d\u1eef li\u1ec7u. V\u00ec v\u1eady R l\u00e0 m\u1ed9t c\u00f4ng c\u1ee5 tuy\u1ec7t v\u1eddi cho m\u1ed9t tr\u01b0\u1eddng h\u1ee3p c\u1ee5 th\u1ec3 c\u1ee7a m\u1ed9t ng\u00e0nh khoa h\u1ecdc c\u1ee5 th\u1ec3. C\u00e1c gi\u1edbi h\u1ea1n v\u1ec1 kh\u1ea3 n\u0103ng \u1ee9ng d\u1ee5ng c\u1ee7a R trong c\u00e1c b\u00e0i to\u00e1n kh\u00e1c b\u1ed7ng tr\u1edf n\u00ean kh\u00f4ng quan tr\u1ecdng. (N\u00f3i nh\u01b0 v\u1eady kh\u00f4ng ph\u1ea3i l\u00e0 R kh\u00f4ng th\u1ec3 l\u00e0m \u0111\u01b0\u1ee3c nh\u1eefng vi\u1ec7c kh\u00e1c. \u00d4ng n\u1ed9i Hadley vi\u1ebft c\u1ea3 m\u1ea5y quy\u1ec3n s\u00e1ch v\u1ec1 R \u0111\u01b0\u1ee3c d\u00e0n trang b\u1eb1ng... R, s\u1eed d\u1ee5ng RMarkdown v\u00e0 Bookdown, 2 package ch\u00ednh \u1ed5ng ph\u00e1t tri\u1ec3n cho R. Ngo\u00e0i ra c\u0169ng c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng Sparkling \u0111\u1ec3 vi\u1ebft web service cho c\u00e1c routine R, kh\u00e1 l\u00e0 h\u1eefu d\u1ee5ng \u0111\u1ec3 \u0111\u01b0a c\u00e1c nghi\u00ean c\u1ee9u R v\u00e0o \u00e1p d\u1ee5ng th\u1ef1c t\u1ebf. V\u00ec y\u00eau c\u1ea7u c\u1ee7a c\u00f4ng vi\u1ec7c trong d\u1ef1 \u00e1n, t\u1eeb ng\u00e0y h\u00f4m nay m\u00ecnh s\u1ebd b\u1eaft \u0111\u1ea7u c\u00e0y cu\u1ed1c R \u0111\u1ebfn khi n\u00e0o R v\u00e0 Python c\u1ee7a m\u00ecnh \u0111\u1ea1t \u0111\u01b0\u1ee3c t\u00ecnh \u0111\u1ed9 t\u01b0\u01a1ng \u0111\u1ed1i \u0111\u1ed3ng nh\u1ea5t v\u1edbi nhau. Gi\u1ed1ng nh\u01b0 tay tr\u00e1i v\u00e0 tay ph\u1ea3i v\u1eady, nh\u01b0ng m\u00ecnh l\u00e0 ng\u01b0\u1eddi thu\u1eadn c\u1ea3 2 tay. C\u00e1c t\u00e0i li\u1ec7u, reference v\u00e0 v\u00ed d\u1ee5 s\u1ebd s\u1eed d\u1ee5ng b\u1ed9 c\u00f4ng c\u1ee5 R chu\u1ea9n, ch\u1ea1y tr\u00ean RStudio tr\u00ean n\u1ec1n Windows 10. T\u00e0i li\u1ec7u tham kh\u1ea3o ch\u00ednh s\u1ebd l\u00e0 R for Data Science c\u1ee7a Hadley (\u00f4ng tr\u00f9m ng\u00e0nh R) \u0111\u1ed1i v\u1edbi R v\u00e0 How to think like a Computer Scientist \u0111\u1ed1i v\u1edbi Python. Let's the journey begin.","title":"H\u00e0nh tr\u00ecnh khoa h\u1ecdc: Python v\u00e0 R"},{"location":"Scientific Journey/xgboost_c+t/","text":"In this document I go in depth into using XGBoost to enhence the performance of C+T by combining multiple C+T predictors. 1 Prequisites The following packages are required to knit this document: bigstatsr bigsnpr data.table xgboost Dataset Please change working direction in directory I use the \"Simus\" simulated dataset provided by Florian. This dataset contain ~650,000 SNPs in 2 chromosomes. 20% of them are cases and the rest 80% are controls. C+T step Generating a matrix of C+T with varied clumping radius and p-value thresholds. \\beta s and p-values are aquired in sumstats file (see sumstats.txt ) XGBoost only accept dataframe, so all FBM output must be converted to data frame Data treatment: Oversampling To oversampling I extract all positive ( affection = 1 ), and append them a number of times. For this simulated dataset, I enhanced the number of cases 10 times, thus the ratio become ~70:30 (from 20:80). XGBoost XGBoost provide slightly better result compare to SCT. However this result heavily depend on booster and objective selection. Experiments shown that gblinear booster with count:poisson objective give the best results. For the first experimentation, I run 9 different max_depth and 9 nrounds with count:poisson objective function. Each combination is repeated 10 times and the mean value calculated to rule out the randomess of gblinear booster poisson_AUCs <- matrix(,nrow = 10, ncol = 10) for (i in seq(2 : 10)){ for (j in seq(2: 10)) { temp <- c() for(t in seq(1:10)){ bstSparse <- xgboost(data = ds[], label = y.train, booster=\"gblinear\", max_depth = i, eta = 1, nthread = 2, nrounds = j, lambda = 0.1, objective = \"count:poisson\" ) pred <- predict(bstSparse, PRS_test[]) temp <- append(temp,AUC(pred = pred, test$fam$affection)) } poisson_AUCs[i,j] <- mean(temp) } } ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.7817714 0.7823509 0.7823041 0.7822926 0.7825028 0.7824836 ## [2,] 0.7820513 0.7822373 0.7823633 0.7824287 0.7822630 0.7824896 ## [3,] 0.7814487 0.7821501 0.7823655 0.7826146 0.7825234 0.7825795 ## [4,] 0.7820921 0.7824120 0.7824182 0.7826145 0.7825073 0.7824446 ## [5,] 0.7821936 0.7824070 0.7821748 0.7823153 0.7826239 0.7824142 ## [6,] 0.7819266 0.7822376 0.7822175 0.7824563 0.7823541 0.7824829 ## [7,] 0.7818918 0.7820830 0.7823140 0.7823306 0.7824132 0.7824501 ## [8,] 0.7820375 0.7822164 0.7824588 0.7824749 0.7825382 0.7825095 ## [9,] 0.7822184 0.7819440 0.7825596 0.7823647 0.7824570 0.7823492 ## [10,] NA NA NA NA NA NA ## [,7] [,8] [,9] [,10] ## [1,] 0.7823923 0.7821336 0.7818004 NA ## [2,] 0.7823814 0.7821682 0.7817430 NA ## [3,] 0.7823602 0.7821881 0.7817512 NA ## [4,] 0.7823655 0.7820304 0.7818152 NA ## [5,] 0.7822578 0.7821342 0.7818768 NA ## [6,] 0.7824436 0.7822035 0.7817998 NA ## [7,] 0.7824093 0.7822224 0.7817339 NA ## [8,] 0.7824048 0.7821033 0.7817913 NA ## [9,] 0.7823197 0.7822398 0.7817214 NA ## [10,] NA NA NA NA I omitted the code for the next 2 experiments as they are largely similar, with exception of the option objective for xgboost function. For the next experimentation, I run 9 different max_depth and 9 nrounds with reg:logistic objective function. Each combination is repeated 10 times and the mean value calculated to rule out the randomess of gblinear booster For the final experimentation, I run 9 different max_depth and 9 nrounds with binary:logicraw objective function. This function is interesting since it's specific for binary classification. Here is a composite boxplot comparing different objective functions. Index 1 is count:poisson , 2 is reg:logistic and 3 is binary:logistic The effect of Boosters I replicate the experiment with the gbtree booster. Every other aspect stay the same. The effect of max_depth and nrounds is similar to random forest : more depth means more stability, while more rounds might lead to overfitting. tree_poisson_AUCs <- matrix(,nrow = 10, ncol = 10) for (i in seq(2 : 10)){ for (j in seq(2: 10)) { temp <- c() bstSparse <- xgboost(data = ds[], label = y.train, booster=\"gbtree\", max_depth = i, eta = 1, nthread = 2, nrounds = j, lambda = 0.1, objective = \"count:poisson\" ) pred <- predict(bstSparse, PRS_test[]) tree_poisson_AUCs[i,j] <- append(temp,AUC(pred = pred, test$fam$affection)) } } ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.5997502 0.6783138 0.6797928 0.7031598 0.7184346 0.7197522 ## [2,] 0.6857363 0.6944414 0.7274155 0.7318843 0.7333248 0.7421854 ## [3,] 0.7133489 0.7344719 0.7470404 0.7456065 0.7437271 0.7440640 ## [4,] 0.7346792 0.7346642 0.7398461 0.7412047 0.7496907 0.7480144 ## [5,] 0.7158010 0.7299822 0.7408819 0.7438784 0.7454125 0.7431477 ## [6,] 0.7019057 0.7270527 0.7316911 0.7332872 0.7334402 0.7384122 ## [7,] 0.7100347 0.7288694 0.7394506 0.7396847 0.7387767 0.7415382 ## [8,] 0.7152308 0.7363614 0.7428634 0.7431836 0.7450280 0.7444436 ## [9,] 0.7097638 0.7221877 0.7252510 0.7301520 0.7315682 0.7289940 ## [10,] NA NA NA NA NA NA ## [,7] [,8] [,9] [,10] ## [1,] 0.7196084 0.7276948 0.7265067 NA ## [2,] 0.7405149 0.7404706 0.7459368 NA ## [3,] 0.7417071 0.7426235 0.7399623 NA ## [4,] 0.7473723 0.7508377 0.7525843 NA ## [5,] 0.7372777 0.7398243 0.7404263 NA ## [6,] 0.7414204 0.7451333 0.7419437 NA ## [7,] 0.7438240 0.7396513 0.7403887 NA ## [8,] 0.7429152 0.7452253 0.7466324 NA ## [9,] 0.7257426 0.7244041 0.7217195 NA ## [10,] NA NA NA NA The following graph compare gbtree vs gblinear . We can see clearly, the difference between different booster (non-linear/tree-based vs linear) booster. gblinear offer far more stable and better result compare to gbtree in this case. This result, however, might not reflex the whole situation. As demonstrated with gblinear , objective function play a major role in both performance and stability. Original Rmd documment provided here https://squidfunk.github.io/mkdocs-material/getting-started/ \u21a9","title":"XGBoost's objective function comparison"},{"location":"Scientific Journey/xgboost_c+t/#prequisites","text":"The following packages are required to knit this document: bigstatsr bigsnpr data.table xgboost","title":"Prequisites"},{"location":"Scientific Journey/xgboost_c+t/#dataset","text":"Please change working direction in directory I use the \"Simus\" simulated dataset provided by Florian. This dataset contain ~650,000 SNPs in 2 chromosomes. 20% of them are cases and the rest 80% are controls.","title":"Dataset"},{"location":"Scientific Journey/xgboost_c+t/#ct-step","text":"Generating a matrix of C+T with varied clumping radius and p-value thresholds. \\beta s and p-values are aquired in sumstats file (see sumstats.txt ) XGBoost only accept dataframe, so all FBM output must be converted to data frame","title":"C+T step"},{"location":"Scientific Journey/xgboost_c+t/#data-treatment-oversampling","text":"To oversampling I extract all positive ( affection = 1 ), and append them a number of times. For this simulated dataset, I enhanced the number of cases 10 times, thus the ratio become ~70:30 (from 20:80).","title":"Data treatment: Oversampling"},{"location":"Scientific Journey/xgboost_c+t/#xgboost","text":"XGBoost provide slightly better result compare to SCT. However this result heavily depend on booster and objective selection. Experiments shown that gblinear booster with count:poisson objective give the best results. For the first experimentation, I run 9 different max_depth and 9 nrounds with count:poisson objective function. Each combination is repeated 10 times and the mean value calculated to rule out the randomess of gblinear booster poisson_AUCs <- matrix(,nrow = 10, ncol = 10) for (i in seq(2 : 10)){ for (j in seq(2: 10)) { temp <- c() for(t in seq(1:10)){ bstSparse <- xgboost(data = ds[], label = y.train, booster=\"gblinear\", max_depth = i, eta = 1, nthread = 2, nrounds = j, lambda = 0.1, objective = \"count:poisson\" ) pred <- predict(bstSparse, PRS_test[]) temp <- append(temp,AUC(pred = pred, test$fam$affection)) } poisson_AUCs[i,j] <- mean(temp) } } ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.7817714 0.7823509 0.7823041 0.7822926 0.7825028 0.7824836 ## [2,] 0.7820513 0.7822373 0.7823633 0.7824287 0.7822630 0.7824896 ## [3,] 0.7814487 0.7821501 0.7823655 0.7826146 0.7825234 0.7825795 ## [4,] 0.7820921 0.7824120 0.7824182 0.7826145 0.7825073 0.7824446 ## [5,] 0.7821936 0.7824070 0.7821748 0.7823153 0.7826239 0.7824142 ## [6,] 0.7819266 0.7822376 0.7822175 0.7824563 0.7823541 0.7824829 ## [7,] 0.7818918 0.7820830 0.7823140 0.7823306 0.7824132 0.7824501 ## [8,] 0.7820375 0.7822164 0.7824588 0.7824749 0.7825382 0.7825095 ## [9,] 0.7822184 0.7819440 0.7825596 0.7823647 0.7824570 0.7823492 ## [10,] NA NA NA NA NA NA ## [,7] [,8] [,9] [,10] ## [1,] 0.7823923 0.7821336 0.7818004 NA ## [2,] 0.7823814 0.7821682 0.7817430 NA ## [3,] 0.7823602 0.7821881 0.7817512 NA ## [4,] 0.7823655 0.7820304 0.7818152 NA ## [5,] 0.7822578 0.7821342 0.7818768 NA ## [6,] 0.7824436 0.7822035 0.7817998 NA ## [7,] 0.7824093 0.7822224 0.7817339 NA ## [8,] 0.7824048 0.7821033 0.7817913 NA ## [9,] 0.7823197 0.7822398 0.7817214 NA ## [10,] NA NA NA NA I omitted the code for the next 2 experiments as they are largely similar, with exception of the option objective for xgboost function. For the next experimentation, I run 9 different max_depth and 9 nrounds with reg:logistic objective function. Each combination is repeated 10 times and the mean value calculated to rule out the randomess of gblinear booster For the final experimentation, I run 9 different max_depth and 9 nrounds with binary:logicraw objective function. This function is interesting since it's specific for binary classification. Here is a composite boxplot comparing different objective functions. Index 1 is count:poisson , 2 is reg:logistic and 3 is binary:logistic","title":"XGBoost"},{"location":"Scientific Journey/xgboost_c+t/#the-effect-of-boosters","text":"I replicate the experiment with the gbtree booster. Every other aspect stay the same. The effect of max_depth and nrounds is similar to random forest : more depth means more stability, while more rounds might lead to overfitting. tree_poisson_AUCs <- matrix(,nrow = 10, ncol = 10) for (i in seq(2 : 10)){ for (j in seq(2: 10)) { temp <- c() bstSparse <- xgboost(data = ds[], label = y.train, booster=\"gbtree\", max_depth = i, eta = 1, nthread = 2, nrounds = j, lambda = 0.1, objective = \"count:poisson\" ) pred <- predict(bstSparse, PRS_test[]) tree_poisson_AUCs[i,j] <- append(temp,AUC(pred = pred, test$fam$affection)) } } ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.5997502 0.6783138 0.6797928 0.7031598 0.7184346 0.7197522 ## [2,] 0.6857363 0.6944414 0.7274155 0.7318843 0.7333248 0.7421854 ## [3,] 0.7133489 0.7344719 0.7470404 0.7456065 0.7437271 0.7440640 ## [4,] 0.7346792 0.7346642 0.7398461 0.7412047 0.7496907 0.7480144 ## [5,] 0.7158010 0.7299822 0.7408819 0.7438784 0.7454125 0.7431477 ## [6,] 0.7019057 0.7270527 0.7316911 0.7332872 0.7334402 0.7384122 ## [7,] 0.7100347 0.7288694 0.7394506 0.7396847 0.7387767 0.7415382 ## [8,] 0.7152308 0.7363614 0.7428634 0.7431836 0.7450280 0.7444436 ## [9,] 0.7097638 0.7221877 0.7252510 0.7301520 0.7315682 0.7289940 ## [10,] NA NA NA NA NA NA ## [,7] [,8] [,9] [,10] ## [1,] 0.7196084 0.7276948 0.7265067 NA ## [2,] 0.7405149 0.7404706 0.7459368 NA ## [3,] 0.7417071 0.7426235 0.7399623 NA ## [4,] 0.7473723 0.7508377 0.7525843 NA ## [5,] 0.7372777 0.7398243 0.7404263 NA ## [6,] 0.7414204 0.7451333 0.7419437 NA ## [7,] 0.7438240 0.7396513 0.7403887 NA ## [8,] 0.7429152 0.7452253 0.7466324 NA ## [9,] 0.7257426 0.7244041 0.7217195 NA ## [10,] NA NA NA NA The following graph compare gbtree vs gblinear . We can see clearly, the difference between different booster (non-linear/tree-based vs linear) booster. gblinear offer far more stable and better result compare to gbtree in this case. This result, however, might not reflex the whole situation. As demonstrated with gblinear , objective function play a major role in both performance and stability. Original Rmd documment provided here https://squidfunk.github.io/mkdocs-material/getting-started/ \u21a9","title":"The effect of Boosters"},{"location":"Scientific Journey/Report/report/","text":"In this document I investigate multiple method to calculate PRS. Some of them are based on combining multiple C+T predictors to enhence the performance of C+T. They are based on the work of Florian Priv\u00e9 . 1 Methods In this RMD documents there are 5 methods demonstrated: SCT XGBoost (with SCT 1^{st} layer) Lassosum Random Forest (with SCT 1^{st} layer) Neural Net (with SCT 1^{st} layer) More promising (SCT, XGBoost, Lassosum, Random Forest) are place first, and Neural Net are included for reference. For some reason I can not get Keras to work with SCT first layer (always return AUC of 0.5). This can either be a problem of the target function (I have yet to find a good function that operate well), or a bug within Keras (unlikely). Or perhap the Euclid distance after C+T layer are too small for neuralnet - that's why regression based method work better? Required packages: bigstatsr bigsnpr lassosum data.table xgboost ranger keras library(bigstatsr) library(bigsnpr) library(lassosum) library(data.table) library(xgboost) library(ranger) library(keras) Dataset Please change working direction in directory I use the \"Simus\" simulated dataset provided by Florian. This dataset contain ~650,000 SNPs in 2 chromosomes. 20% of them are cases and the rest 80% are controls. setwd(directory) # Train data sumstats <- bigreadr::fread2(file_sumstat) # snp_readBed(paste(file_train, \"bed\", sep='.')) train <- snp_attach(paste(file_train, \"rds\", sep='.')) G.train <- train$genotypes CHR <- train$map$chromosome POS <- train$map$physical.pos NCORES <- nb_cores() lpval <- -log10(sumstats$pval) y.train <- train$fam$affection # Test data # snp_readBed(paste(file_test, \"bed\", sep='.')) test <- snp_attach(paste(file_test, \"rds\", sep='.')) G.test <- test$genotypes # C+T step Generating a matrix of C+T with varied clumping radius and p-value thresholds. \\beta s and p-values are aquired in sumstats file (see sumstats.txt ) all_keep <- snp_grid_clumping(G.train, CHR, POS, lpval, ncores = NCORES) PRS <-snp_grid_PRS(G.train,all_keep = all_keep, betas = sumstats$beta,lpval) PRS_test <- snp_grid_PRS(G.test, all_keep = all_keep, betas = sumstats$beta,lpval) Other than SCT, other methods require DataFrame or R Matrix than FBM to manage data. I converted the FBM output of C+T to DataFrame and change the corresponding columns. Data treatment: Oversampling To oversampling I extract all positive ( affection = 1 ), and append them a number of times. For this simulated dataset, I enhanced the number of cases 10 times, thus the ratio become ~70:30 (from 20:80). y_case <- which(train$fam$affection %in% c(1)) for (i in seq(1, 10)){ y.train <- append(y.train, train$fam$affection[y_case]) ds <- rbind(ds, PRS[y_case,]) } ds.FBM <- as_FBM(ds) Method 1: SCT No parameters required. Combining (Sparse Logistic Regression) C+T predictors for best result. # Reference # M <- snp_grid_stacking(multi_PRS = ds.FBM, y.train = y.train, ncores = NCORES) M <- snp_grid_stacking(multi_PRS = PRS, y.train = train$fam$affection) beta <- as_FBM(matrix(M$beta.G)) pred.SCT <- big_prodMat(G.test, beta) #AUC(pred = pred.SCT, test$fam$affection) AUC(pred = pred.SCT, test$fam$affection) ## [1] 0.7816448 Method 2: XGBoost XGBoost provide slightly better result compare to SCT. However this result heavily depend on booster and objective selection. Experiments shown that gblinear booster with count:poisson objective give the best results. For experimentation, I run 9 different max_depth and 9 nrounds . Each combination is repeated 10 times and the mean value calculated to rule out the randomess of gblinear booster AUCs <- matrix(,nrow = 10, ncol = 10) for (i in seq(2 : 10)){ for (j in seq(2: 10)) { temp <- c() for(t in seq(1:10)){ bstSparse <- xgboost(data = ds[], label = y.train, booster=\"gblinear\", max_depth = i, eta = 1, nthread = 2, nrounds = j, lambda = 0.1, objective = \"count:poisson\" ) pred <- predict(bstSparse, PRS_test[]) temp <- append(temp,AUC(pred = pred, test$fam$affection)) } AUCs[i,j] <- mean(temp) } } AUCs ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.7810081 0.7814691 0.7819848 0.7820644 0.7821634 0.7819551 ## [2,] 0.7815161 0.7816789 0.7819097 0.7820451 0.7820130 0.7820510 ## [3,] 0.7811385 0.7817903 0.7820209 0.7821792 0.7821172 0.7820448 ## [4,] 0.7812592 0.7814580 0.7820362 0.7819456 0.7820135 0.7819257 ## [5,] 0.7811024 0.7814672 0.7818094 0.7819867 0.7820772 0.7820431 ## [6,] 0.7813253 0.7816065 0.7818425 0.7822446 0.7821884 0.7819205 ## [7,] 0.7810239 0.7817146 0.7817587 0.7819281 0.7822180 0.7819721 ## [8,] 0.7811480 0.7820816 0.7818343 0.7820383 0.7820944 0.7819818 ## [9,] 0.7815687 0.7814758 0.7816728 0.7818929 0.7820611 0.7820187 ## [10,] NA NA NA NA NA NA ## [,7] [,8] [,9] [,10] ## [1,] 0.7817276 0.7816945 0.7814971 NA ## [2,] 0.7819033 0.7817853 0.7816005 NA ## [3,] 0.7818634 0.7816886 0.7814594 NA ## [4,] 0.7819806 0.7818078 0.7813999 NA ## [5,] 0.7818562 0.7816829 0.7814363 NA ## [6,] 0.7819113 0.7817263 0.7814301 NA ## [7,] 0.7818913 0.7817029 0.7815198 NA ## [8,] 0.7819087 0.7816597 0.7814085 NA ## [9,] 0.7817407 0.7817544 0.7815585 NA ## [10,] NA NA NA NA boxplot(AUCs) Method 3: Lassosum Lassosum use L1 regularization to better fit the regression model. For basic L1 regularization of Linear Regression, 2 metaparameters are required: a learning rate \\alpha and a regularization parameter \\lambda . However, lassosum does not require these parameters, as the software automatically scan the parameters space and select the best \\alpha and \\lambda setwd(directory) cor <- p2cor(p = sumstats$pval, n = 8000, sign=sumstats$beta) out <- lassosum.pipeline(cor =cor, chr=sumstats$chromosome, pos = sumstats$physical.pos, A1 = sumstats$allele1, A2 = sumstats$allele2, ref.bfile = file_train, test.bfile = file_test, LDblocks = LDblocks) v <- validate(out) out2 <- subset(out, s=v$best.s, lambda = v$lambda) v2 <- validate(out2) v2$best.validation.result AUC(v$best.pgs, v$pheno) ## [1] 0.7308175 Method 4: Random Forest Random forest also give good AUC (above 70%, but less than other methods except for keras ). The result highly dependen on the number of trees (more trees equals more convergence), and the learning rate also contribute to an lesser extend. The code in this session will sweep through the parameters: 1 to 10 trees and alpha from 0.1 to 1 (0.1 increment). PRS_test.df <- as.data.frame(PRS_test[]) f <- as.formula(paste(\"y ~\", paste(cols[!cols %in% \"PRS.df\"], collapse = \" + \"))) for (j in seq(1 : 10)){ for (i in seq(1: 10)) { rf <- ranger(f, data = PRS.df, num.trees = j*100, verbose = TRUE, write.forest = TRUE, alpha = (i/10)) pred <- predict(rf, PRS_test.df) AUCs[i,j] <- AUC(pred = pred$predictions, test$fam$affection) } } AUCs ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.7532205 0.7535048 0.7564268 0.7501722 0.7507132 0.7515333 ## [2,] 0.7440113 0.7530198 0.7540833 0.7532506 0.7518778 0.7546351 ## [3,] 0.7545732 0.7500895 0.7514272 0.7556325 0.7545590 0.7539930 ## [4,] 0.7549227 0.7515300 0.7500226 0.7548174 0.7503537 0.7543692 ## [5,] 0.7488554 0.7515392 0.7508904 0.7535265 0.7544863 0.7536828 ## [6,] 0.7501346 0.7512984 0.7525976 0.7547204 0.7520592 0.7536302 ## [7,] 0.7496137 0.7509414 0.7527130 0.7549386 0.7512415 0.7553466 ## [8,] 0.7435724 0.7513862 0.7545431 0.7513971 0.7525416 0.7559703 ## [9,] 0.7513695 0.7512825 0.7537756 0.7543274 0.7507516 0.7562646 ## [10,] 0.7473798 0.7476314 0.7535859 0.7539036 0.7543751 0.7537062 ## [,7] [,8] [,9] [,10] ## [1,] 0.7541560 0.7521771 0.7544930 0.7532481 ## [2,] 0.7547923 0.7526261 0.7558700 0.7539353 ## [3,] 0.7526620 0.7529212 0.7531896 0.7540800 ## [4,] 0.7532573 0.7556534 0.7528978 0.7538868 ## [5,] 0.7518979 0.7547931 0.7558106 0.7535190 ## [6,] 0.7544119 0.7547555 0.7558792 0.7548132 ## [7,] 0.7524614 0.7543283 0.7537857 0.7549896 ## [8,] 0.7533702 0.7535374 0.7533777 0.7520208 ## [9,] 0.7569953 0.7558482 0.7558123 0.7530600 ## [10,] 0.7538342 0.7547547 0.7521612 0.7539888 boxplot(AUCs) Method 5: Nerual net (Keras) For some reason, keras cannot return good result; the prediction always biased to controls. Even with adjusted weights and oversampling, the bias is still there and AUC is always 0.5 . I included it here in case I figure it out in the future. In this example, I put a 10:1 weight ratio between case and control. y.train <- train$fam$affection model <- keras_model_sequential() model %>% layer_dense(units = 1000, activation = \"softmax\", input_shape = c(ncol(ds)), kernel_regularizer=regularizer_l1(0.02)) %>% # layer_dense(units = 1000, activation = \"softmax\", input_shape = c(2800)) %>% # layer_dropout(rate=0.1) %>% layer_dense(units = 2, activation = \"softmax\", kernel_regularizer=regularizer_l1(0.01)) model %>% compile( #loss = 'categorical_crossentropy', loss = 'mean_absolute_percentage_error', optimizer = 'SGD', metrics = c('accuracy') ) y.train <- to_categorical(y.train) history <- model$fit( ds, y.train, class_weight = list(1, 10), epochs = as.integer(10), batch_size = as.integer(28) ) AUC(predict_classes(model, PRS_test[]), test$fam$affection) ## [1] 0.5 Original Rmd documment provided here \u21a9","title":"Viable method to combine C+T predictors for enhanced prediction power"},{"location":"Scientific Journey/Report/report/#methods","text":"In this RMD documents there are 5 methods demonstrated: SCT XGBoost (with SCT 1^{st} layer) Lassosum Random Forest (with SCT 1^{st} layer) Neural Net (with SCT 1^{st} layer) More promising (SCT, XGBoost, Lassosum, Random Forest) are place first, and Neural Net are included for reference. For some reason I can not get Keras to work with SCT first layer (always return AUC of 0.5). This can either be a problem of the target function (I have yet to find a good function that operate well), or a bug within Keras (unlikely). Or perhap the Euclid distance after C+T layer are too small for neuralnet - that's why regression based method work better?","title":"Methods"},{"location":"Scientific Journey/Report/report/#required-packages","text":"bigstatsr bigsnpr lassosum data.table xgboost ranger keras library(bigstatsr) library(bigsnpr) library(lassosum) library(data.table) library(xgboost) library(ranger) library(keras)","title":"Required packages:"},{"location":"Scientific Journey/Report/report/#dataset","text":"Please change working direction in directory I use the \"Simus\" simulated dataset provided by Florian. This dataset contain ~650,000 SNPs in 2 chromosomes. 20% of them are cases and the rest 80% are controls. setwd(directory) # Train data sumstats <- bigreadr::fread2(file_sumstat) # snp_readBed(paste(file_train, \"bed\", sep='.')) train <- snp_attach(paste(file_train, \"rds\", sep='.')) G.train <- train$genotypes CHR <- train$map$chromosome POS <- train$map$physical.pos NCORES <- nb_cores() lpval <- -log10(sumstats$pval) y.train <- train$fam$affection # Test data # snp_readBed(paste(file_test, \"bed\", sep='.')) test <- snp_attach(paste(file_test, \"rds\", sep='.')) G.test <- test$genotypes #","title":"Dataset"},{"location":"Scientific Journey/Report/report/#ct-step","text":"Generating a matrix of C+T with varied clumping radius and p-value thresholds. \\beta s and p-values are aquired in sumstats file (see sumstats.txt ) all_keep <- snp_grid_clumping(G.train, CHR, POS, lpval, ncores = NCORES) PRS <-snp_grid_PRS(G.train,all_keep = all_keep, betas = sumstats$beta,lpval) PRS_test <- snp_grid_PRS(G.test, all_keep = all_keep, betas = sumstats$beta,lpval) Other than SCT, other methods require DataFrame or R Matrix than FBM to manage data. I converted the FBM output of C+T to DataFrame and change the corresponding columns.","title":"C+T step"},{"location":"Scientific Journey/Report/report/#data-treatment-oversampling","text":"To oversampling I extract all positive ( affection = 1 ), and append them a number of times. For this simulated dataset, I enhanced the number of cases 10 times, thus the ratio become ~70:30 (from 20:80). y_case <- which(train$fam$affection %in% c(1)) for (i in seq(1, 10)){ y.train <- append(y.train, train$fam$affection[y_case]) ds <- rbind(ds, PRS[y_case,]) } ds.FBM <- as_FBM(ds)","title":"Data treatment: Oversampling"},{"location":"Scientific Journey/Report/report/#method-1-sct","text":"No parameters required. Combining (Sparse Logistic Regression) C+T predictors for best result. # Reference # M <- snp_grid_stacking(multi_PRS = ds.FBM, y.train = y.train, ncores = NCORES) M <- snp_grid_stacking(multi_PRS = PRS, y.train = train$fam$affection) beta <- as_FBM(matrix(M$beta.G)) pred.SCT <- big_prodMat(G.test, beta) #AUC(pred = pred.SCT, test$fam$affection) AUC(pred = pred.SCT, test$fam$affection) ## [1] 0.7816448","title":"Method 1: SCT"},{"location":"Scientific Journey/Report/report/#method-2-xgboost","text":"XGBoost provide slightly better result compare to SCT. However this result heavily depend on booster and objective selection. Experiments shown that gblinear booster with count:poisson objective give the best results. For experimentation, I run 9 different max_depth and 9 nrounds . Each combination is repeated 10 times and the mean value calculated to rule out the randomess of gblinear booster AUCs <- matrix(,nrow = 10, ncol = 10) for (i in seq(2 : 10)){ for (j in seq(2: 10)) { temp <- c() for(t in seq(1:10)){ bstSparse <- xgboost(data = ds[], label = y.train, booster=\"gblinear\", max_depth = i, eta = 1, nthread = 2, nrounds = j, lambda = 0.1, objective = \"count:poisson\" ) pred <- predict(bstSparse, PRS_test[]) temp <- append(temp,AUC(pred = pred, test$fam$affection)) } AUCs[i,j] <- mean(temp) } } AUCs ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.7810081 0.7814691 0.7819848 0.7820644 0.7821634 0.7819551 ## [2,] 0.7815161 0.7816789 0.7819097 0.7820451 0.7820130 0.7820510 ## [3,] 0.7811385 0.7817903 0.7820209 0.7821792 0.7821172 0.7820448 ## [4,] 0.7812592 0.7814580 0.7820362 0.7819456 0.7820135 0.7819257 ## [5,] 0.7811024 0.7814672 0.7818094 0.7819867 0.7820772 0.7820431 ## [6,] 0.7813253 0.7816065 0.7818425 0.7822446 0.7821884 0.7819205 ## [7,] 0.7810239 0.7817146 0.7817587 0.7819281 0.7822180 0.7819721 ## [8,] 0.7811480 0.7820816 0.7818343 0.7820383 0.7820944 0.7819818 ## [9,] 0.7815687 0.7814758 0.7816728 0.7818929 0.7820611 0.7820187 ## [10,] NA NA NA NA NA NA ## [,7] [,8] [,9] [,10] ## [1,] 0.7817276 0.7816945 0.7814971 NA ## [2,] 0.7819033 0.7817853 0.7816005 NA ## [3,] 0.7818634 0.7816886 0.7814594 NA ## [4,] 0.7819806 0.7818078 0.7813999 NA ## [5,] 0.7818562 0.7816829 0.7814363 NA ## [6,] 0.7819113 0.7817263 0.7814301 NA ## [7,] 0.7818913 0.7817029 0.7815198 NA ## [8,] 0.7819087 0.7816597 0.7814085 NA ## [9,] 0.7817407 0.7817544 0.7815585 NA ## [10,] NA NA NA NA boxplot(AUCs)","title":"Method 2: XGBoost"},{"location":"Scientific Journey/Report/report/#method-3-lassosum","text":"Lassosum use L1 regularization to better fit the regression model. For basic L1 regularization of Linear Regression, 2 metaparameters are required: a learning rate \\alpha and a regularization parameter \\lambda . However, lassosum does not require these parameters, as the software automatically scan the parameters space and select the best \\alpha and \\lambda setwd(directory) cor <- p2cor(p = sumstats$pval, n = 8000, sign=sumstats$beta) out <- lassosum.pipeline(cor =cor, chr=sumstats$chromosome, pos = sumstats$physical.pos, A1 = sumstats$allele1, A2 = sumstats$allele2, ref.bfile = file_train, test.bfile = file_test, LDblocks = LDblocks) v <- validate(out) out2 <- subset(out, s=v$best.s, lambda = v$lambda) v2 <- validate(out2) v2$best.validation.result AUC(v$best.pgs, v$pheno) ## [1] 0.7308175","title":"Method 3: Lassosum"},{"location":"Scientific Journey/Report/report/#method-4-random-forest","text":"Random forest also give good AUC (above 70%, but less than other methods except for keras ). The result highly dependen on the number of trees (more trees equals more convergence), and the learning rate also contribute to an lesser extend. The code in this session will sweep through the parameters: 1 to 10 trees and alpha from 0.1 to 1 (0.1 increment). PRS_test.df <- as.data.frame(PRS_test[]) f <- as.formula(paste(\"y ~\", paste(cols[!cols %in% \"PRS.df\"], collapse = \" + \"))) for (j in seq(1 : 10)){ for (i in seq(1: 10)) { rf <- ranger(f, data = PRS.df, num.trees = j*100, verbose = TRUE, write.forest = TRUE, alpha = (i/10)) pred <- predict(rf, PRS_test.df) AUCs[i,j] <- AUC(pred = pred$predictions, test$fam$affection) } } AUCs ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.7532205 0.7535048 0.7564268 0.7501722 0.7507132 0.7515333 ## [2,] 0.7440113 0.7530198 0.7540833 0.7532506 0.7518778 0.7546351 ## [3,] 0.7545732 0.7500895 0.7514272 0.7556325 0.7545590 0.7539930 ## [4,] 0.7549227 0.7515300 0.7500226 0.7548174 0.7503537 0.7543692 ## [5,] 0.7488554 0.7515392 0.7508904 0.7535265 0.7544863 0.7536828 ## [6,] 0.7501346 0.7512984 0.7525976 0.7547204 0.7520592 0.7536302 ## [7,] 0.7496137 0.7509414 0.7527130 0.7549386 0.7512415 0.7553466 ## [8,] 0.7435724 0.7513862 0.7545431 0.7513971 0.7525416 0.7559703 ## [9,] 0.7513695 0.7512825 0.7537756 0.7543274 0.7507516 0.7562646 ## [10,] 0.7473798 0.7476314 0.7535859 0.7539036 0.7543751 0.7537062 ## [,7] [,8] [,9] [,10] ## [1,] 0.7541560 0.7521771 0.7544930 0.7532481 ## [2,] 0.7547923 0.7526261 0.7558700 0.7539353 ## [3,] 0.7526620 0.7529212 0.7531896 0.7540800 ## [4,] 0.7532573 0.7556534 0.7528978 0.7538868 ## [5,] 0.7518979 0.7547931 0.7558106 0.7535190 ## [6,] 0.7544119 0.7547555 0.7558792 0.7548132 ## [7,] 0.7524614 0.7543283 0.7537857 0.7549896 ## [8,] 0.7533702 0.7535374 0.7533777 0.7520208 ## [9,] 0.7569953 0.7558482 0.7558123 0.7530600 ## [10,] 0.7538342 0.7547547 0.7521612 0.7539888 boxplot(AUCs)","title":"Method 4: Random Forest"},{"location":"Scientific Journey/Report/report/#method-5-nerual-net-keras","text":"For some reason, keras cannot return good result; the prediction always biased to controls. Even with adjusted weights and oversampling, the bias is still there and AUC is always 0.5 . I included it here in case I figure it out in the future. In this example, I put a 10:1 weight ratio between case and control. y.train <- train$fam$affection model <- keras_model_sequential() model %>% layer_dense(units = 1000, activation = \"softmax\", input_shape = c(ncol(ds)), kernel_regularizer=regularizer_l1(0.02)) %>% # layer_dense(units = 1000, activation = \"softmax\", input_shape = c(2800)) %>% # layer_dropout(rate=0.1) %>% layer_dense(units = 2, activation = \"softmax\", kernel_regularizer=regularizer_l1(0.01)) model %>% compile( #loss = 'categorical_crossentropy', loss = 'mean_absolute_percentage_error', optimizer = 'SGD', metrics = c('accuracy') ) y.train <- to_categorical(y.train) history <- model$fit( ds, y.train, class_weight = list(1, 10), epochs = as.integer(10), batch_size = as.integer(28) ) AUC(predict_classes(model, PRS_test[]), test$fam$affection) ## [1] 0.5 Original Rmd documment provided here \u21a9","title":"Method 5: Nerual net (Keras)"},{"location":"Template/Cover-Letter/","text":"L\u00e2m (V\u0169) \u0110\u1eb7ng 133/32/18 Xu\u00e2n Th\u1ee7y, C\u1ea7u Gi\u1ea5y, H\u00e0 N\u1ed9i","title":"L\u00e2m (V\u0169) \u0110\u1eb7ng"},{"location":"Template/Cover-Letter/#lam-vu-ang","text":"","title":"L\u00e2m (V\u0169) \u0110\u1eb7ng"},{"location":"Template/Cover-Letter/#1333218-xuan-thuy-cau-giay-ha-noi","text":"","title":"133/32/18 Xu\u00e2n Th\u1ee7y, C\u1ea7u Gi\u1ea5y, H\u00e0 N\u1ed9i"},{"location":"Thought/FabLab-recruit/","text":"FabLab USTH Recruit Pipette 2 Thuy\u1ebft tr\u00ecnh ch\u01b0a t\u1ed1t. L\u00e0m kh\u00e1 t\u1ed1t, ch\u1ee7 \u0111\u1ed9ng. Ng\u1ecdc, Ph\u01b0\u1ee3ng, Linh c\u00f3 ch\u1ee7 \u0111\u1ed9ng h\u01a1n c\u00e1c b\u1ea1n kh\u00e1c. C\u00f3 c\u1ed1 g\u1eafng thi\u1ebft k\u1ebf th\u00eam. Ch\u01b0a ho\u00e0n thi\u1ec7n nh\u01b0ng c\u00f3 \u00fd t\u01b0\u1edfng. Pipette 1 Slide kh\u00e1 t\u1ed1t nh\u01b0ng c\u00f2n l\u1ed7i Cansat 2 Kh\u00f4ng s\u1ed1ng s\u00f3t Th\u1eddi gian ng\u1eafn, ti\u1ebft ki\u1ec7m Slide t\u1ed1t Cansat 1 Qu\u1ea3n l\u00fd th\u1eddi gian ch\u01b0a t\u1ed1t Slide kh\u00e1 t\u1ed1t, \u0111\u1eb9p nh\u01b0ng nhi\u1ec1u ch\u1eef Hi\u1ec1n Minh, Kh\u00e1nh... ch\u1ecbu kh\u00f3 l\u00e0m (b\u1ea1n l\u00e0m \u0111i\u1ec7n) thuy\u1ebft tr\u00ecnh ch\u01b0a t\u1ed1t Ch\u01b0a truy\u1ec1n \u0111\u01b0\u1ee3c t\u00edn hi\u1ec7u. Thi\u1ebft k\u1ebf t\u1ed1t, kh\u00f4ng th\u1eeba nhi\u1ec1u C\u00f3 t\u00ednh to\u00e1n Xe Mua kit M\u00e1y bay","title":"FabLab USTH Recruit"},{"location":"Thought/FabLab-recruit/#fablab-usth-recruit","text":"","title":"FabLab USTH Recruit"},{"location":"Thought/FabLab-recruit/#pipette-2","text":"Thuy\u1ebft tr\u00ecnh ch\u01b0a t\u1ed1t. L\u00e0m kh\u00e1 t\u1ed1t, ch\u1ee7 \u0111\u1ed9ng. Ng\u1ecdc, Ph\u01b0\u1ee3ng, Linh c\u00f3 ch\u1ee7 \u0111\u1ed9ng h\u01a1n c\u00e1c b\u1ea1n kh\u00e1c. C\u00f3 c\u1ed1 g\u1eafng thi\u1ebft k\u1ebf th\u00eam. Ch\u01b0a ho\u00e0n thi\u1ec7n nh\u01b0ng c\u00f3 \u00fd t\u01b0\u1edfng.","title":"Pipette 2"},{"location":"Thought/FabLab-recruit/#pipette-1","text":"Slide kh\u00e1 t\u1ed1t nh\u01b0ng c\u00f2n l\u1ed7i","title":"Pipette 1"},{"location":"Thought/FabLab-recruit/#cansat-2","text":"Kh\u00f4ng s\u1ed1ng s\u00f3t Th\u1eddi gian ng\u1eafn, ti\u1ebft ki\u1ec7m Slide t\u1ed1t","title":"Cansat 2"},{"location":"Thought/FabLab-recruit/#cansat-1","text":"Qu\u1ea3n l\u00fd th\u1eddi gian ch\u01b0a t\u1ed1t Slide kh\u00e1 t\u1ed1t, \u0111\u1eb9p nh\u01b0ng nhi\u1ec1u ch\u1eef Hi\u1ec1n Minh, Kh\u00e1nh... ch\u1ecbu kh\u00f3 l\u00e0m (b\u1ea1n l\u00e0m \u0111i\u1ec7n) thuy\u1ebft tr\u00ecnh ch\u01b0a t\u1ed1t Ch\u01b0a truy\u1ec1n \u0111\u01b0\u1ee3c t\u00edn hi\u1ec7u. Thi\u1ebft k\u1ebf t\u1ed1t, kh\u00f4ng th\u1eeba nhi\u1ec1u C\u00f3 t\u00ednh to\u00e1n","title":"Cansat 1"},{"location":"Thought/FabLab-recruit/#xe","text":"Mua kit","title":"Xe"},{"location":"Thought/FabLab-recruit/#may-bay","text":"","title":"M\u00e1y bay"},{"location":"Thought/Uncle/","text":"Two Snooty Uncles Eating to the Beat A Short Story by Jane Doe Rick McCallister had always loved cosy Sidney with its robust, raspy rivers. It was a place where he felt unstable. He was a smart, tight-fisted, squash drinker with curvaceous moles and beautiful hands. His friends saw him as an envious, embarrassed elephant. Once, he had even helped an uptight blind person cross the road. That's the sort of man he was. Rick walked over to the window and reflected on his cold surroundings. The sleet rained like talking horses. Then he saw something in the distance, or rather someone. It was the figure of Helen Thornhill. Helen was a noble friend with ginger moles and slimy hands. Rick gulped. He was not prepared for Helen. As Rick stepped outside and Helen came closer, he could see the vacant glint in her eye. \"I am here because I want a pencil,\" Helen bellowed, in a mean tone. She slammed her fist against Rick's chest, with the force of 3526 hamsters. \"I frigging love you, Rick McCallister.\" Rick looked back, even more happy and still fingering the bendy piano. \"Helen, let's move in together,\" he replied. They looked at each other with jumpy feelings, like two knowing, kindhearted koalas drinking at a very down to earth funeral, which had classical music playing in the background and two snooty uncles eating to the beat. Rick regarded Helen's ginger moles and slimy hands. He held out his hand. \"Let's not fight,\" he whispered, gently. \"Hmph,\" pondered Helen. \"Please?\" begged Rick with puppy dog eyes. Helen looked sleepy, her body blushing like a gentle, giant gun. Then Helen came inside for a nice beaker of squash. THE END. Auto Praise for Two Snooty Uncles Eating to the Beat \"I feel like I know Rick McCallister. In a way, it feels as though I've always known him.\" - The Daily Tale \"About as enjoyable as being hailed on whilst taking in washing that has been targeted by seagulls with the squits.\" - Enid Kibbler \"Saying the sleet rained like talking horses is just the kind of literary device that makes this brilliant.\" - Hit the Spoof \"I could do better.\" - Zob Gloop","title":"Two Snooty Uncles Eating to the Beat"},{"location":"Thought/Uncle/#two-snooty-uncles-eating-to-the-beat","text":"A Short Story by Jane Doe Rick McCallister had always loved cosy Sidney with its robust, raspy rivers. It was a place where he felt unstable. He was a smart, tight-fisted, squash drinker with curvaceous moles and beautiful hands. His friends saw him as an envious, embarrassed elephant. Once, he had even helped an uptight blind person cross the road. That's the sort of man he was. Rick walked over to the window and reflected on his cold surroundings. The sleet rained like talking horses. Then he saw something in the distance, or rather someone. It was the figure of Helen Thornhill. Helen was a noble friend with ginger moles and slimy hands. Rick gulped. He was not prepared for Helen. As Rick stepped outside and Helen came closer, he could see the vacant glint in her eye. \"I am here because I want a pencil,\" Helen bellowed, in a mean tone. She slammed her fist against Rick's chest, with the force of 3526 hamsters. \"I frigging love you, Rick McCallister.\" Rick looked back, even more happy and still fingering the bendy piano. \"Helen, let's move in together,\" he replied. They looked at each other with jumpy feelings, like two knowing, kindhearted koalas drinking at a very down to earth funeral, which had classical music playing in the background and two snooty uncles eating to the beat. Rick regarded Helen's ginger moles and slimy hands. He held out his hand. \"Let's not fight,\" he whispered, gently. \"Hmph,\" pondered Helen. \"Please?\" begged Rick with puppy dog eyes. Helen looked sleepy, her body blushing like a gentle, giant gun. Then Helen came inside for a nice beaker of squash. THE END. Auto Praise for Two Snooty Uncles Eating to the Beat \"I feel like I know Rick McCallister. In a way, it feels as though I've always known him.\" - The Daily Tale \"About as enjoyable as being hailed on whilst taking in washing that has been targeted by seagulls with the squits.\" - Enid Kibbler \"Saying the sleet rained like talking horses is just the kind of literary device that makes this brilliant.\" - Hit the Spoof \"I could do better.\" - Zob Gloop","title":"Two Snooty Uncles Eating to the Beat"},{"location":"Thought/nov-28-17/","text":"Random note #1 Xin ng\u1eaft ch\u01b0\u01a1ng tr\u00ecnh Bioinformatics h\u00e0ng ng\u00e0y \u0111\u1ec3 mang \u0111\u1ebfn nh\u1eefng suy ngh\u0129 r\u1eddi r\u1ea1c (sau 2 chai bia) c\u1ee7a b\u1ea3n th\u00e2n. Khoa h\u1ecdc v\u00e0 tr\u00ed t\u00f2 m\u00f2 H\u1ecdc \u1edf USTH m\u1ed9t th\u1eddi gian m\u00ecnh nh\u1eadn ra r\u1eb1ng tr\u00ed t\u00f2 m\u00f2 c\u1ee7a sinh vi\u00ean th\u1eadt k\u00ec l\u1ea1. R\u1ea5t nhi\u1ec1u ng\u01b0\u1eddi b\u1ea1n c\u1ee7a m\u00ecnh \u0111\u1ebfn v\u1edbi USTH v\u1edbi m\u1ed9t tinh th\u1ea7n r\u1ea5t c\u1ea7u ti\u1ebfn v\u00e0 mong mu\u1ed1n h\u1ecdc h\u1ecfi r\u1ea5t m\u1ea1nh. M\u1ecdi ng\u01b0\u1eddi \u0111\u1ec1u r\u1ea5t gi\u1ecfi v\u00e0 h\u1ea5p th\u1ee5 r\u1ea5t nhi\u1ec1u ki\u1ebfn th\u1ee9c khoa h\u1ecdc v\u1edbi t\u1ed1c \u0111\u1ed9 l\u00e0m m\u00ecnh ph\u1ea3i ng\u1ea1c nhi\u00ean. Nh\u01b0ng \u0111i\u1ec1u n\u00e0y d\u1eebng l\u1ea1i \u1edf chuy\u00ean m\u00f4n h\u1eb9p c\u1ee7a c\u00e1c b\u1ea1n. Trong v\u00e0i n\u0103m h\u1ecdc \u1edf USTH, vi\u1ec7c ph\u1ed1i h\u1ee3p gi\u1eefa c\u00e1c ng\u00e0nh c\u00f3 li\u00ean quan r\u1ea5t kh\u00f3, v\u00e0 ch\u1ec9 c\u00f3 m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng \u00edt c\u00e1c b\u1ea1n h\u1ecdc c\u1ee7a m\u00ecnh tr\u1edf th\u00e0nh c\u00e1c chuy\u00ean gia \u0111a ng\u00e0nh. \u0110i\u1ec1u n\u00e0y c\u00f3 l\u1ebd b\u1eaft ngu\u1ed3n t\u1eeb m\u1ee5c ti\u00eau h\u1ecdc th\u00e0nh t\u00edch cao c\u1ee7a c\u00e1c sinh vi\u00ean \u0111\u1ec9nh cao, nh\u1eefng ng\u01b0\u1eddi c\u00f3 kh\u1ea3 n\u0103ng nh\u1ea5t \u0111\u1ec3 tr\u1edf th\u00e0nh c\u00e1c chuy\u00ean gia n\u00e0y (hay c\u00e1c nh\u00e0 b\u00e1c h\u1ecdc ha ha). \u0110\u00f4i khi m\u00ecnh ch\u01b0a nh\u1eadn th\u1ea5y s\u1ef1 ham th\u00edch ki\u1ebfn th\u1ee9c n\u00f3i chung c\u1ee7a anh em xung quanh. Khi h\u1ecdc m\u1ed9t ki\u1ebfn th\u1ee9c m\u1edbi m\u1ecdi ng\u01b0\u1eddi lu\u00f4n \u0111\u1eb7t m\u1ed9t c\u00e2u h\u1ecfi - c\u00e2u h\u1ecfi n\u00e0y hay xu\u1ea5t hi\u1ec7n trong c\u00e1c s\u00e1ch selfhelp, m\u00e0 theo m\u00ecnh l\u00e0 ph\u1ea3n t\u00e1c d\u1ee5ng kh\u1ee7ng khi\u1ebfp: \"H\u1ecdc c\u00e1i n\u00e0y \u0111\u1ec3 l\u00e0m g\u00ec\". Th\u1eadt ra m\u00ecnh l\u00e0 ai \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 c\u00e1c b\u1ea1n \u0111\u1ed3ng trang l\u1ee9a c\u1ee7a m\u00ecnh. Ch\u1ec9 l\u00e0... \u0111\u00f4i khi m\u00ecnh th\u1ea5y th\u1eadt c\u00f4 \u0111\u1ed9c tr\u00ean con \u0111\u01b0\u1eddng t\u00ecm ki\u1ebfm tri th\u1ee9c. May c\u0169ng c\u00f3 m\u1ed9t v\u00e0i ng\u01b0\u1eddi b\u1ea1n chia s\u1ebb, nh\u01b0ng ph\u1ea7n l\u1edbn m\u1ecdi ng\u01b0\u1eddi r\u01a1i v\u00e0o nh\u00f3m m\u00ecnh m\u00f4 t\u1ea3 \u1edf tr\u00ean. \u0110\u00f4i \u0111i\u1ec1u nh\u1ecf nhoi H\u00f4m nay m\u00ecnh c\u00f3 n\u00f3i 1 2 c\u00e2u v\u1ec1 c\u00e1ch \u0111\u1ec3 m\u00ecnh \u0111\u1ecdc. \u0110\u1ecdc m\u1ed9t b\u00e0i th\u01a1, \u0111\u1ecdc m\u1ed9t cu\u1ed1n s\u00e1ch hay \u0111\u1ecdc m\u1ed9t b\u00e0i b\u00e1o khoa h\u1ecdc, m\u00ecnh \u0111\u1ec1u c\u00f3 m\u1ed9t \"gi\u1ea3i thu\u1eadt\" chung. \u0110\u1ecdc 3 l\u1ea7n. Chi ti\u1ebft c\u00f3 th\u1ec3 xem \u1edf b\u00e0i n\u00e0y Gi\u1ea3i thu\u1eadt \u0111\u1ec3 \u0111\u1ecdc n\u00f3i chung \u0111\u01a1n gi\u1ea3n th\u1ebf n\u00e0y: L\u1ea7n \u0111\u1ea7u \u0111\u1ecdc cho bi\u1ebft L\u1ea7n hai \u0111\u1ecdc cho hi\u1ec3u L\u1ea7n ba \u0111\u1ecdc cho vui Th\u1ebf gi\u1edbi nh\u01b0 t\u00f4i th\u1ea5y M\u1ed9t s\u1ed1 ng\u01b0\u1eddi bi\u1ebft m\u00ecnh c\u00f3 th\u1ec3 s\u1ebd bi\u1ebft m\u00ecnh c\u00f3 ch\u1ee5p \u1ea3nh, cho vui. \u0110\u00f3 l\u00e0 m\u1ed9t trong s\u1ed1 nh\u1eefng th\u00fa vui gi\u1eef cho m\u00ecnh c\u00f3 m\u1ed9t tr\u1ea1ng th\u00e1i c\u00e2n b\u1eb1ng v\u00e0 t\u1ec9nh t\u00e1o trong cu\u1ed9c s\u1ed1ng. Nhi\u1ebfp \u1ea3nh v\u1edbi m\u00ecnh c\u0169ng l\u00e0 m\u1ed9t s\u1ef1 ph\u1ea3n \u00e1nh c\u00e1ch m\u00ecnh t\u00ecm hi\u1ec3u cu\u1ed9c s\u1ed1ng v\u00e0 khoa h\u1ecdc. M\u1ed9t c\u00e1ch nh\u00ecn m\u1edbi, c\u00e1ch nh\u00ecn kh\u00e1c v\u1ec1 cu\u1ed9c s\u1ed1ng, v\u1ec1 s\u1ef1 v\u1eadt xung quanh m\u00ecnh: \u0111\u01b0a th\u00eam m\u1ed9t l\u1edbp k\u00ednh v\u00e0o gi\u1eefa m\u00ecnh v\u00e0 s\u1ef1 v\u1eadt, nh\u00ecn m\u1ecdi th\u1ee9 t\u1eeb m\u1ed9t g\u00f3c nh\u00ecn kh\u00e1c. Ti\u00eau \u0111\u1ec1 ph\u1ea7n n\u00e0y l\u1ea5y t\u1eeb m\u1ed9t quy\u1ec3n s\u00e1ch c\u1ee7a Einstein Th\u1ebf gi\u1edbi nh\u01b0 t\u00f4i th\u1ea5y","title":"Random note #1"},{"location":"Thought/nov-28-17/#random-note-1","text":"Xin ng\u1eaft ch\u01b0\u01a1ng tr\u00ecnh Bioinformatics h\u00e0ng ng\u00e0y \u0111\u1ec3 mang \u0111\u1ebfn nh\u1eefng suy ngh\u0129 r\u1eddi r\u1ea1c (sau 2 chai bia) c\u1ee7a b\u1ea3n th\u00e2n.","title":"Random note #1"},{"location":"Thought/nov-28-17/#khoa-hoc-va-tri-to-mo","text":"H\u1ecdc \u1edf USTH m\u1ed9t th\u1eddi gian m\u00ecnh nh\u1eadn ra r\u1eb1ng tr\u00ed t\u00f2 m\u00f2 c\u1ee7a sinh vi\u00ean th\u1eadt k\u00ec l\u1ea1. R\u1ea5t nhi\u1ec1u ng\u01b0\u1eddi b\u1ea1n c\u1ee7a m\u00ecnh \u0111\u1ebfn v\u1edbi USTH v\u1edbi m\u1ed9t tinh th\u1ea7n r\u1ea5t c\u1ea7u ti\u1ebfn v\u00e0 mong mu\u1ed1n h\u1ecdc h\u1ecfi r\u1ea5t m\u1ea1nh. M\u1ecdi ng\u01b0\u1eddi \u0111\u1ec1u r\u1ea5t gi\u1ecfi v\u00e0 h\u1ea5p th\u1ee5 r\u1ea5t nhi\u1ec1u ki\u1ebfn th\u1ee9c khoa h\u1ecdc v\u1edbi t\u1ed1c \u0111\u1ed9 l\u00e0m m\u00ecnh ph\u1ea3i ng\u1ea1c nhi\u00ean. Nh\u01b0ng \u0111i\u1ec1u n\u00e0y d\u1eebng l\u1ea1i \u1edf chuy\u00ean m\u00f4n h\u1eb9p c\u1ee7a c\u00e1c b\u1ea1n. Trong v\u00e0i n\u0103m h\u1ecdc \u1edf USTH, vi\u1ec7c ph\u1ed1i h\u1ee3p gi\u1eefa c\u00e1c ng\u00e0nh c\u00f3 li\u00ean quan r\u1ea5t kh\u00f3, v\u00e0 ch\u1ec9 c\u00f3 m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng \u00edt c\u00e1c b\u1ea1n h\u1ecdc c\u1ee7a m\u00ecnh tr\u1edf th\u00e0nh c\u00e1c chuy\u00ean gia \u0111a ng\u00e0nh. \u0110i\u1ec1u n\u00e0y c\u00f3 l\u1ebd b\u1eaft ngu\u1ed3n t\u1eeb m\u1ee5c ti\u00eau h\u1ecdc th\u00e0nh t\u00edch cao c\u1ee7a c\u00e1c sinh vi\u00ean \u0111\u1ec9nh cao, nh\u1eefng ng\u01b0\u1eddi c\u00f3 kh\u1ea3 n\u0103ng nh\u1ea5t \u0111\u1ec3 tr\u1edf th\u00e0nh c\u00e1c chuy\u00ean gia n\u00e0y (hay c\u00e1c nh\u00e0 b\u00e1c h\u1ecdc ha ha). \u0110\u00f4i khi m\u00ecnh ch\u01b0a nh\u1eadn th\u1ea5y s\u1ef1 ham th\u00edch ki\u1ebfn th\u1ee9c n\u00f3i chung c\u1ee7a anh em xung quanh. Khi h\u1ecdc m\u1ed9t ki\u1ebfn th\u1ee9c m\u1edbi m\u1ecdi ng\u01b0\u1eddi lu\u00f4n \u0111\u1eb7t m\u1ed9t c\u00e2u h\u1ecfi - c\u00e2u h\u1ecfi n\u00e0y hay xu\u1ea5t hi\u1ec7n trong c\u00e1c s\u00e1ch selfhelp, m\u00e0 theo m\u00ecnh l\u00e0 ph\u1ea3n t\u00e1c d\u1ee5ng kh\u1ee7ng khi\u1ebfp: \"H\u1ecdc c\u00e1i n\u00e0y \u0111\u1ec3 l\u00e0m g\u00ec\". Th\u1eadt ra m\u00ecnh l\u00e0 ai \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 c\u00e1c b\u1ea1n \u0111\u1ed3ng trang l\u1ee9a c\u1ee7a m\u00ecnh. Ch\u1ec9 l\u00e0... \u0111\u00f4i khi m\u00ecnh th\u1ea5y th\u1eadt c\u00f4 \u0111\u1ed9c tr\u00ean con \u0111\u01b0\u1eddng t\u00ecm ki\u1ebfm tri th\u1ee9c. May c\u0169ng c\u00f3 m\u1ed9t v\u00e0i ng\u01b0\u1eddi b\u1ea1n chia s\u1ebb, nh\u01b0ng ph\u1ea7n l\u1edbn m\u1ecdi ng\u01b0\u1eddi r\u01a1i v\u00e0o nh\u00f3m m\u00ecnh m\u00f4 t\u1ea3 \u1edf tr\u00ean.","title":"Khoa h\u1ecdc v\u00e0 tr\u00ed t\u00f2 m\u00f2"},{"location":"Thought/nov-28-17/#oi-ieu-nho-nhoi","text":"H\u00f4m nay m\u00ecnh c\u00f3 n\u00f3i 1 2 c\u00e2u v\u1ec1 c\u00e1ch \u0111\u1ec3 m\u00ecnh \u0111\u1ecdc. \u0110\u1ecdc m\u1ed9t b\u00e0i th\u01a1, \u0111\u1ecdc m\u1ed9t cu\u1ed1n s\u00e1ch hay \u0111\u1ecdc m\u1ed9t b\u00e0i b\u00e1o khoa h\u1ecdc, m\u00ecnh \u0111\u1ec1u c\u00f3 m\u1ed9t \"gi\u1ea3i thu\u1eadt\" chung. \u0110\u1ecdc 3 l\u1ea7n. Chi ti\u1ebft c\u00f3 th\u1ec3 xem \u1edf b\u00e0i n\u00e0y Gi\u1ea3i thu\u1eadt \u0111\u1ec3 \u0111\u1ecdc n\u00f3i chung \u0111\u01a1n gi\u1ea3n th\u1ebf n\u00e0y: L\u1ea7n \u0111\u1ea7u \u0111\u1ecdc cho bi\u1ebft L\u1ea7n hai \u0111\u1ecdc cho hi\u1ec3u L\u1ea7n ba \u0111\u1ecdc cho vui","title":"\u0110\u00f4i \u0111i\u1ec1u nh\u1ecf nhoi"},{"location":"Thought/nov-28-17/#the-gioi-nhu-toi-thay","text":"M\u1ed9t s\u1ed1 ng\u01b0\u1eddi bi\u1ebft m\u00ecnh c\u00f3 th\u1ec3 s\u1ebd bi\u1ebft m\u00ecnh c\u00f3 ch\u1ee5p \u1ea3nh, cho vui. \u0110\u00f3 l\u00e0 m\u1ed9t trong s\u1ed1 nh\u1eefng th\u00fa vui gi\u1eef cho m\u00ecnh c\u00f3 m\u1ed9t tr\u1ea1ng th\u00e1i c\u00e2n b\u1eb1ng v\u00e0 t\u1ec9nh t\u00e1o trong cu\u1ed9c s\u1ed1ng. Nhi\u1ebfp \u1ea3nh v\u1edbi m\u00ecnh c\u0169ng l\u00e0 m\u1ed9t s\u1ef1 ph\u1ea3n \u00e1nh c\u00e1ch m\u00ecnh t\u00ecm hi\u1ec3u cu\u1ed9c s\u1ed1ng v\u00e0 khoa h\u1ecdc. M\u1ed9t c\u00e1ch nh\u00ecn m\u1edbi, c\u00e1ch nh\u00ecn kh\u00e1c v\u1ec1 cu\u1ed9c s\u1ed1ng, v\u1ec1 s\u1ef1 v\u1eadt xung quanh m\u00ecnh: \u0111\u01b0a th\u00eam m\u1ed9t l\u1edbp k\u00ednh v\u00e0o gi\u1eefa m\u00ecnh v\u00e0 s\u1ef1 v\u1eadt, nh\u00ecn m\u1ecdi th\u1ee9 t\u1eeb m\u1ed9t g\u00f3c nh\u00ecn kh\u00e1c. Ti\u00eau \u0111\u1ec1 ph\u1ea7n n\u00e0y l\u1ea5y t\u1eeb m\u1ed9t quy\u1ec3n s\u00e1ch c\u1ee7a Einstein Th\u1ebf gi\u1edbi nh\u01b0 t\u00f4i th\u1ea5y","title":"Th\u1ebf gi\u1edbi nh\u01b0 t\u00f4i th\u1ea5y"},{"location":"Thought/on-writing/","text":"V\u1ec1 chuy\u1ec7n vi\u1ebft l\u00e1ch Okay, v\u1eady l\u00e0 m\u00ecnh \u0111\u00e3 kh\u00f4ng th\u00e0nh c\u00f4ng (l\u1eafm) trong m\u1ee5c ti\u00eau vi\u1ebft b\u00e0i tr\u01b0\u1edbc ng\u00e0y sinh nh\u1eadt m\u00ecnh (th\u1eadt ra c\u0169ng vi\u1ebft nhi\u1ec1u h\u01a1n s\u1ed1 post l\u00ean \u0111\u00e2y). M\u1ed9t s\u1ed1 l\u00fd do v\u00e0 b\u00e0i h\u1ecdc l\u00e0: \u0110\u1ea7u ti\u00ean l\u00e0 do m\u00ecnh \u0111\u00e3 r\u1ea5t b\u1eadn r\u1ed9n trong th\u1eddi gian m\u1ed9t th\u00e1ng v\u1eeba r\u1ed3i. \u0110\u00e2y l\u00e0 m\u1ed9t l\u00fd do c\u0169 m\u00e8m v\u00e0 c\u1ed5 l\u1ed7 nh\u01b0ng r\u1ea5t th\u1eadt. Gi\u1eefa vi\u1ec7c nghi\u00ean c\u1ee9u t\u1ea1i ICTLab - IRD, nghi\u00ean c\u1ee9u t\u1ea1i FabLab USTH, qu\u1ea3n tr\u1ecb v\u00e0 h\u01b0\u1edbng d\u1eabn c\u00e1c em m\u1edbi v\u00e0o FabLab, h\u01b0\u1edbng d\u1eabn c\u00e1c b\u1ea1n Biologist, th\u1eddi gian d\u00e0nh cho b\u1ea3n th\u00e2n m\u00ecnh khoong c\u00f3 nhi\u1ec1u. Ph\u1ea7n l\u1edbn th\u1eddi gian ri\u00eang n\u00e0y m\u00ecnh d\u00e0nh cho b\u1ea1n g\u00e1i m\u00ecnh. Th\u00e0nh th\u1ef1c m\u00e0 n\u00f3i, \u0111\u1ed9ng l\u1ef1c \u0111\u1ec3 vi\u1ebft c\u1ee7a m\u00ecnh gi\u1ea3m s\u00fat trong th\u1eddi gian v\u1eeba qua. M\u1ed9t l\u00fd do l\u1edbn \u0111\u1ec3 m\u00ecnh b\u1eaft \u0111\u1ea7u vi\u1ebft l\u00e0 do s\u1ef1 m\u1ea5t c\u00e2n b\u1eb1ng v\u00e0 b\u00ed b\u00e1ch trong cu\u1ed9c s\u1ed1ng n\u1ed9i t\u00e2m d\u1eabn \u0111\u1ebfn nhu c\u1ea7u gi\u1ea3i t\u1ecfa ra b\u00ean ngo\u00e0i. N\u00f3i v\u1eady kh\u00f4ng c\u00f3 ngh\u0129a l\u00e0 k\u1ec3 l\u1ec3 hay kh\u00f4ng bu\u1ed3n ch\u00e1n l\u00e0 s\u1ebd kh\u00f4ng vi\u1ebft. Tuy v\u1eady gi\u1eefa m\u1ed9t l\u1ecbch c\u00f4ng vi\u1ec7c ch\u1eadt ch\u1ed9i v\u00e0 t\u00e2m tr\u00ed b\u1ecb chi\u1ebfm d\u1ee5ng (b\u1edfi l\u1ecbch c\u00f4ng vi\u1ec7c tr\u00ean), ng\u01b0\u1eddi ta c\u0169ng kh\u00f3 c\u00f3 th\u1ec3 ng\u1ed3i vi\u1ebft m\u1ed9t b\u00e0i ra h\u1ed3n. M\u1ee5c \u0111\u00edch ch\u00ednh c\u1ee7a trang n\u00e0y l\u00e0 n\u01a1i \u0111\u1ec3 chia s\u1ebb v\u1ec1 khoa h\u1ecdc k\u1ef9 thu\u1eadt - c\u00f9ng m\u1ed9t s\u1ed1 b\u00e0i vi\u1ebft ng\u1eafn. M\u00e0 th\u1eadt ra trong th\u1eddi gian v\u1eeba r\u1ed3i m\u00ecnh c\u0169ng kh\u00f4ng c\u00f3 nhi\u1ec1u \u0111i\u1ec1u \u0111\u1ec3 chia s\u1ebb l\u1eafm. D\u1ed1i v\u1edbi k\u1ef9 thu\u1eadt tin sinh, m\u00ecnh \u0111ang ph\u1ea3i v\u1eadt l\u1ed9n v\u1edbi vi\u1ec7c ph\u00e1t tri\u1ec3n c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng S4 cho R. \u0110i\u1ec1u n\u00e0y kh\u00e1 kh\u00f3 v\u1edbi m\u1ed9t ng\u01b0\u1eddi h\u1ecdc l\u1eadp tr\u00ecnh theo ki\u1ec3u formal nh\u01b0 m\u00ecnh (s\u1ebd c\u1ea7n c\u1ea3 m\u1ed9t b\u00e0i ri\u00eang \u0111\u1ec3 n\u00f3i v\u1ec1 qu\u00e1 tr\u00ecnh v\u1edbi S4). V\u1edbi c\u00e1c c\u00f4ng vi\u1ec7c \u1edf Fab Lab cu\u1ed1i c\u00f9ng m\u00ecnh \u0111\u00e3 l\u00e0m cho chi\u1ebfc m\u00e1y in nh\u1ecf ch\u1ea1y \u0111\u01b0\u1ee3c - th\u00eam m\u1ed9t m\u1eabu m\u00e1y in trong kho t\u00e0ng c\u1ee7a nh\u00f3m. Ti\u1ebfp theo s\u1ebd l\u00e0 m\u00e1y CNC v\u00e0 c\u00e1c c\u00f4ng ngh\u1ec7 h\u1ed7 tr\u1ee3 cho in 3D (farm, server etc.). C\u00f4ng cu\u1ed9c nghi\u00ean c\u1ee9u khoa h\u1ecdc c\u0169ng c\u00f2n \u0111ang ph\u1ea3i m\u00f2 m\u1eabm trong m\u1ed9t n\u1ed9i dung \u0111\u1ea7y m\u1edbi m\u1ebb v\u00e0 th\u1eed th\u00e1ch \u0111\u1ed1i v\u1edbi m\u00ecnh. C\u1ea7n ph\u1ea3i s\u1ee5c s\u1ea1o, \u0111\u00e0o b\u1edbi \u0111\u1ec3 t\u00ecm c\u00e1c kh\u00eda c\u1ea1nh hay v\u00e0 th\u1eed th\u00e1ch - c\u0169ng nh\u01b0 gi\u1ee5c s\u1ebfp \u0111\u1ec3 ti\u1ebfn \u0111\u1ebfn nh\u1eefng m\u1ea3ng th\u00fa v\u1ecb trong chuy\u00ean m\u00f4n. Hi v\u1ecdng s\u1ebd ra th\u00e0nh qu\u1ea3 tr\u01b0\u1edbc th\u00e1ng 3 - \u0111\u1ec3 c\u00f3 th\u1ec3 y\u00ean t\u00e2m b\u01b0\u1edbc ti\u1ebfp Tuy nh\u1eefng l\u00fd do tr\u00ean s\u1ebd kh\u00f4ng m\u1ea5t \u0111i, v\u00e0 nh\u1eefng d\u1ef1 \u0111\u1ecbnh s\u1eafp t\u1edbi ch\u1ec9 c\u00f3 nhi\u1ec1u t h\u00eam, m\u00ecnh v\u1eabn th\u1ef1c s\u1ef1 mu\u1ed1n c\u1ea7m b\u00fat (hay b\u00e0n ph\u00edm). C\u00f3 th\u1ec3 nhwunxg ng\u00e0y th\u00e1ng t\u1edbi s\u1ebd kh\u00f4ng vi\u1ebft \u0111\u01b0\u1ee3c nhi\u1ec1u, vi\u1ebft \u0111\u01b0\u1ee3c th\u01b0\u1eddng xuy\u00ean, nh\u01b0ng s\u1ebd l\u00e0 m\u1ed9t vi\u1ec7c m\u00ecnh duy tr\u00ec. \u0110\u00e2y l\u00e0 m\u1ed9t g\u00f3c ri\u00eang c\u1ee7a m\u00ecnh \u0111\u1ec3 chia s\u1ebb. Mu\u1ed1n \u0111\u01b0\u1ee3c nh\u01b0 v\u1eady m\u1ed9t s\u1ed1 \u0111i\u1ec1u sau m\u00ecnh s\u1ebd ph\u1ea3i \u00e1p d\u1ee5ng v\u1edbi b\u1ea3n th\u00e2n: Ng\u1eebng vi\u1ec7c \u0111\u1eb7t m\u1ee5c ti\u00eau, l\u1ecbch vi\u1ebft. \u0110i\u1ec1u n\u00e0y ch\u1ec9 c\u00f3 l\u1ee3i v\u1edbi m\u1ed9t s\u1ed1 vi\u1ec7c, m\u1ed9t s\u1ed1 ng\u01b0\u1eddi c\u1ee5 th\u1ec3. N\u00f3 s\u1ebd kh\u00f4ng hi\u1ec7u qu\u1ea3 n\u1ebfu vi\u1ec7c \u0111ang l\u00e0m kh\u00f4ng mang l\u1ea1i incentive ho\u1eb7c l\u00e0m m\u1ea5t h\u1ee9ng th\u00fa. Ch\u1ec9 c\u00f4ng vi\u1ec7c m\u1edbi n\u00ean c\u00f3 \u00e1p l\u1ef1c nh\u01b0 v\u1eady. Vi\u1ebft, tr\u01b0\u1edbc h\u1ebft, l\u00e0 vui. Vi\u1ebft nh\u00e1p tr\u01b0\u1edbc b\u1eb1ng tay, ra gi\u1ea5y. S\u1edf d\u0129 m\u00ecnh r\u1ea5t d\u1ec5 c\u1ee5t h\u1ee9ng (v\u00e0 c\u00f3 nhi\u1ec1u b\u00e0i vi\u1ebft m\u1ed9t n\u1eeda) l\u00e0 do \u0111ang c\u00f3 \u00fd t\u01b0\u1edfng, c\u00f3 m\u1ea1ch hay nh\u01b0ng l\u1ea1i kh\u00f4ng c\u00f3 s\u1eb5n m\u00e1y t\u00ednh, m\u00e0 v\u1ec1 sau l\u1ea1i qu\u00ean m\u1ea5t, r\u1ea5t kh\u00f3 ch\u1ecbu. Vi\u1ebft c\u0169ng s\u1ebd bu\u1ed9c l\u1eddi v\u0103n ph\u1ea3i c\u1ea9n th\u1eadn v\u00e0 trau chu\u1ed1t h\u01a1n. C\u1ea7n c\u00f3 c\u00e1c \u0111\u1ec3 c\u00f3 ph\u1ea3n h\u1ed3i v\u00e0 t\u01b0\u01a1ng t\u00e1c. C\u00f3 th\u1ec3 s\u1ebd repost l\u00ean Wordpress v\u00e0 Facebook. Vi\u1ebft m\u00e0 kh\u00f4ng \u0111\u01b0\u1ee3c \u0111\u1ecdc c\u0169ng c\u1ea3m th\u1ea5y thi\u1ebfu thi\u1ebfu. Ai \u0111\u00f3 c\u00f3 th\u1ec3 b\u1ea3o ta s\u1ed1ng \u1ea3o, nh\u01b0ng th\u1eadt ra trong th\u1ebf gi\u1edbi \u1ea3o c\u00e0ng c\u1ea7n m\u1edf to m\u1eaft c\u0103ng r\u1ed9ng tai \u0111\u1ec3 m\u00e0 nghe, m\u00e0 h\u1ecdc. Confirmation bias l\u00e0 c\u00f3 th\u1eadt.","title":"V\u1ec1 chuy\u1ec7n vi\u1ebft l\u00e1ch"},{"location":"Thought/on-writing/#ve-chuyen-viet-lach","text":"Okay, v\u1eady l\u00e0 m\u00ecnh \u0111\u00e3 kh\u00f4ng th\u00e0nh c\u00f4ng (l\u1eafm) trong m\u1ee5c ti\u00eau vi\u1ebft b\u00e0i tr\u01b0\u1edbc ng\u00e0y sinh nh\u1eadt m\u00ecnh (th\u1eadt ra c\u0169ng vi\u1ebft nhi\u1ec1u h\u01a1n s\u1ed1 post l\u00ean \u0111\u00e2y). M\u1ed9t s\u1ed1 l\u00fd do v\u00e0 b\u00e0i h\u1ecdc l\u00e0: \u0110\u1ea7u ti\u00ean l\u00e0 do m\u00ecnh \u0111\u00e3 r\u1ea5t b\u1eadn r\u1ed9n trong th\u1eddi gian m\u1ed9t th\u00e1ng v\u1eeba r\u1ed3i. \u0110\u00e2y l\u00e0 m\u1ed9t l\u00fd do c\u0169 m\u00e8m v\u00e0 c\u1ed5 l\u1ed7 nh\u01b0ng r\u1ea5t th\u1eadt. Gi\u1eefa vi\u1ec7c nghi\u00ean c\u1ee9u t\u1ea1i ICTLab - IRD, nghi\u00ean c\u1ee9u t\u1ea1i FabLab USTH, qu\u1ea3n tr\u1ecb v\u00e0 h\u01b0\u1edbng d\u1eabn c\u00e1c em m\u1edbi v\u00e0o FabLab, h\u01b0\u1edbng d\u1eabn c\u00e1c b\u1ea1n Biologist, th\u1eddi gian d\u00e0nh cho b\u1ea3n th\u00e2n m\u00ecnh khoong c\u00f3 nhi\u1ec1u. Ph\u1ea7n l\u1edbn th\u1eddi gian ri\u00eang n\u00e0y m\u00ecnh d\u00e0nh cho b\u1ea1n g\u00e1i m\u00ecnh. Th\u00e0nh th\u1ef1c m\u00e0 n\u00f3i, \u0111\u1ed9ng l\u1ef1c \u0111\u1ec3 vi\u1ebft c\u1ee7a m\u00ecnh gi\u1ea3m s\u00fat trong th\u1eddi gian v\u1eeba qua. M\u1ed9t l\u00fd do l\u1edbn \u0111\u1ec3 m\u00ecnh b\u1eaft \u0111\u1ea7u vi\u1ebft l\u00e0 do s\u1ef1 m\u1ea5t c\u00e2n b\u1eb1ng v\u00e0 b\u00ed b\u00e1ch trong cu\u1ed9c s\u1ed1ng n\u1ed9i t\u00e2m d\u1eabn \u0111\u1ebfn nhu c\u1ea7u gi\u1ea3i t\u1ecfa ra b\u00ean ngo\u00e0i. N\u00f3i v\u1eady kh\u00f4ng c\u00f3 ngh\u0129a l\u00e0 k\u1ec3 l\u1ec3 hay kh\u00f4ng bu\u1ed3n ch\u00e1n l\u00e0 s\u1ebd kh\u00f4ng vi\u1ebft. Tuy v\u1eady gi\u1eefa m\u1ed9t l\u1ecbch c\u00f4ng vi\u1ec7c ch\u1eadt ch\u1ed9i v\u00e0 t\u00e2m tr\u00ed b\u1ecb chi\u1ebfm d\u1ee5ng (b\u1edfi l\u1ecbch c\u00f4ng vi\u1ec7c tr\u00ean), ng\u01b0\u1eddi ta c\u0169ng kh\u00f3 c\u00f3 th\u1ec3 ng\u1ed3i vi\u1ebft m\u1ed9t b\u00e0i ra h\u1ed3n. M\u1ee5c \u0111\u00edch ch\u00ednh c\u1ee7a trang n\u00e0y l\u00e0 n\u01a1i \u0111\u1ec3 chia s\u1ebb v\u1ec1 khoa h\u1ecdc k\u1ef9 thu\u1eadt - c\u00f9ng m\u1ed9t s\u1ed1 b\u00e0i vi\u1ebft ng\u1eafn. M\u00e0 th\u1eadt ra trong th\u1eddi gian v\u1eeba r\u1ed3i m\u00ecnh c\u0169ng kh\u00f4ng c\u00f3 nhi\u1ec1u \u0111i\u1ec1u \u0111\u1ec3 chia s\u1ebb l\u1eafm. D\u1ed1i v\u1edbi k\u1ef9 thu\u1eadt tin sinh, m\u00ecnh \u0111ang ph\u1ea3i v\u1eadt l\u1ed9n v\u1edbi vi\u1ec7c ph\u00e1t tri\u1ec3n c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng S4 cho R. \u0110i\u1ec1u n\u00e0y kh\u00e1 kh\u00f3 v\u1edbi m\u1ed9t ng\u01b0\u1eddi h\u1ecdc l\u1eadp tr\u00ecnh theo ki\u1ec3u formal nh\u01b0 m\u00ecnh (s\u1ebd c\u1ea7n c\u1ea3 m\u1ed9t b\u00e0i ri\u00eang \u0111\u1ec3 n\u00f3i v\u1ec1 qu\u00e1 tr\u00ecnh v\u1edbi S4). V\u1edbi c\u00e1c c\u00f4ng vi\u1ec7c \u1edf Fab Lab cu\u1ed1i c\u00f9ng m\u00ecnh \u0111\u00e3 l\u00e0m cho chi\u1ebfc m\u00e1y in nh\u1ecf ch\u1ea1y \u0111\u01b0\u1ee3c - th\u00eam m\u1ed9t m\u1eabu m\u00e1y in trong kho t\u00e0ng c\u1ee7a nh\u00f3m. Ti\u1ebfp theo s\u1ebd l\u00e0 m\u00e1y CNC v\u00e0 c\u00e1c c\u00f4ng ngh\u1ec7 h\u1ed7 tr\u1ee3 cho in 3D (farm, server etc.). C\u00f4ng cu\u1ed9c nghi\u00ean c\u1ee9u khoa h\u1ecdc c\u0169ng c\u00f2n \u0111ang ph\u1ea3i m\u00f2 m\u1eabm trong m\u1ed9t n\u1ed9i dung \u0111\u1ea7y m\u1edbi m\u1ebb v\u00e0 th\u1eed th\u00e1ch \u0111\u1ed1i v\u1edbi m\u00ecnh. C\u1ea7n ph\u1ea3i s\u1ee5c s\u1ea1o, \u0111\u00e0o b\u1edbi \u0111\u1ec3 t\u00ecm c\u00e1c kh\u00eda c\u1ea1nh hay v\u00e0 th\u1eed th\u00e1ch - c\u0169ng nh\u01b0 gi\u1ee5c s\u1ebfp \u0111\u1ec3 ti\u1ebfn \u0111\u1ebfn nh\u1eefng m\u1ea3ng th\u00fa v\u1ecb trong chuy\u00ean m\u00f4n. Hi v\u1ecdng s\u1ebd ra th\u00e0nh qu\u1ea3 tr\u01b0\u1edbc th\u00e1ng 3 - \u0111\u1ec3 c\u00f3 th\u1ec3 y\u00ean t\u00e2m b\u01b0\u1edbc ti\u1ebfp Tuy nh\u1eefng l\u00fd do tr\u00ean s\u1ebd kh\u00f4ng m\u1ea5t \u0111i, v\u00e0 nh\u1eefng d\u1ef1 \u0111\u1ecbnh s\u1eafp t\u1edbi ch\u1ec9 c\u00f3 nhi\u1ec1u t h\u00eam, m\u00ecnh v\u1eabn th\u1ef1c s\u1ef1 mu\u1ed1n c\u1ea7m b\u00fat (hay b\u00e0n ph\u00edm). C\u00f3 th\u1ec3 nhwunxg ng\u00e0y th\u00e1ng t\u1edbi s\u1ebd kh\u00f4ng vi\u1ebft \u0111\u01b0\u1ee3c nhi\u1ec1u, vi\u1ebft \u0111\u01b0\u1ee3c th\u01b0\u1eddng xuy\u00ean, nh\u01b0ng s\u1ebd l\u00e0 m\u1ed9t vi\u1ec7c m\u00ecnh duy tr\u00ec. \u0110\u00e2y l\u00e0 m\u1ed9t g\u00f3c ri\u00eang c\u1ee7a m\u00ecnh \u0111\u1ec3 chia s\u1ebb. Mu\u1ed1n \u0111\u01b0\u1ee3c nh\u01b0 v\u1eady m\u1ed9t s\u1ed1 \u0111i\u1ec1u sau m\u00ecnh s\u1ebd ph\u1ea3i \u00e1p d\u1ee5ng v\u1edbi b\u1ea3n th\u00e2n: Ng\u1eebng vi\u1ec7c \u0111\u1eb7t m\u1ee5c ti\u00eau, l\u1ecbch vi\u1ebft. \u0110i\u1ec1u n\u00e0y ch\u1ec9 c\u00f3 l\u1ee3i v\u1edbi m\u1ed9t s\u1ed1 vi\u1ec7c, m\u1ed9t s\u1ed1 ng\u01b0\u1eddi c\u1ee5 th\u1ec3. N\u00f3 s\u1ebd kh\u00f4ng hi\u1ec7u qu\u1ea3 n\u1ebfu vi\u1ec7c \u0111ang l\u00e0m kh\u00f4ng mang l\u1ea1i incentive ho\u1eb7c l\u00e0m m\u1ea5t h\u1ee9ng th\u00fa. Ch\u1ec9 c\u00f4ng vi\u1ec7c m\u1edbi n\u00ean c\u00f3 \u00e1p l\u1ef1c nh\u01b0 v\u1eady. Vi\u1ebft, tr\u01b0\u1edbc h\u1ebft, l\u00e0 vui. Vi\u1ebft nh\u00e1p tr\u01b0\u1edbc b\u1eb1ng tay, ra gi\u1ea5y. S\u1edf d\u0129 m\u00ecnh r\u1ea5t d\u1ec5 c\u1ee5t h\u1ee9ng (v\u00e0 c\u00f3 nhi\u1ec1u b\u00e0i vi\u1ebft m\u1ed9t n\u1eeda) l\u00e0 do \u0111ang c\u00f3 \u00fd t\u01b0\u1edfng, c\u00f3 m\u1ea1ch hay nh\u01b0ng l\u1ea1i kh\u00f4ng c\u00f3 s\u1eb5n m\u00e1y t\u00ednh, m\u00e0 v\u1ec1 sau l\u1ea1i qu\u00ean m\u1ea5t, r\u1ea5t kh\u00f3 ch\u1ecbu. Vi\u1ebft c\u0169ng s\u1ebd bu\u1ed9c l\u1eddi v\u0103n ph\u1ea3i c\u1ea9n th\u1eadn v\u00e0 trau chu\u1ed1t h\u01a1n. C\u1ea7n c\u00f3 c\u00e1c \u0111\u1ec3 c\u00f3 ph\u1ea3n h\u1ed3i v\u00e0 t\u01b0\u01a1ng t\u00e1c. C\u00f3 th\u1ec3 s\u1ebd repost l\u00ean Wordpress v\u00e0 Facebook. Vi\u1ebft m\u00e0 kh\u00f4ng \u0111\u01b0\u1ee3c \u0111\u1ecdc c\u0169ng c\u1ea3m th\u1ea5y thi\u1ebfu thi\u1ebfu. Ai \u0111\u00f3 c\u00f3 th\u1ec3 b\u1ea3o ta s\u1ed1ng \u1ea3o, nh\u01b0ng th\u1eadt ra trong th\u1ebf gi\u1edbi \u1ea3o c\u00e0ng c\u1ea7n m\u1edf to m\u1eaft c\u0103ng r\u1ed9ng tai \u0111\u1ec3 m\u00e0 nghe, m\u00e0 h\u1ecdc. Confirmation bias l\u00e0 c\u00f3 th\u1eadt.","title":"V\u1ec1 chuy\u1ec7n vi\u1ebft l\u00e1ch"},{"location":"Thought/the-death-of-expertise-vi/","text":"C\u00e1i ch\u1ebft c\u1ee7a chuy\u00ean m\u00f4n Tom Nichols L\u1eddi ng\u01b0\u1eddi d\u1ecbch: \u0110\u00e2y l\u00e0 m\u1ed9t b\u00e0i vi\u1ebft kh\u00e1 hay c\u1ee7a t\u00e1c gi\u1ea3 Tom Nichols. Tuy n\u00f3 \u0111\u00e3 c\u0169 nh\u01b0ng th\u1eddi gian ch\u1ec9 ch\u1ee9ng minh c\u00e0ng l\u00fac n\u00f3 c\u00e0ng c\u00f3 ngh\u0129a. T\u00f4i (ho\u1eb7c \u00edt ra t\u00f4i t\u1ef1 nh\u1eadn) l\u00e0 m\u1ed9t chuy\u00ean vi\u00ean. Kh\u00f4ng ph\u1ea3i v\u1ec1 t\u1ea5t c\u1ea3 m\u1ecdi th\u1ee9, m\u00e0 l\u00e0 trong m\u1ed9t l\u0129nh v\u1ef1c h\u1eb9p c\u1ee7a ki\u1ebfn th\u1ee9c con ng\u01b0\u1eddi: Khoa h\u1ecdc X\u00e3 h\u1ed9i v\u00e0 ch\u00ednh s\u00e1ch c\u00f4ng. Khi t\u00f4i \u0111\u01b0a ra l\u1eadp lu\u1eadn v\u1ec1 m\u1ed9t ch\u1ee7 \u0111\u1ec1, t\u00f4i k\u1ef3 v\u1ecdng \u00fd ki\u1ebfn c\u1ee7a t\u00f4i s\u1ebd c\u00f3 tr\u1ecdng l\u01b0\u1ee3ng h\u01a1n nh\u1eefng ng\u01b0\u1eddi kh\u00f4ng c\u00f3 chuy\u00ean m\u00f4n. T\u00f4i kh\u00f4ng th\u1ec3 ng\u1edd \u0111\u01b0\u1ee3c r\u1eb1ng nh\u1eefng c\u00e2u tr\u00ean th\u1eadt scandal. H\u00f3a ra ch\u00fang l\u00e0 nh\u1eefng c\u00e2u n\u00f3i g\u00e2y ra ch\u1ec9 tr\u00edch. Ng\u00e0y nay, m\u1ecdi s\u1ef1 tuy\u00ean b\u1ed1 v\u1ec1 m\u1eb7t chuy\u00ean m\u00f4n \u0111\u1ec1 g\u00e2y ra m\u1ed9t c\u01a1n ch\u1ea5n \u0111\u1ed9ng d\u1eef d\u1ed9i t\u1eeb m\u1ed9t g\u00f3c c\u1ee7a coogn ch\u00fang Hoa K\u1ef3, nh\u1eefng ng\u01b0\u1eddi ngay l\u1eadp t\u1ee9c ch\u1ec9 tay r\u1eb1ng l\u1eadp lu\u1eadn n\u00e0y ho\u00e0n to\u00e0n ch\u1ec9 l\u00e0 ng\u1ee5y bi\u1ec7n , l\u00e0 d\u1ea5u hi\u1ec7u c\u1ee7a nh\u1eefng tay b\u00e0n gi\u1ea5y \u0111\u00e1ng khinh b\u1ec9, v\u00e0 l\u00e0 m\u1ed9t n\u1ed7 l\u1ef1c \u0111\u00e1ng x\u1ea5u h\u1ed5 \u0111\u1ec3 d\u00f9ng uy t\u00edn d\u1eadp t\u1eaft nh\u1eefng cu\u1ed9c tranh lu\u1eadn d\u00e2n ch\u1ee7. M\u1eb7c d\u00f9 n\u1ec1n d\u00e2n ch\u1ee7, nh\u01b0 t\u00f4i \u0111\u00e3 n\u00f3i trong m\u1ed9t ti\u1ec3u lu\u1eadn v\u1ec1 C.S. Lewis v\u00e0 v\u1ee5 vi\u1ec7c Snowden, th\u1ef1c ch\u1ea5t l\u00e0 m\u1ed9t h\u1ec7 th\u1ed1ng ch\u00ednh quy\u1ec1n, ch\u1ee9 kh\u00f4ng ph\u1ea3i l\u00e0 m\u1ed9t tr\u1ea1ng th\u00e1i c\u00f4ng b\u1eb1ng nh\u01b0 qu\u00fd v\u1ecb h\u00ecnh dung. \u0110i\u1ec1u \u0111\u00f3 c\u00f3 ngh\u0129a ch\u00fang ta t\u1eadn h\u01b0\u1edfng nh\u1eefng \u0111\u1eb7c quy\u1ec1n t\u01b0\u01a1ng \u0111\u01b0\u01a1ng tr\u01b0\u1edbc ch\u00ednh quy\u1ec1n v\u00e0 gi\u1eefa m\u1ed7i c\u00e1 nh\u00e2n. C\u00f3 quy\u1ec1n l\u1ef1c ngang h\u00e0ng, \u0111i\u1ec1u n\u00e0y kh\u00f4ng c\u00f3 ngh\u0129a hai ng\u01b0\u1eddi ngang h\u00e0ng v\u1ec1 t\u00e0i n\u0103ng, kh\u1ea3 n\u0103ng hay ki\u1ebfn th\u1ee9c. N\u00f3 ch\u1eafc ch\u1eafn kh\u00f4ng c\u00f3 ngh\u0129a \u00fd ki\u1ebfn c\u1ee7a m\u1ed7i c\u00e1 nh\u00e2n \u0111\u1ec1u c\u00f3 gi\u00e1 tr\u1ecb nh\u01b0 m\u1ecdi c\u00e1i nh\u00e2n kh\u00e1c. V\u1eady m\u00e0, \u0111\u00f3 l\u1ea1i l\u00e0 m\u1ed9t c\u00e1ch nh\u00ecn nh\u1eadn c\u1ee7a kh\u00e1 nhi\u1ec1u ng\u01b0\u1eddi, b\u1ea5t ch\u1ea5p s\u1ef1 v\u00f4 l\u00fd c\u1ee7a n\u00f3 Chuy\u1ec7n g\u00ec \u0111ang di\u1ec5n ra? T\u00f4i e r\u1eb1ng ch\u00fang ta \u0111ang ch\u1ee9ng ki\u1ebfn c\u00e1i ch\u1ebft c\u1ee7a chuy\u00ean m\u00f4n : S\u1ef1 s\u1ee5p \u0111\u1ed5 c\u1ee7a b\u1ea5t c\u1ee9 s\u1ef1 ph\u00e2n chia n\u00e0o gi\u1eefa nh\u1eefng ng\u01b0\u1eddi c\u00f3 chuy\u00ean m\u00f4n v\u00e0 qu\u1ea7n ch\u00fang, h\u1ecdc sinh v\u00e0 gi\u00e1o vi\u00ean, nh\u1eefng ng\u01b0\u1eddi th\u1eafc m\u1eafc v\u00e0 nh\u1eefng ng\u01b0\u1eddi hi\u1ec3u r\u00f5 - n\u00f3i m\u1ed9t c\u00e1ch kh\u00e1c, gi\u1eefa nh\u1eefng ng\u01b0\u1eddi \u0111\u1ea1t \u0111\u01b0\u1ee3c th\u00e0nh t\u1ef1u trong m\u1ed9t l\u0129nh v\u1ef1c c\u1ee5 th\u1ec3 v\u00e0 nh\u1eefng k\u1ebb b\u1ea5t \u0111\u1eafc ch\u00ed. S\u1ef1 s\u1ee5p \u0111\u1ed5 n\u00e0y kh\u00f4ng \u0111\u1ed3ng ngh\u0129a v\u1edbi s\u1ef1 s\u1ee5p \u0111\u1ed5 th\u1ef1c s\u1ef1 c\u1ee7a ki\u1ebfn th\u1ee9c chuy\u00ean ng\u00e0nh - nh\u1eefng hi\u1ec3u bi\u1ebft v\u1ec1 s\u1ef1 vi\u1ec7c, s\u1ef1 v\u1eadt c\u1ee5 th\u1ec3 \u0111\u1eb7t ra s\u1ef1 kh\u00e1c bi\u1ec7t gi\u1eefa nh\u1eefng c\u00e1 nh\u00e2n trong x\u00e3 h\u1ed9i. S\u1ebd lu\u00f4n lu\u00f4n c\u00f3 nh\u1eefng b\u00e1c s\u0129, lu\u1eadt s\u01b0, k\u1ef9 s\u01b0 v\u00e0 nh\u1eefng chuy\u00ean gia trong t\u1eebng l\u0129nh v\u1ef1c. \u0110i\u1ec1u t\u00f4i lo s\u1ee3 l\u00e0 s\u1ef1 b\u1ea5t c\u00f4ng nh\u1eadn v\u1ec1 m\u1eb7t chuy\u00ean m\u00f4n, th\u1ee9 s\u1ebd thanh \u0111\u1ed5i c\u00e1ch ch\u00fang ta s\u1ed1ng v\u00e0 suy ngh\u0129. \u0110\u00e2y l\u00e0 m\u1ed9t \u0111i\u1ec1u t\u1ed3i t\u1ec7. \u0110\u00fang, c\u00e1c chuy\u00ean gia c\u0169ng c\u00f3 th\u1ec3 c\u00f3 sai s\u00f3t, t\u1eeb s\u1ef1 c\u1ed1 thalidomide cho \u0111\u1ebfn v\u1ee5 n\u1ed5 t\u00e0u Challenger \u0111\u1ec1u nh\u1eafc nh\u1edf ch\u00fang ta v\u1ec1 s\u1ef1 th\u1eadt n\u00e0y. Tuy v\u1eady, c\u00e1c chuy\u00ean gia c\u00f3 t\u1ec9 l\u1ec7 \u0111\u00fang cao h\u01a1n nh\u1eefng ng\u01b0\u1eddi b\u00ecnh th\u01b0\u1eddng. B\u00e1c s\u0129, d\u00f9 h\u1ecd c\u00f3 th\u1ec3 c\u00f3 sai s\u00f3t, v\u1eabn ngon h\u01a1n nh\u1eefng b\u00e0 \u0111\u1ed3ng ch\u1eefa b\u1ec7nh b\u1eb1ng ngo\u1ea1i c\u1ea3m hay ru\u1ed9t g\u00e0 \u0111\u1eb7c ch\u1ebf b\u1edfi b\u00e0 c\u00f4 b\u00ean ngo\u1ea1i. T\u1eeb ch\u1ed1i quan \u0111i\u1ec3m chuy\u00ean m\u00f4n v\u00e0 thay th\u1ebf n\u00f3 b\u1eb1ng \u00fd t\u01b0\u1edfng m\u1ecdi \u00fd ki\u1ebfn \u0111\u1ec1u c\u00f3 tr\u1ecdng l\u01b0\u1ee3ng ngang nhau l\u00e0 m\u1ed9t \u0111i\u1ec1u ng\u1edb ng\u1ea9n. T\u1ec7 h\u01a1n n\u1eefa, n\u00f3 c\u00f2n nguy hi\u1ec3m. C\u00e1i ch\u1ebft c\u1ee7a chuy\u00ean m\u00f4n l\u00e0 s\u1ef1 ch\u1ed1i b\u1ecf kh\u00f4ng ch\u1ec9 v\u1edbi ki\u1ebfn th\u1ee9c, m\u00e0 c\u00f2n v\u1edbi con \u0111\u01b0\u1eddng ch\u00fang ta theo \u0111u\u1ed5i h\u1ecdc v\u1ea5n v\u00e0 t\u00ecm ra hi\u1ec3u bi\u1ebft - n\u1ec1n t\u1ea3ng c\u1ee7a n\u1ec1n v\u0103n minh Ph\u01b0\u01a1ng T\u00e2y. \u0110\u00fang, t\u00f4i n\u00f3i \u0111\u1ebfn n\u1ec1n v\u0103n minh Ph\u01b0\u01a1ng T\u00e2y : c\u00e1i n\u1ec1n v\u0103n minh ph\u1ee5 h\u1ec7, ph\u00e1t x\u00edt, ph\u00e2n bi\u1ec7t ch\u1ee7ng t\u1ed9c, nh\u1eefng k\u1ebb \u0111\u00e3 t\u1ea1o ra bom nguy\u00ean t\u1eed, xe Edsel, n\u01b0\u1edbc t\u0103ng l\u1ef1c, m\u1eafc ch\u1ee9ng b\u00e9o ph\u00ec nh\u01b0ng c\u0169ng h\u1ea1 c\u00e1nh m\u00e1y bay trong \u0111\u00eam t\u1ed1i v\u00e0 vi\u1ebft ra nh\u1eefng v\u0103n b\u1ea3n t\u1ea7m c\u1ee1 Hi\u1ebfn ch\u01b0\u01a1ng Li\u00ean H\u1ee3p Qu\u1ed1c.","title":"C\u00e1i ch\u1ebft c\u1ee7a chuy\u00ean m\u00f4n"},{"location":"Thought/the-death-of-expertise-vi/#cai-chet-cua-chuyen-mon","text":"Tom Nichols L\u1eddi ng\u01b0\u1eddi d\u1ecbch: \u0110\u00e2y l\u00e0 m\u1ed9t b\u00e0i vi\u1ebft kh\u00e1 hay c\u1ee7a t\u00e1c gi\u1ea3 Tom Nichols. Tuy n\u00f3 \u0111\u00e3 c\u0169 nh\u01b0ng th\u1eddi gian ch\u1ec9 ch\u1ee9ng minh c\u00e0ng l\u00fac n\u00f3 c\u00e0ng c\u00f3 ngh\u0129a. T\u00f4i (ho\u1eb7c \u00edt ra t\u00f4i t\u1ef1 nh\u1eadn) l\u00e0 m\u1ed9t chuy\u00ean vi\u00ean. Kh\u00f4ng ph\u1ea3i v\u1ec1 t\u1ea5t c\u1ea3 m\u1ecdi th\u1ee9, m\u00e0 l\u00e0 trong m\u1ed9t l\u0129nh v\u1ef1c h\u1eb9p c\u1ee7a ki\u1ebfn th\u1ee9c con ng\u01b0\u1eddi: Khoa h\u1ecdc X\u00e3 h\u1ed9i v\u00e0 ch\u00ednh s\u00e1ch c\u00f4ng. Khi t\u00f4i \u0111\u01b0a ra l\u1eadp lu\u1eadn v\u1ec1 m\u1ed9t ch\u1ee7 \u0111\u1ec1, t\u00f4i k\u1ef3 v\u1ecdng \u00fd ki\u1ebfn c\u1ee7a t\u00f4i s\u1ebd c\u00f3 tr\u1ecdng l\u01b0\u1ee3ng h\u01a1n nh\u1eefng ng\u01b0\u1eddi kh\u00f4ng c\u00f3 chuy\u00ean m\u00f4n. T\u00f4i kh\u00f4ng th\u1ec3 ng\u1edd \u0111\u01b0\u1ee3c r\u1eb1ng nh\u1eefng c\u00e2u tr\u00ean th\u1eadt scandal. H\u00f3a ra ch\u00fang l\u00e0 nh\u1eefng c\u00e2u n\u00f3i g\u00e2y ra ch\u1ec9 tr\u00edch. Ng\u00e0y nay, m\u1ecdi s\u1ef1 tuy\u00ean b\u1ed1 v\u1ec1 m\u1eb7t chuy\u00ean m\u00f4n \u0111\u1ec1 g\u00e2y ra m\u1ed9t c\u01a1n ch\u1ea5n \u0111\u1ed9ng d\u1eef d\u1ed9i t\u1eeb m\u1ed9t g\u00f3c c\u1ee7a coogn ch\u00fang Hoa K\u1ef3, nh\u1eefng ng\u01b0\u1eddi ngay l\u1eadp t\u1ee9c ch\u1ec9 tay r\u1eb1ng l\u1eadp lu\u1eadn n\u00e0y ho\u00e0n to\u00e0n ch\u1ec9 l\u00e0 ng\u1ee5y bi\u1ec7n , l\u00e0 d\u1ea5u hi\u1ec7u c\u1ee7a nh\u1eefng tay b\u00e0n gi\u1ea5y \u0111\u00e1ng khinh b\u1ec9, v\u00e0 l\u00e0 m\u1ed9t n\u1ed7 l\u1ef1c \u0111\u00e1ng x\u1ea5u h\u1ed5 \u0111\u1ec3 d\u00f9ng uy t\u00edn d\u1eadp t\u1eaft nh\u1eefng cu\u1ed9c tranh lu\u1eadn d\u00e2n ch\u1ee7. M\u1eb7c d\u00f9 n\u1ec1n d\u00e2n ch\u1ee7, nh\u01b0 t\u00f4i \u0111\u00e3 n\u00f3i trong m\u1ed9t ti\u1ec3u lu\u1eadn v\u1ec1 C.S. Lewis v\u00e0 v\u1ee5 vi\u1ec7c Snowden, th\u1ef1c ch\u1ea5t l\u00e0 m\u1ed9t h\u1ec7 th\u1ed1ng ch\u00ednh quy\u1ec1n, ch\u1ee9 kh\u00f4ng ph\u1ea3i l\u00e0 m\u1ed9t tr\u1ea1ng th\u00e1i c\u00f4ng b\u1eb1ng nh\u01b0 qu\u00fd v\u1ecb h\u00ecnh dung. \u0110i\u1ec1u \u0111\u00f3 c\u00f3 ngh\u0129a ch\u00fang ta t\u1eadn h\u01b0\u1edfng nh\u1eefng \u0111\u1eb7c quy\u1ec1n t\u01b0\u01a1ng \u0111\u01b0\u01a1ng tr\u01b0\u1edbc ch\u00ednh quy\u1ec1n v\u00e0 gi\u1eefa m\u1ed7i c\u00e1 nh\u00e2n. C\u00f3 quy\u1ec1n l\u1ef1c ngang h\u00e0ng, \u0111i\u1ec1u n\u00e0y kh\u00f4ng c\u00f3 ngh\u0129a hai ng\u01b0\u1eddi ngang h\u00e0ng v\u1ec1 t\u00e0i n\u0103ng, kh\u1ea3 n\u0103ng hay ki\u1ebfn th\u1ee9c. N\u00f3 ch\u1eafc ch\u1eafn kh\u00f4ng c\u00f3 ngh\u0129a \u00fd ki\u1ebfn c\u1ee7a m\u1ed7i c\u00e1 nh\u00e2n \u0111\u1ec1u c\u00f3 gi\u00e1 tr\u1ecb nh\u01b0 m\u1ecdi c\u00e1i nh\u00e2n kh\u00e1c. V\u1eady m\u00e0, \u0111\u00f3 l\u1ea1i l\u00e0 m\u1ed9t c\u00e1ch nh\u00ecn nh\u1eadn c\u1ee7a kh\u00e1 nhi\u1ec1u ng\u01b0\u1eddi, b\u1ea5t ch\u1ea5p s\u1ef1 v\u00f4 l\u00fd c\u1ee7a n\u00f3","title":"C\u00e1i ch\u1ebft c\u1ee7a chuy\u00ean m\u00f4n"},{"location":"Thought/the-death-of-expertise-vi/#chuyen-gi-ang-dien-ra","text":"T\u00f4i e r\u1eb1ng ch\u00fang ta \u0111ang ch\u1ee9ng ki\u1ebfn c\u00e1i ch\u1ebft c\u1ee7a chuy\u00ean m\u00f4n : S\u1ef1 s\u1ee5p \u0111\u1ed5 c\u1ee7a b\u1ea5t c\u1ee9 s\u1ef1 ph\u00e2n chia n\u00e0o gi\u1eefa nh\u1eefng ng\u01b0\u1eddi c\u00f3 chuy\u00ean m\u00f4n v\u00e0 qu\u1ea7n ch\u00fang, h\u1ecdc sinh v\u00e0 gi\u00e1o vi\u00ean, nh\u1eefng ng\u01b0\u1eddi th\u1eafc m\u1eafc v\u00e0 nh\u1eefng ng\u01b0\u1eddi hi\u1ec3u r\u00f5 - n\u00f3i m\u1ed9t c\u00e1ch kh\u00e1c, gi\u1eefa nh\u1eefng ng\u01b0\u1eddi \u0111\u1ea1t \u0111\u01b0\u1ee3c th\u00e0nh t\u1ef1u trong m\u1ed9t l\u0129nh v\u1ef1c c\u1ee5 th\u1ec3 v\u00e0 nh\u1eefng k\u1ebb b\u1ea5t \u0111\u1eafc ch\u00ed. S\u1ef1 s\u1ee5p \u0111\u1ed5 n\u00e0y kh\u00f4ng \u0111\u1ed3ng ngh\u0129a v\u1edbi s\u1ef1 s\u1ee5p \u0111\u1ed5 th\u1ef1c s\u1ef1 c\u1ee7a ki\u1ebfn th\u1ee9c chuy\u00ean ng\u00e0nh - nh\u1eefng hi\u1ec3u bi\u1ebft v\u1ec1 s\u1ef1 vi\u1ec7c, s\u1ef1 v\u1eadt c\u1ee5 th\u1ec3 \u0111\u1eb7t ra s\u1ef1 kh\u00e1c bi\u1ec7t gi\u1eefa nh\u1eefng c\u00e1 nh\u00e2n trong x\u00e3 h\u1ed9i. S\u1ebd lu\u00f4n lu\u00f4n c\u00f3 nh\u1eefng b\u00e1c s\u0129, lu\u1eadt s\u01b0, k\u1ef9 s\u01b0 v\u00e0 nh\u1eefng chuy\u00ean gia trong t\u1eebng l\u0129nh v\u1ef1c. \u0110i\u1ec1u t\u00f4i lo s\u1ee3 l\u00e0 s\u1ef1 b\u1ea5t c\u00f4ng nh\u1eadn v\u1ec1 m\u1eb7t chuy\u00ean m\u00f4n, th\u1ee9 s\u1ebd thanh \u0111\u1ed5i c\u00e1ch ch\u00fang ta s\u1ed1ng v\u00e0 suy ngh\u0129. \u0110\u00e2y l\u00e0 m\u1ed9t \u0111i\u1ec1u t\u1ed3i t\u1ec7. \u0110\u00fang, c\u00e1c chuy\u00ean gia c\u0169ng c\u00f3 th\u1ec3 c\u00f3 sai s\u00f3t, t\u1eeb s\u1ef1 c\u1ed1 thalidomide cho \u0111\u1ebfn v\u1ee5 n\u1ed5 t\u00e0u Challenger \u0111\u1ec1u nh\u1eafc nh\u1edf ch\u00fang ta v\u1ec1 s\u1ef1 th\u1eadt n\u00e0y. Tuy v\u1eady, c\u00e1c chuy\u00ean gia c\u00f3 t\u1ec9 l\u1ec7 \u0111\u00fang cao h\u01a1n nh\u1eefng ng\u01b0\u1eddi b\u00ecnh th\u01b0\u1eddng. B\u00e1c s\u0129, d\u00f9 h\u1ecd c\u00f3 th\u1ec3 c\u00f3 sai s\u00f3t, v\u1eabn ngon h\u01a1n nh\u1eefng b\u00e0 \u0111\u1ed3ng ch\u1eefa b\u1ec7nh b\u1eb1ng ngo\u1ea1i c\u1ea3m hay ru\u1ed9t g\u00e0 \u0111\u1eb7c ch\u1ebf b\u1edfi b\u00e0 c\u00f4 b\u00ean ngo\u1ea1i. T\u1eeb ch\u1ed1i quan \u0111i\u1ec3m chuy\u00ean m\u00f4n v\u00e0 thay th\u1ebf n\u00f3 b\u1eb1ng \u00fd t\u01b0\u1edfng m\u1ecdi \u00fd ki\u1ebfn \u0111\u1ec1u c\u00f3 tr\u1ecdng l\u01b0\u1ee3ng ngang nhau l\u00e0 m\u1ed9t \u0111i\u1ec1u ng\u1edb ng\u1ea9n. T\u1ec7 h\u01a1n n\u1eefa, n\u00f3 c\u00f2n nguy hi\u1ec3m. C\u00e1i ch\u1ebft c\u1ee7a chuy\u00ean m\u00f4n l\u00e0 s\u1ef1 ch\u1ed1i b\u1ecf kh\u00f4ng ch\u1ec9 v\u1edbi ki\u1ebfn th\u1ee9c, m\u00e0 c\u00f2n v\u1edbi con \u0111\u01b0\u1eddng ch\u00fang ta theo \u0111u\u1ed5i h\u1ecdc v\u1ea5n v\u00e0 t\u00ecm ra hi\u1ec3u bi\u1ebft - n\u1ec1n t\u1ea3ng c\u1ee7a n\u1ec1n v\u0103n minh Ph\u01b0\u01a1ng T\u00e2y. \u0110\u00fang, t\u00f4i n\u00f3i \u0111\u1ebfn n\u1ec1n v\u0103n minh Ph\u01b0\u01a1ng T\u00e2y : c\u00e1i n\u1ec1n v\u0103n minh ph\u1ee5 h\u1ec7, ph\u00e1t x\u00edt, ph\u00e2n bi\u1ec7t ch\u1ee7ng t\u1ed9c, nh\u1eefng k\u1ebb \u0111\u00e3 t\u1ea1o ra bom nguy\u00ean t\u1eed, xe Edsel, n\u01b0\u1edbc t\u0103ng l\u1ef1c, m\u1eafc ch\u1ee9ng b\u00e9o ph\u00ec nh\u01b0ng c\u0169ng h\u1ea1 c\u00e1nh m\u00e1y bay trong \u0111\u00eam t\u1ed1i v\u00e0 vi\u1ebft ra nh\u1eefng v\u0103n b\u1ea3n t\u1ea7m c\u1ee1 Hi\u1ebfn ch\u01b0\u01a1ng Li\u00ean H\u1ee3p Qu\u1ed1c.","title":"Chuy\u1ec7n g\u00ec \u0111ang di\u1ec5n ra?"},{"location":"Thought/the-death-of-expertise/","text":"The Death Of Expertise Tom Nichols I am (or at least think I am) an expert. Not on everything, but in a particular area of human knowledge, specifically social science and public policy. When I say something on those subjects, I expect that my opinion holds more weight than that of most other people. I never thought those were particularly controversial statements. As it turns out, they\u2019re plenty controversial. Today, any assertion of expertise produces an explosion of anger from certain quarters of the American public, who immediately complain that such claims are nothing more than fallacious appeals to authority , sure signs of dreadful elitism , and an obvious effort to use credentials to stifle the dialogue required by a real democracy. But democracy, as I wrote in an essay about C.S. Lewis and the Snowden affair, denotes a system of government, not an actual state of equality. It means that we enjoy equal rights versus the government, and in relation to each other. Having equal rights does not mean having equal talents, equal abilities, or equal knowledge. It assuredly does not mean that everyone\u2019s opinion about anything is as good as anyone else\u2019s. And yet, this is now enshrined as the credo of a fair number of people despite being obvious nonsense. What\u2019s going on here? I fear we are witnessing the death of expertise : a Google-fueled, Wikipedia-based, blog-sodden collapse of any division between professionals and laymen, students and teachers, knowers and wonderers \u2013 in other words, between those of any achievement in an area and those with none at all. By this, I do not mean the death of actual expertise, the knowledge of specific things that sets some people apart from others in various areas. There will always be doctors, lawyers, engineers, and other specialists in various fields. Rather, what I fear has died is any acknowledgement of expertise as anything that should alter our thoughts or change the way we live. What has died is any acknowledgement of expertise as anything that should alter our thoughts or change the way we live. This is a very bad thing. Yes, it\u2019s true that experts can make mistakes, as disasters from thalidomide to the Challenger explosion tragically remind us. But mostly, experts have a pretty good batting average compared to laymen: doctors, whatever their errors, seem to do better with most illnesses than faith healers or your Aunt Ginny and her special chicken gut poultice. To reject the notion of expertise, and to replace it with a sanctimonious insistence that every person has a right to his or her own opinion, is silly. Worse, it\u2019s dangerous. The death of expertise is a rejection not only of knowledge, but of the ways in which we gain knowledge and learn about things. Fundamentally, it\u2019s a rejection of science and rationality, which are the foundations of Western civilization itself. Yes, I said Western civilization : that paternalistic, racist, ethnocentric approach to knowledge that created the nuclear bomb, the Edsel, and New Coke, but which also keeps diabetics alive, lands mammoth airliners in the dark, and writes documents like the Charter of the United Nations. This isn\u2019t just about politics, which would be bad enough. No, it\u2019s worse than that: the perverse effect of the death of expertise is that without real experts, everyone is an expert on everything. To take but one horrifying example, we live today in an advanced post-industrial country that is now fighting a resurgence of whooping cough \u2014 a scourge nearly eliminated a century ago \u2014 merely because otherwise intelligent people have been second-guessing their doctors and refusing to vaccinate their kids after reading stuff written by people who know exactly zip about medicine. (Yes, I mean people like Jenny McCarthy ). In politics, too, the problem has reached ridiculous proportions. People in political debates no longer distinguish the phrase you\u2019re wrong from the phrase you\u2019re stupid. To disagree is to insult. To correct another is to be a hater. And to refuse to acknowledge alternative views, no matter how fantastic or inane, is to be closed-minded. How conversation became exhausting Critics might dismiss all this by saying that everyone has a right to participate in the public sphere. That\u2019s true. But every discussion must take place within limits and above a certain baseline of competence. And competence is sorely lacking in the public arena. People with strong views on going to war in other countries can barely find their own nation on a map; people who want to punish Congress for this or that law can\u2019t name their own member of the House. People with strong views on going to war in other countries can barely find their own nation on a map. None of this ignorance stops people from arguing as though they are research scientists. Tackle a complex policy issue with a layman today, and you will get snippy and sophistic demands to show ever increasing amounts of proof or evidence for your case, even though the ordinary interlocutor in such debates isn\u2019t really equipped to decide what constitutes evidence or to know it when it\u2019s presented. The use of evidence is a specialized form of knowledge that takes a long time to learn, which is why articles and books are subjected to peer review and not to everyone review, but don\u2019t tell that to someone hectoring you about the how things really work in Moscow or Beijing or Washington. This subverts any real hope of a conversation, because it is simply exhausting \u2014 at least speaking from my perspective as the policy expert in most of these discussions \u2014 to have to start from the very beginning of every argument and establish the merest baseline of knowledge, and then constantly to have to negotiate the rules of logical argument. (Most people I encounter, for example, have no idea what a non-sequitur is, or when they\u2019re using one; nor do they understand the difference between generalizations and stereotypes.) Most people are already huffy and offended before ever encountering the substance of the issue at hand. Once upon a time \u2014 way back in the Dark Ages before the 2000s \u2014 people seemed to understand, in a general way, the difference between experts and laymen. There was a clear demarcation in political food fights, as objections and dissent among experts came from their peers \u2014 that is, from people equipped with similar knowledge. The public, largely, were spectators. This was both good and bad. While it strained out the kook factor in discussions (editors controlled their letters pages, which today would be called moderating ), it also meant that sometimes public policy debate was too esoteric, conducted less for public enlightenment and more as just so much dueling jargon between experts. If experts go back to only talking to each other, that\u2019s bad for democracy. No one \u2014 not me, anyway \u2014 wants to return to those days. I like the 21st century, and I like the democratization of knowledge and the wider circle of public participation. That greater participation, however, is endangered by the utterly illogical insistence that every opinion should have equal weight, because people like me, sooner or later, are forced to tune out people who insist that we\u2019re all starting from intellectual scratch. (Spoiler: We\u2019re not.) And if that happens, experts will go back to only talking to each other. And that\u2019s bad for democracy. The downside of no gatekeepers How did this peevishness about expertise come about, and how can it have gotten so immensely foolish? Some of it is purely due to the globalization of communication. There are no longer any gatekeepers: the journals and op-ed pages that were once strictly edited have been drowned under the weight of self-publishable blogs. There was once a time when participation in public debate, even in the pages of the local newspaper, required submission of a letter or an article, and that submission had to be written intelligently, pass editorial review, and stand with the author\u2019s name attached. Even then, it was a big deal to get a letter in a major newspaper. Now, anyone can bum rush the comments section of any major publication. Sometimes, that results in a free-for-all that spurs better thinking. Most of the time, however, it means that anyone can post anything they want, under any anonymous cover, and never have to defend their views or get called out for being wrong. Another reason for the collapse of expertise lies not with the global commons but with the increasingly partisan nature of U.S. political campaigns. There was once a time when presidents would win elections and then scour universities and think-tanks for a brain trust; that\u2019s how Henry Kissinger, Samuel Huntington, Zbigniew Brzezinski and others ended up in government service while moving between places like Harvard and Columbia. This is the code of the samurai, not the intellectual, and it privileges the campaign loyalist over the expert. Those days are gone . To be sure, some of the blame rests with the increasing irrelevance of overly narrow research in the social sciences. But it is also because the primary requisite of seniority in the policy world is too often an answer to the question: What did you do during the campaign? This is the code of the samurai, not the intellectual, and it privileges the campaign loyalist over the expert. I have a hard time, for example, imagining that I would be called to Washington today in the way I was back in 1990, when the senior Senator from Pennsylvania asked a former U.S. Ambassador to the UN who she might recommend to advise him on foreign affairs, and she gave him my name. Despite the fact that I had no connection to Pennsylvania and had never worked on his campaigns, he called me at the campus where I was teaching, and later invited me to join his personal staff. Universities, without doubt, have to own some of this mess. The idea of telling students that professors run the show and know better than they do strikes many students as something like uppity lip from the help, and so many profs don\u2019t do it. (One of the greatest teachers I ever had, James Schall, once wrote many years ago that students have obligations to teachers, including trust, docility, effort, and thinking, an assertion that would produce howls of outrage from the entitled generations roaming campuses today.) As a result, many academic departments are boutiques, in which the professors are expected to be something like intellectual valets. This produces nothing but a delusion of intellectual adequacy in children who should be instructed, not catered to. The confidence of the dumb There\u2019s also that immutable problem known as human nature. It has a name now: it\u2019s called the Dunning-Kruger effect, which says, in sum, that the dumber you are, the more confident you are that you\u2019re not actually dumb. And when you get invested in being aggressively dumb\u2026well, the last thing you want to encounter are experts who disagree with you, and so you dismiss them in order to maintain your unreasonably high opinion of yourself. (There\u2019s a lot of that loose on social media, especially.) All of these are symptoms of the same disease: a manic reinterpretation of democracy in which everyone must have their say, and no one must be disrespected. (The verb to disrespect is one of the most obnoxious and insidious innovations in our language in years, because it really means to fail to pay me the impossibly high requirement of respect I demand. ) This yearning for respect and equality, even\u2014perhaps especially\u2014if unearned, is so intense that it brooks no disagreement. It represents the full flowering of a therapeutic culture where self-esteem, not achievement, is the ultimate human value, and it\u2019s making us all dumber by the day. Thus, at least some of the people who reject expertise are not really, as they often claim, showing their independence of thought. They are instead rejecting anything that might stir a gnawing insecurity that their own opinion might not be worth all that much. Experts: the servants, not masters, of a democracy So what can we do? Not much, sadly, since this is a cultural and generational issue that will take a long time come right, if it ever does. Personally, I don\u2019t think technocrats and intellectuals should rule the world: we had quite enough of that in the late 20th century, thank you, and it should be clear now that intellectualism makes for lousy policy without some sort of political common sense. Indeed, in an ideal world, experts are the servants, not the masters, of a democracy. But when citizens forgo their basic obligation to learn enough to actually govern themselves, and instead remain stubbornly imprisoned by their fragile egos and caged by their own sense of entitlement, experts will end up running things by default. That\u2019s a terrible outcome for everyone. Expertise is necessary, and it\u2019s not going away. Unless we return it to a healthy role in public policy, we\u2019re going to have stupider and less productive arguments every day. So here, presented without modesty or political sensitivity, are some things to think about when engaging with experts in their area of specialization. We can all stipulate: the expert isn\u2019t always right. But an expert is far more likely to be right than you are. On a question of factual interpretation or evaluation, it shouldn\u2019t engender insecurity or anxiety to think that an expert\u2019s view is likely to be better-informed than yours. (Because, likely, it is.) Experts come in many flavors. Education enables it, but practitioners in a field acquire expertise through experience; usually the combination of the two is the mark of a true expert in a field. But if you have neither education nor experience, you might want to consider exactly what it is you\u2019re bringing to the argument. In any discussion, you have a positive obligation to learn at least enough to make the conversation possible. The University of Google doesn\u2019t count. Remember: having a strong opinion about something isn\u2019t the same as knowing something. And yes, your political opinions have value. Of course they do: you\u2019re a member of a democracy and what you want is as important as what any other voter wants. As a layman, however, your political analysis, has far less value, and probably isn\u2019t \u2014 indeed, almost certainly isn\u2019t \u2014 as good as you think it is. And how do I know all this? Just who do I think I am? Well, of course: I\u2019m an expert.","title":"The Death of Expertise"},{"location":"Thought/the-death-of-expertise/#the-death-of-expertise","text":"Tom Nichols I am (or at least think I am) an expert. Not on everything, but in a particular area of human knowledge, specifically social science and public policy. When I say something on those subjects, I expect that my opinion holds more weight than that of most other people. I never thought those were particularly controversial statements. As it turns out, they\u2019re plenty controversial. Today, any assertion of expertise produces an explosion of anger from certain quarters of the American public, who immediately complain that such claims are nothing more than fallacious appeals to authority , sure signs of dreadful elitism , and an obvious effort to use credentials to stifle the dialogue required by a real democracy. But democracy, as I wrote in an essay about C.S. Lewis and the Snowden affair, denotes a system of government, not an actual state of equality. It means that we enjoy equal rights versus the government, and in relation to each other. Having equal rights does not mean having equal talents, equal abilities, or equal knowledge. It assuredly does not mean that everyone\u2019s opinion about anything is as good as anyone else\u2019s. And yet, this is now enshrined as the credo of a fair number of people despite being obvious nonsense.","title":"The Death Of Expertise"},{"location":"Thought/the-death-of-expertise/#whats-going-on-here","text":"I fear we are witnessing the death of expertise : a Google-fueled, Wikipedia-based, blog-sodden collapse of any division between professionals and laymen, students and teachers, knowers and wonderers \u2013 in other words, between those of any achievement in an area and those with none at all. By this, I do not mean the death of actual expertise, the knowledge of specific things that sets some people apart from others in various areas. There will always be doctors, lawyers, engineers, and other specialists in various fields. Rather, what I fear has died is any acknowledgement of expertise as anything that should alter our thoughts or change the way we live. What has died is any acknowledgement of expertise as anything that should alter our thoughts or change the way we live. This is a very bad thing. Yes, it\u2019s true that experts can make mistakes, as disasters from thalidomide to the Challenger explosion tragically remind us. But mostly, experts have a pretty good batting average compared to laymen: doctors, whatever their errors, seem to do better with most illnesses than faith healers or your Aunt Ginny and her special chicken gut poultice. To reject the notion of expertise, and to replace it with a sanctimonious insistence that every person has a right to his or her own opinion, is silly. Worse, it\u2019s dangerous. The death of expertise is a rejection not only of knowledge, but of the ways in which we gain knowledge and learn about things. Fundamentally, it\u2019s a rejection of science and rationality, which are the foundations of Western civilization itself. Yes, I said Western civilization : that paternalistic, racist, ethnocentric approach to knowledge that created the nuclear bomb, the Edsel, and New Coke, but which also keeps diabetics alive, lands mammoth airliners in the dark, and writes documents like the Charter of the United Nations. This isn\u2019t just about politics, which would be bad enough. No, it\u2019s worse than that: the perverse effect of the death of expertise is that without real experts, everyone is an expert on everything. To take but one horrifying example, we live today in an advanced post-industrial country that is now fighting a resurgence of whooping cough \u2014 a scourge nearly eliminated a century ago \u2014 merely because otherwise intelligent people have been second-guessing their doctors and refusing to vaccinate their kids after reading stuff written by people who know exactly zip about medicine. (Yes, I mean people like Jenny McCarthy ). In politics, too, the problem has reached ridiculous proportions. People in political debates no longer distinguish the phrase you\u2019re wrong from the phrase you\u2019re stupid. To disagree is to insult. To correct another is to be a hater. And to refuse to acknowledge alternative views, no matter how fantastic or inane, is to be closed-minded.","title":"What\u2019s going on here?"},{"location":"Thought/the-death-of-expertise/#how-conversation-became-exhausting","text":"Critics might dismiss all this by saying that everyone has a right to participate in the public sphere. That\u2019s true. But every discussion must take place within limits and above a certain baseline of competence. And competence is sorely lacking in the public arena. People with strong views on going to war in other countries can barely find their own nation on a map; people who want to punish Congress for this or that law can\u2019t name their own member of the House. People with strong views on going to war in other countries can barely find their own nation on a map. None of this ignorance stops people from arguing as though they are research scientists. Tackle a complex policy issue with a layman today, and you will get snippy and sophistic demands to show ever increasing amounts of proof or evidence for your case, even though the ordinary interlocutor in such debates isn\u2019t really equipped to decide what constitutes evidence or to know it when it\u2019s presented. The use of evidence is a specialized form of knowledge that takes a long time to learn, which is why articles and books are subjected to peer review and not to everyone review, but don\u2019t tell that to someone hectoring you about the how things really work in Moscow or Beijing or Washington. This subverts any real hope of a conversation, because it is simply exhausting \u2014 at least speaking from my perspective as the policy expert in most of these discussions \u2014 to have to start from the very beginning of every argument and establish the merest baseline of knowledge, and then constantly to have to negotiate the rules of logical argument. (Most people I encounter, for example, have no idea what a non-sequitur is, or when they\u2019re using one; nor do they understand the difference between generalizations and stereotypes.) Most people are already huffy and offended before ever encountering the substance of the issue at hand. Once upon a time \u2014 way back in the Dark Ages before the 2000s \u2014 people seemed to understand, in a general way, the difference between experts and laymen. There was a clear demarcation in political food fights, as objections and dissent among experts came from their peers \u2014 that is, from people equipped with similar knowledge. The public, largely, were spectators. This was both good and bad. While it strained out the kook factor in discussions (editors controlled their letters pages, which today would be called moderating ), it also meant that sometimes public policy debate was too esoteric, conducted less for public enlightenment and more as just so much dueling jargon between experts. If experts go back to only talking to each other, that\u2019s bad for democracy. No one \u2014 not me, anyway \u2014 wants to return to those days. I like the 21st century, and I like the democratization of knowledge and the wider circle of public participation. That greater participation, however, is endangered by the utterly illogical insistence that every opinion should have equal weight, because people like me, sooner or later, are forced to tune out people who insist that we\u2019re all starting from intellectual scratch. (Spoiler: We\u2019re not.) And if that happens, experts will go back to only talking to each other. And that\u2019s bad for democracy.","title":"How conversation became exhausting"},{"location":"Thought/the-death-of-expertise/#the-downside-of-no-gatekeepers","text":"How did this peevishness about expertise come about, and how can it have gotten so immensely foolish? Some of it is purely due to the globalization of communication. There are no longer any gatekeepers: the journals and op-ed pages that were once strictly edited have been drowned under the weight of self-publishable blogs. There was once a time when participation in public debate, even in the pages of the local newspaper, required submission of a letter or an article, and that submission had to be written intelligently, pass editorial review, and stand with the author\u2019s name attached. Even then, it was a big deal to get a letter in a major newspaper. Now, anyone can bum rush the comments section of any major publication. Sometimes, that results in a free-for-all that spurs better thinking. Most of the time, however, it means that anyone can post anything they want, under any anonymous cover, and never have to defend their views or get called out for being wrong. Another reason for the collapse of expertise lies not with the global commons but with the increasingly partisan nature of U.S. political campaigns. There was once a time when presidents would win elections and then scour universities and think-tanks for a brain trust; that\u2019s how Henry Kissinger, Samuel Huntington, Zbigniew Brzezinski and others ended up in government service while moving between places like Harvard and Columbia. This is the code of the samurai, not the intellectual, and it privileges the campaign loyalist over the expert. Those days are gone . To be sure, some of the blame rests with the increasing irrelevance of overly narrow research in the social sciences. But it is also because the primary requisite of seniority in the policy world is too often an answer to the question: What did you do during the campaign? This is the code of the samurai, not the intellectual, and it privileges the campaign loyalist over the expert. I have a hard time, for example, imagining that I would be called to Washington today in the way I was back in 1990, when the senior Senator from Pennsylvania asked a former U.S. Ambassador to the UN who she might recommend to advise him on foreign affairs, and she gave him my name. Despite the fact that I had no connection to Pennsylvania and had never worked on his campaigns, he called me at the campus where I was teaching, and later invited me to join his personal staff. Universities, without doubt, have to own some of this mess. The idea of telling students that professors run the show and know better than they do strikes many students as something like uppity lip from the help, and so many profs don\u2019t do it. (One of the greatest teachers I ever had, James Schall, once wrote many years ago that students have obligations to teachers, including trust, docility, effort, and thinking, an assertion that would produce howls of outrage from the entitled generations roaming campuses today.) As a result, many academic departments are boutiques, in which the professors are expected to be something like intellectual valets. This produces nothing but a delusion of intellectual adequacy in children who should be instructed, not catered to.","title":"The downside of no gatekeepers"},{"location":"Thought/the-death-of-expertise/#the-confidence-of-the-dumb","text":"There\u2019s also that immutable problem known as human nature. It has a name now: it\u2019s called the Dunning-Kruger effect, which says, in sum, that the dumber you are, the more confident you are that you\u2019re not actually dumb. And when you get invested in being aggressively dumb\u2026well, the last thing you want to encounter are experts who disagree with you, and so you dismiss them in order to maintain your unreasonably high opinion of yourself. (There\u2019s a lot of that loose on social media, especially.) All of these are symptoms of the same disease: a manic reinterpretation of democracy in which everyone must have their say, and no one must be disrespected. (The verb to disrespect is one of the most obnoxious and insidious innovations in our language in years, because it really means to fail to pay me the impossibly high requirement of respect I demand. ) This yearning for respect and equality, even\u2014perhaps especially\u2014if unearned, is so intense that it brooks no disagreement. It represents the full flowering of a therapeutic culture where self-esteem, not achievement, is the ultimate human value, and it\u2019s making us all dumber by the day. Thus, at least some of the people who reject expertise are not really, as they often claim, showing their independence of thought. They are instead rejecting anything that might stir a gnawing insecurity that their own opinion might not be worth all that much.","title":"The confidence of the dumb"},{"location":"Thought/the-death-of-expertise/#experts-the-servants-not-masters-of-a-democracy","text":"So what can we do? Not much, sadly, since this is a cultural and generational issue that will take a long time come right, if it ever does. Personally, I don\u2019t think technocrats and intellectuals should rule the world: we had quite enough of that in the late 20th century, thank you, and it should be clear now that intellectualism makes for lousy policy without some sort of political common sense. Indeed, in an ideal world, experts are the servants, not the masters, of a democracy. But when citizens forgo their basic obligation to learn enough to actually govern themselves, and instead remain stubbornly imprisoned by their fragile egos and caged by their own sense of entitlement, experts will end up running things by default. That\u2019s a terrible outcome for everyone. Expertise is necessary, and it\u2019s not going away. Unless we return it to a healthy role in public policy, we\u2019re going to have stupider and less productive arguments every day. So here, presented without modesty or political sensitivity, are some things to think about when engaging with experts in their area of specialization. We can all stipulate: the expert isn\u2019t always right. But an expert is far more likely to be right than you are. On a question of factual interpretation or evaluation, it shouldn\u2019t engender insecurity or anxiety to think that an expert\u2019s view is likely to be better-informed than yours. (Because, likely, it is.) Experts come in many flavors. Education enables it, but practitioners in a field acquire expertise through experience; usually the combination of the two is the mark of a true expert in a field. But if you have neither education nor experience, you might want to consider exactly what it is you\u2019re bringing to the argument. In any discussion, you have a positive obligation to learn at least enough to make the conversation possible. The University of Google doesn\u2019t count. Remember: having a strong opinion about something isn\u2019t the same as knowing something. And yes, your political opinions have value. Of course they do: you\u2019re a member of a democracy and what you want is as important as what any other voter wants. As a layman, however, your political analysis, has far less value, and probably isn\u2019t \u2014 indeed, almost certainly isn\u2019t \u2014 as good as you think it is. And how do I know all this? Just who do I think I am? Well, of course: I\u2019m an expert.","title":"Experts: the servants, not masters, of a democracy"},{"location":"Thought/tiresome/","text":"M\u1ecfi Ai, N\u00e0o \u0111\u00e2u mu\u1ed1n gi\u00e0 Th\u1eddi gian b\u1ea1c \u00e1c T\u00e2m tr\u1ea1ng ng\u00e0y h\u00f4m nay c\u1ee7a m\u00ecnh kh\u00f4ng t\u1ed1t. C\u00f3 l\u1ebd kh\u00f4ng n\u00ean vi\u1ebft nh\u01b0ng m\u00e0 t\u1eeb h\u00f4m qua \u0111\u00e3 vi\u1ebft r\u1ed3i n\u00ean k\u1ec7. M\u1ed9t tr\u1ea1ng th\u00e1i c\u1ee7a tr\u00ed n\u00e3o A state of mind . Nh\u1eefng l\u00fac n\u00e0y l\u00e0 nh\u1eefng l\u00fac m\u00ecnh c\u1ea7n con ng\u01b0\u1eddi. \u0110\u00fang h\u01a1n l\u00e0 c\u1ea7n \u0111\u00fang ng\u01b0\u1eddi. T\u00f4i l\u00e0 m\u1ed9t con ng\u01b0\u1eddi c\u00f4 \u0111\u1ed9c. C\u00f4 \u0111\u1ed9c kh\u00f4ng ph\u1ea3i do l\u1ed7i c\u1ee7a t\u00f4i, m\u00e0 th\u1eadt ra l\u1ea1i ch\u00ednh l\u00e0 do t\u00f4i. Do ch\u00ednh b\u1ea3n th\u00e2n m\u00ecnh l\u00e0 m\u00ecnh, n\u00ean m\u00ecnh b\u1ecb c\u00f4 \u0111\u1ed9c. S\u1ed1ng m\u1ed9t cu\u1ed9c \u0111\u1eddi kh\u00f4ng \u0111\u1ec1 ph\u00f2ng b\u1ea5t c\u1ee9 ai, m\u1edf l\u00f2ng v\u1edbi t\u1ea5t c\u1ea3 m\u1ecdi ng\u01b0\u1eddi n\u00ean t\u1ea5t th\u1ea3y m\u1ecdi ng\u01b0\u1eddi l\u1ea1i \u0111\u1ec1 ph\u00f2ng. C\u00f3 l\u1ebd c\u00f3 m\u1ed9t s\u1ef1 c\u00e2n b\u1eb1ng \u1edf \u0111\u00e2y? T\u00f4i lu\u00f4n r\u1ea5t mu\u1ed1n h\u00e1t nh\u1eefng kh\u00fac ca b\u1ecfng ch\u00e1y H\u00f2a m\u00ecnh v\u00e0o d\u00f2ng ng\u01b0\u1eddi t\u1eebng ng\u00e0y kh\u00f4ng l\u1eabn m\u00ecnh v\u1edbi ai Kh\u00f4ng cho m\u00ecnh nhi\u1ec1u \u0111\u1eafn \u0111o khi \u0111\u1ee9ng tr\u01b0\u1edbc nh\u1eefng l\u1ed1i r\u1ebd B\u00ecnh th\u1ea3n trong gian nan, tin \u1edf ch\u00ednh m\u00ecnh V\u00e0o nh\u1eefng l\u00fac ch\u00e1n n\u1ea3n th\u1ebf n\u00e0y t\u00f4i l\u1ea1i t\u00ecm v\u1ec1 ch\u00fa L\u1eadp. L\u1eddi ca c\u1ee7a ch\u00fa l\u00e0 tu\u1ed5i tr\u1ebb c\u1ee7a t\u00f4i, l\u00e0 ti\u1ebfng l\u00f2ng c\u1ee7a t\u00f4i. Kh\u00f4ng c\u00f3 nh\u1ea1c c\u1ee7a B\u1ee9c t\u01b0\u1eddng v\u00e0 Tr\u1ea7n L\u1eadp, kh\u00f4ng hi\u1ec3u t\u00f4i s\u1ebd s\u1ed1ng \u0111\u1ebfn gi\u1edd n\u00e0y b\u1eb1ng c\u00e1ch n\u00e0o. M\u1ed7i khi t\u00f4i c\u1ea3m th\u1ea5y tuy\u1ec7t v\u1ecdng, m\u1ed7i khi cu\u1ed9c \u0111\u1eddi h\u1eaft h\u1ee7i nh\u1eefng \u0111i\u1ec1u nh\u1ecf nhoi m\u00e0 t\u00f4i vun v\u00e9n trong cu\u1ed9c s\u1ed1ng, ch\u1ec9 c\u00f3 ch\u00fa L\u1eadp m\u1edbi hi\u1ec3u \u0111\u01b0\u1ee3c kh\u00e1t khao \u0111\u01b0\u1ee3c s\u1ed1ng m\u1ed9t cu\u1ed9c \u0111\u1eddi nh\u1ecf b\u00e9. M\u1ed7i l\u00fac tr\u1edf n\u00ean tuy\u1ec7t v\u1ecdng, t\u00f4i th\u01b0\u1eddng ngh\u0129 \u0111\u1ebfn s\u1ef1 k\u1ebft th\u00fac. N\u00f3i nh\u01b0 m\u1ed9t cu\u1ed1n ti\u1ec3u thuy\u1ebft th\u00ec k\u1ebft th\u00fac l\u1ea1i m\u00e0 m\u1ed9t s\u1ef1 b\u1eaft \u0111\u1ea7u m\u1edbi. Nh\u1ea3m nh\u00ed. K\u1ebft th\u00fac l\u00e0 h\u1ebft. L\u00e0 gi\u1ea3i tho\u00e1t kh\u1ecfi nh\u1eefng c\u00f9m x\u00edch v\u00e2y quanh c\u1ed5. L\u00e0 tr\u1ed1n ch\u1ea1y kh\u1ecfi nh\u1eefng ni\u1ec1m vui v\u00e0 \u0111am m\u00ea cu\u1ed9c s\u1ed1ng. M\u1ed9t ng\u00e0y 15 tu\u1ed5i, t\u00f4i ng\u1ed3i b\u00ean ngo\u00e0i lan can s\u00e2n th\u01b0\u1ee3ng nh\u00e0 t\u00f4i. M\u1ed9t n\u1ed7i \u00e1m \u1ea3nh d\u00e2ng l\u00ean t\u1eeb l\u1ed3ng ng\u1ef1c: n\u1ebfu c\u00f3 m\u1ed9t c\u00e1i g\u00ec \u0111\u00f3 k\u00e9o ch\u00e2n m\u00ecnh xu\u1ed1ng... \u0110\u00f3 l\u00e0 n\u1ed7i \u00e1m \u1ea3nh th\u01b0\u1eddng tr\u1ef1c nh\u1ea5t c\u1ee7a t\u00f4i. L\u00e0 c\u00e1ch m\u00e0 trong nh\u1eefng c\u01a1n m\u01a1 c\u1ee7a m\u1ed9t gi\u1ea5c ng\u1ee7 ch\u1eadp ch\u1eddn t\u00f4i th\u1ea5y s\u1ef1 k\u1ebft th\u00fac c\u1ee7a m\u00ecnh. \u0110\u1ebfn gi\u1edd t\u00f4i c\u0169ng kh\u00f4ng c\u00f2n nh\u1edb m\u00ecnh c\u00f3 th\u1ef1c s\u1ef1 tuy\u1ec7t v\u1ecdng v\u00e0o c\u00e1i th\u1eddi \u0111i\u1ec3m \u0111\u01b0a c\u1ea3 hai ch\u00e2n ra kh\u1ecfi lan can, hay ch\u1ec9 l\u00e0 s\u1ef1 ng\u00f4ng cu\u1ed3ng c\u1ee7a m\u1ed9t tu\u1ed5i tr\u1ebb b\u1ecb k\u00ecm n\u00e9n? M\u1ed9t ng\u00e0y n\u00e0o \u0111\u00f3 t\u00f4i s\u1ebd r\u1eddi xa kh\u1ecfi \u0111\u00e2y. \u0110\u00f3 v\u1edbi t\u00f4i c\u0169ng nh\u01b0 l\u00e0 m\u1ed9t c\u00e1i ch\u1ebft nho nh\u1ecf. T\u00f4i y\u00eau m\u1ea3nh \u0111\u1ea5t n\u00e0y, t\u00f4i y\u00eau th\u00e0nh ph\u1ed1 c\u1ee7a t\u00f4i, v\u1edbi nh\u1eefng \u0111i\u1ec1u nho nh\u1ecf m\u00e0 ch\u1ec9 c\u00f2n m\u1ed9t m\u00ecnh t\u00f4i g\u00ecn gi\u1eef. V\u1edbi nh\u1eefng \u0111i\u1ec1u t\u1ed1t \u0111\u1eb9p m\u00e0 ch\u1ec9 m\u00ecnh t\u00f4i t\u1ef1 h\u00e0o, v\u1edbi nh\u1eefng l\u1ecbch s\u1eed m\u00e0 ch\u1ec9 m\u00ecnh t\u00f4i ghi nh\u1edb. Nh\u01b0ng ch\u01b0a m\u1ed9t ng\u00e0y t\u00f4i ngh\u0129 t\u00f4i s\u1ebd x\u00e2y d\u1ef1ng m\u1ed9t cu\u1ed9c s\u1ed1ng \u1edf th\u00e0nh ph\u1ed1 n\u00e0y. M\u1ed9t ng\u00e0y n\u00e0o \u0111\u00f3 H\u00e0 N\u1ed9i s\u1ebd kh\u00f4ng c\u00f2n m\u1ed9t thanh ni\u00ean th\u1ea5p b\u00e9 \u0111i l\u1ea1i tr\u00ean nh\u1eefng v\u1ec9a h\u00e8, nh\u00f2m v\u00e0o nh\u1eefng con ng\u00f5, \u0111\u01b0a m\u00e1y \u1ea3nh m\u00ecnh v\u00e0o nh\u1eefng khung h\u00ecnh, nh\u1eefng c\u1ea3nh v\u1eadt m\u00e0 kh\u00f4ng ai \u0111\u1ec3 \u00fd, kh\u00f4ng ai ch\u00fa \u00fd \u0111\u1ebfn. Nh\u01b0ng c\u0169ng gi\u1ed1ng nh\u01b0 H\u00e0 N\u1ed9i \u0111\u00e3 kh\u00f4ng c\u00f2n m\u1ed9t c\u1ee5 gi\u00e0 vi\u1ebft t\u1ea3n v\u0103n , hay m\u1ed9t ngh\u1ec7 s\u0129 y\u00eau xe , c\u00f3 l\u1ebd m\u1ed9t H\u00e0 N\u1ed9i kh\u00f4ng c\u00f3 m\u00ecnh s\u1ebd ch\u1eb3ng l\u00e0 m\u1ed9t s\u1ef1 bi\u1ebfn chuy\u1ec3n g\u00ec. Nh\u01b0ng t\u1eebng l\u1edbp ng\u01b0\u1eddi H\u00e0 N\u1ed9i r\u1eddi \u0111i mang theo m\u1ed9t m\u1ea3nh c\u1ee7a qu\u00ea h\u01b0\u01a1ng, m\u00e0 H\u00e0 N\u1ed9i th\u00ec thay \u0111\u1ed5i nhanh ch\u00f3ng. Th\u00f4i th\u00ec c\u0169ng l\u00e0 m\u1ed9t m\u1ea3nh k\u00ed \u1ee9c d\u00f4ng d\u00e0i \u0111\u1ec3 l\u1ea1i \u1edf \u0111\u00e2y.","title":"M\u1ecfi"},{"location":"Thought/tiresome/#moi","text":"Ai, N\u00e0o \u0111\u00e2u mu\u1ed1n gi\u00e0 Th\u1eddi gian b\u1ea1c \u00e1c T\u00e2m tr\u1ea1ng ng\u00e0y h\u00f4m nay c\u1ee7a m\u00ecnh kh\u00f4ng t\u1ed1t. C\u00f3 l\u1ebd kh\u00f4ng n\u00ean vi\u1ebft nh\u01b0ng m\u00e0 t\u1eeb h\u00f4m qua \u0111\u00e3 vi\u1ebft r\u1ed3i n\u00ean k\u1ec7.","title":"M\u1ecfi"},{"location":"Thought/tiresome/#mot-trang-thai-cua-tri-nao","text":"A state of mind . Nh\u1eefng l\u00fac n\u00e0y l\u00e0 nh\u1eefng l\u00fac m\u00ecnh c\u1ea7n con ng\u01b0\u1eddi. \u0110\u00fang h\u01a1n l\u00e0 c\u1ea7n \u0111\u00fang ng\u01b0\u1eddi. T\u00f4i l\u00e0 m\u1ed9t con ng\u01b0\u1eddi c\u00f4 \u0111\u1ed9c. C\u00f4 \u0111\u1ed9c kh\u00f4ng ph\u1ea3i do l\u1ed7i c\u1ee7a t\u00f4i, m\u00e0 th\u1eadt ra l\u1ea1i ch\u00ednh l\u00e0 do t\u00f4i. Do ch\u00ednh b\u1ea3n th\u00e2n m\u00ecnh l\u00e0 m\u00ecnh, n\u00ean m\u00ecnh b\u1ecb c\u00f4 \u0111\u1ed9c. S\u1ed1ng m\u1ed9t cu\u1ed9c \u0111\u1eddi kh\u00f4ng \u0111\u1ec1 ph\u00f2ng b\u1ea5t c\u1ee9 ai, m\u1edf l\u00f2ng v\u1edbi t\u1ea5t c\u1ea3 m\u1ecdi ng\u01b0\u1eddi n\u00ean t\u1ea5t th\u1ea3y m\u1ecdi ng\u01b0\u1eddi l\u1ea1i \u0111\u1ec1 ph\u00f2ng. C\u00f3 l\u1ebd c\u00f3 m\u1ed9t s\u1ef1 c\u00e2n b\u1eb1ng \u1edf \u0111\u00e2y?","title":"M\u1ed9t tr\u1ea1ng th\u00e1i c\u1ee7a tr\u00ed n\u00e3o"},{"location":"Thought/tiresome/#toi-luon-rat-muon-hat-nhung-khuc-ca-bong-chay","text":"H\u00f2a m\u00ecnh v\u00e0o d\u00f2ng ng\u01b0\u1eddi t\u1eebng ng\u00e0y kh\u00f4ng l\u1eabn m\u00ecnh v\u1edbi ai Kh\u00f4ng cho m\u00ecnh nhi\u1ec1u \u0111\u1eafn \u0111o khi \u0111\u1ee9ng tr\u01b0\u1edbc nh\u1eefng l\u1ed1i r\u1ebd B\u00ecnh th\u1ea3n trong gian nan, tin \u1edf ch\u00ednh m\u00ecnh V\u00e0o nh\u1eefng l\u00fac ch\u00e1n n\u1ea3n th\u1ebf n\u00e0y t\u00f4i l\u1ea1i t\u00ecm v\u1ec1 ch\u00fa L\u1eadp. L\u1eddi ca c\u1ee7a ch\u00fa l\u00e0 tu\u1ed5i tr\u1ebb c\u1ee7a t\u00f4i, l\u00e0 ti\u1ebfng l\u00f2ng c\u1ee7a t\u00f4i. Kh\u00f4ng c\u00f3 nh\u1ea1c c\u1ee7a B\u1ee9c t\u01b0\u1eddng v\u00e0 Tr\u1ea7n L\u1eadp, kh\u00f4ng hi\u1ec3u t\u00f4i s\u1ebd s\u1ed1ng \u0111\u1ebfn gi\u1edd n\u00e0y b\u1eb1ng c\u00e1ch n\u00e0o. M\u1ed7i khi t\u00f4i c\u1ea3m th\u1ea5y tuy\u1ec7t v\u1ecdng, m\u1ed7i khi cu\u1ed9c \u0111\u1eddi h\u1eaft h\u1ee7i nh\u1eefng \u0111i\u1ec1u nh\u1ecf nhoi m\u00e0 t\u00f4i vun v\u00e9n trong cu\u1ed9c s\u1ed1ng, ch\u1ec9 c\u00f3 ch\u00fa L\u1eadp m\u1edbi hi\u1ec3u \u0111\u01b0\u1ee3c kh\u00e1t khao \u0111\u01b0\u1ee3c s\u1ed1ng m\u1ed9t cu\u1ed9c \u0111\u1eddi nh\u1ecf b\u00e9. M\u1ed7i l\u00fac tr\u1edf n\u00ean tuy\u1ec7t v\u1ecdng, t\u00f4i th\u01b0\u1eddng ngh\u0129 \u0111\u1ebfn s\u1ef1 k\u1ebft th\u00fac. N\u00f3i nh\u01b0 m\u1ed9t cu\u1ed1n ti\u1ec3u thuy\u1ebft th\u00ec k\u1ebft th\u00fac l\u1ea1i m\u00e0 m\u1ed9t s\u1ef1 b\u1eaft \u0111\u1ea7u m\u1edbi. Nh\u1ea3m nh\u00ed. K\u1ebft th\u00fac l\u00e0 h\u1ebft. L\u00e0 gi\u1ea3i tho\u00e1t kh\u1ecfi nh\u1eefng c\u00f9m x\u00edch v\u00e2y quanh c\u1ed5. L\u00e0 tr\u1ed1n ch\u1ea1y kh\u1ecfi nh\u1eefng ni\u1ec1m vui v\u00e0 \u0111am m\u00ea cu\u1ed9c s\u1ed1ng. M\u1ed9t ng\u00e0y 15 tu\u1ed5i, t\u00f4i ng\u1ed3i b\u00ean ngo\u00e0i lan can s\u00e2n th\u01b0\u1ee3ng nh\u00e0 t\u00f4i. M\u1ed9t n\u1ed7i \u00e1m \u1ea3nh d\u00e2ng l\u00ean t\u1eeb l\u1ed3ng ng\u1ef1c: n\u1ebfu c\u00f3 m\u1ed9t c\u00e1i g\u00ec \u0111\u00f3 k\u00e9o ch\u00e2n m\u00ecnh xu\u1ed1ng... \u0110\u00f3 l\u00e0 n\u1ed7i \u00e1m \u1ea3nh th\u01b0\u1eddng tr\u1ef1c nh\u1ea5t c\u1ee7a t\u00f4i. L\u00e0 c\u00e1ch m\u00e0 trong nh\u1eefng c\u01a1n m\u01a1 c\u1ee7a m\u1ed9t gi\u1ea5c ng\u1ee7 ch\u1eadp ch\u1eddn t\u00f4i th\u1ea5y s\u1ef1 k\u1ebft th\u00fac c\u1ee7a m\u00ecnh. \u0110\u1ebfn gi\u1edd t\u00f4i c\u0169ng kh\u00f4ng c\u00f2n nh\u1edb m\u00ecnh c\u00f3 th\u1ef1c s\u1ef1 tuy\u1ec7t v\u1ecdng v\u00e0o c\u00e1i th\u1eddi \u0111i\u1ec3m \u0111\u01b0a c\u1ea3 hai ch\u00e2n ra kh\u1ecfi lan can, hay ch\u1ec9 l\u00e0 s\u1ef1 ng\u00f4ng cu\u1ed3ng c\u1ee7a m\u1ed9t tu\u1ed5i tr\u1ebb b\u1ecb k\u00ecm n\u00e9n? M\u1ed9t ng\u00e0y n\u00e0o \u0111\u00f3 t\u00f4i s\u1ebd r\u1eddi xa kh\u1ecfi \u0111\u00e2y. \u0110\u00f3 v\u1edbi t\u00f4i c\u0169ng nh\u01b0 l\u00e0 m\u1ed9t c\u00e1i ch\u1ebft nho nh\u1ecf. T\u00f4i y\u00eau m\u1ea3nh \u0111\u1ea5t n\u00e0y, t\u00f4i y\u00eau th\u00e0nh ph\u1ed1 c\u1ee7a t\u00f4i, v\u1edbi nh\u1eefng \u0111i\u1ec1u nho nh\u1ecf m\u00e0 ch\u1ec9 c\u00f2n m\u1ed9t m\u00ecnh t\u00f4i g\u00ecn gi\u1eef. V\u1edbi nh\u1eefng \u0111i\u1ec1u t\u1ed1t \u0111\u1eb9p m\u00e0 ch\u1ec9 m\u00ecnh t\u00f4i t\u1ef1 h\u00e0o, v\u1edbi nh\u1eefng l\u1ecbch s\u1eed m\u00e0 ch\u1ec9 m\u00ecnh t\u00f4i ghi nh\u1edb. Nh\u01b0ng ch\u01b0a m\u1ed9t ng\u00e0y t\u00f4i ngh\u0129 t\u00f4i s\u1ebd x\u00e2y d\u1ef1ng m\u1ed9t cu\u1ed9c s\u1ed1ng \u1edf th\u00e0nh ph\u1ed1 n\u00e0y. M\u1ed9t ng\u00e0y n\u00e0o \u0111\u00f3 H\u00e0 N\u1ed9i s\u1ebd kh\u00f4ng c\u00f2n m\u1ed9t thanh ni\u00ean th\u1ea5p b\u00e9 \u0111i l\u1ea1i tr\u00ean nh\u1eefng v\u1ec9a h\u00e8, nh\u00f2m v\u00e0o nh\u1eefng con ng\u00f5, \u0111\u01b0a m\u00e1y \u1ea3nh m\u00ecnh v\u00e0o nh\u1eefng khung h\u00ecnh, nh\u1eefng c\u1ea3nh v\u1eadt m\u00e0 kh\u00f4ng ai \u0111\u1ec3 \u00fd, kh\u00f4ng ai ch\u00fa \u00fd \u0111\u1ebfn. Nh\u01b0ng c\u0169ng gi\u1ed1ng nh\u01b0 H\u00e0 N\u1ed9i \u0111\u00e3 kh\u00f4ng c\u00f2n m\u1ed9t c\u1ee5 gi\u00e0 vi\u1ebft t\u1ea3n v\u0103n , hay m\u1ed9t ngh\u1ec7 s\u0129 y\u00eau xe , c\u00f3 l\u1ebd m\u1ed9t H\u00e0 N\u1ed9i kh\u00f4ng c\u00f3 m\u00ecnh s\u1ebd ch\u1eb3ng l\u00e0 m\u1ed9t s\u1ef1 bi\u1ebfn chuy\u1ec3n g\u00ec. Nh\u01b0ng t\u1eebng l\u1edbp ng\u01b0\u1eddi H\u00e0 N\u1ed9i r\u1eddi \u0111i mang theo m\u1ed9t m\u1ea3nh c\u1ee7a qu\u00ea h\u01b0\u01a1ng, m\u00e0 H\u00e0 N\u1ed9i th\u00ec thay \u0111\u1ed5i nhanh ch\u00f3ng. Th\u00f4i th\u00ec c\u0169ng l\u00e0 m\u1ed9t m\u1ea3nh k\u00ed \u1ee9c d\u00f4ng d\u00e0i \u0111\u1ec3 l\u1ea1i \u1edf \u0111\u00e2y.","title":"T\u00f4i lu\u00f4n r\u1ea5t mu\u1ed1n h\u00e1t nh\u1eefng kh\u00fac ca b\u1ecfng ch\u00e1y"},{"location":"USTH/note 10-Jun/","text":"Pipeline Retrive Process L\u1ea5y K (Kegg_ko) -> tra c\u1ee9u KEGG L\u1ea5y module & pathway Tr\u1ea3 v\u1ec1 b\u1ea3ng r\u00fat g\u1ecdn (g\u1ed3m name v\u00e0 c\u00e1c id)","title":"note 10 Jun"},{"location":"USTH/note 10-Jun/#pipeline","text":"Retrive","title":"Pipeline"},{"location":"USTH/note 10-Jun/#process","text":"L\u1ea5y K (Kegg_ko) -> tra c\u1ee9u KEGG L\u1ea5y module & pathway Tr\u1ea3 v\u1ec1 b\u1ea3ng r\u00fat g\u1ecdn (g\u1ed3m name v\u00e0 c\u00e1c id)","title":"Process"},{"location":"USTH/process/","text":"Pipeline and process of Python package Pipeline Process incoming query sequence Single simple query # Elaborate 1 database with one gene Bulk query Identify databases Collasping similar ids/query string into list of unique id Create query matrix Query matrix is a table of unique ids/query parameters and database. Each cell constitude to an individual single query Fetch information about databases Database information is stored in a database (metadatabase) Differential between database is intuitively indexed base on both name and function of the database In current version, each function of the databases if accessed differently is considered separated database. For example funricegene db is divided into 3 different databases' entry. In future revision, function of the databases will be integrated into the description of database using template metaprogramming technique. Part of html page that correspond to the data is exacted per definition store in database Current version define the area of data and parse datas into cell according to list of headers Future version will define individual cells and actively search for each headers' content Database definition is stored in database objects. Functions: # in future revision #list of accessor and query function here. # in future revision Running the actual query Powered by BeautifulSoup, Requests and CSV module Requests built using information stored in db Possiblity for paralelization Different IP address/proxy server etc. Survey on how different databases/server handle parallel query (limitation, DDOS etc.) Post-processing of data Text clean up Option for * Regular Expression * Python script (In future version) * Gawk (In future version) 1. Trimming and filterings Option for * Regular Expression * Python script (In future version) * (R ?) (In future version) 1. Exporting 1. Single query: * CSV * Excel * JSON * RDF 2. Query Matrix: * Flat CSV file Cell with multiple data will be delimited with `;` * Structured folder Each folder correspond to one database in the bulk query 1. Chain query(?) Database Object Storable object Attributes: Functions: Accessors Execute Query Result return CSV","title":"Pipeline and process of Python package"},{"location":"USTH/process/#pipeline-and-process-of-python-package","text":"","title":"Pipeline and process of Python package"},{"location":"USTH/process/#pipeline","text":"Process incoming query sequence Single simple query # Elaborate 1 database with one gene Bulk query Identify databases Collasping similar ids/query string into list of unique id Create query matrix Query matrix is a table of unique ids/query parameters and database. Each cell constitude to an individual single query Fetch information about databases Database information is stored in a database (metadatabase) Differential between database is intuitively indexed base on both name and function of the database In current version, each function of the databases if accessed differently is considered separated database. For example funricegene db is divided into 3 different databases' entry. In future revision, function of the databases will be integrated into the description of database using template metaprogramming technique. Part of html page that correspond to the data is exacted per definition store in database Current version define the area of data and parse datas into cell according to list of headers Future version will define individual cells and actively search for each headers' content Database definition is stored in database objects. Functions: # in future revision #list of accessor and query function here. # in future revision Running the actual query Powered by BeautifulSoup, Requests and CSV module Requests built using information stored in db Possiblity for paralelization Different IP address/proxy server etc. Survey on how different databases/server handle parallel query (limitation, DDOS etc.) Post-processing of data Text clean up Option for * Regular Expression * Python script (In future version) * Gawk (In future version) 1. Trimming and filterings Option for * Regular Expression * Python script (In future version) * (R ?) (In future version) 1. Exporting 1. Single query: * CSV * Excel * JSON * RDF 2. Query Matrix: * Flat CSV file Cell with multiple data will be delimited with `;` * Structured folder Each folder correspond to one database in the bulk query 1. Chain query(?)","title":"Pipeline"},{"location":"USTH/process/#database-object","text":"Storable object Attributes: Functions: Accessors Execute Query","title":"Database Object"},{"location":"USTH/process/#result-return","text":"CSV","title":"Result return"}]}